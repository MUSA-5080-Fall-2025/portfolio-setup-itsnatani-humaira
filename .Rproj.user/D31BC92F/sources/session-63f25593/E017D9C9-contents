---
title: "Assignment 4: Predictive Policing"
subtitle: "MUSA 5080 - Fall 2025"
author: "Itsnatani Anaqami"
date: 03 November 2025
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
---

## About This Exercise

In this exercise, I build a spatial predictive model for burglaries using count regression and spatial features.

# Learning Objectives

By the end of this exercise, I am able to:

1.  Create a fishnet grid for aggregating point-level crime data
2.  Calculate spatial features including k-nearest neighbors and distance measures
3.  Diagnose spatial autocorrelation using Local Moran's I
4.  Fit and interpret Poisson and Negative Binomial regression for count data
5.  Implement spatial cross-validation (Leave-One-Group-Out)
6.  Compare model predictions to a Kernel Density Estimation baseline
7.  Evaluate model performance using appropriate metrics

# Setup

```{r setup}
#| message: false
#| warning: false

# Load required packages
library(tidyverse)      # Data manipulation
library(sf)             # Spatial operations
library(here)           # Relative file paths
library(viridis)        # Color scales
library(terra)          # Raster operations (replaces 'raster')
library(spdep)          # Spatial dependence
library(FNN)            # Fast nearest neighbors
library(MASS)           # Negative binomial regression
library(patchwork)      # Plot composition (replaces grid/gridExtra)
library(knitr)          # Tables
library(kableExtra)     # Table formatting
library(classInt)       # Classification intervals
library(here)

# Spatstat split into sub-packages
library(spatstat.geom)    # Spatial geometries
library(spatstat.explore) # Spatial exploration/KDE

# Set options
options(scipen = 999)  # No scientific notation
set.seed(5080)         # Reproducibility

# Create consistent theme for visualizations
theme_crime <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title = element_text(face = "bold", size = base_size + 1),
      plot.subtitle = element_text(color = "gray30", size = base_size - 1),
      legend.position = "right",
      panel.grid.minor = element_blank(),
      axis.text = element_blank(),
      axis.title = element_blank()
    )
}

# Set as default
theme_set(theme_crime())

cat("✓ All packages loaded successfully!\n")
cat("✓ Working directory:", getwd(), "\n")
```

# Part 1: Load and Explore Data

In this step, I loaded the data I needed for my analysis. The primary data needed was error crime data, as that's what I wanted to predict in this assignment. I also used spatial data such as police districts and beats, as well as Chicago boundaries as a spatial base in mapping the number of crimes. In this step, I also visualized the error data to see crime point locations and observe existing spatial patterns.

## Exercise 1.1: Load Chicago Spatial Data

```{r load-boundaries}
#| message: false

# Load police districts (used for spatial cross-validation)
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

# Load police beats (smaller administrative units)
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(Beat = beat_num)

# Load Chicago boundary
chicagoBoundary <- 
  st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") %>%
  st_transform('ESRI:102271')

cat("✓ Loaded spatial boundaries\n")
cat("  - Police districts:", nrow(policeDistricts), "\n")
cat("  - Police beats:", nrow(policeBeats), "\n")
```

::: callout-note
## Coordinate Reference System

I am using `ESRI:102271` (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:

-   It minimizes distortion in this region
-   Uses feet (common in US planning)
-   Allows accurate distance calculations
:::

## Exercise 1.2: Load Burglary Data

```{r load-burglaries}
#| message: false

# Load from provided data file (downloaded from Chicago open data portal)
burglaries <- st_read(here("assignments/assignment4/scripts/data", "burglaries.shp")) %>% 
  st_transform('ESRI:102271')

# Check the data
cat("\n✓ Loaded burglary data\n")
cat("  - Number of burglaries:", nrow(burglaries), "\n")
cat("  - CRS:", st_crs(burglaries)$input, "\n")
cat("  - Date range:", min(burglaries$Date, na.rm = TRUE), "to", 
    max(burglaries$Date, na.rm = TRUE), "\n")
```

**Question 1.1:** How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis? There are 7482 burglaries case in the dataset. The dataset covers data from 2017-01-01 to 2018-01-01. Unifying the The CRS matters because it ensures that all spatial data layers line up correctly and that distance, area, and spatial relationships are measured accurately. Without a consistent and appropriate CRS, the crime points, police districts, and boundaries may misalign, and any calculations (like density or spatial models) would be distorted or wrong.

::: callout-warning
## Critical Pause #1: Data Provenance

Before proceeding, consider where this data came from:

**Who recorded this data?** Chicago Police Department officers and detectives

**What might be missing?**

-   Unreported burglaries (victims didn't call police)
-   Incidents police chose not to record
-   Downgraded offenses (burglary recorded as trespassing)
-   Spatial bias (more patrol = more recorded crime)

**Think About** Was there a Department of Justice investigation of CPD during this period? What did they find about data practices? The United States Department of Justice (DOJ) conducted a “pattern or practice” investigation of the Chicago Police Department (CPD) started on December 2015. The findings were announced on January 2017. The report found that CPD “does not effectively document and meaningfully review officers’ use of force”. If CPD’s data collection, management, or transparency was flawed around that period, then data I am using might share systemic limitations (e.g., missing cases, mis-classification, bias). The DOJ’s focus on “documentation and review” suggests that not all force or misconduct events were reliably reported or investigated, which raises caution about relying solely on official incident datasets without considering possible under-reporting or bias.
:::

## Exercise 1.3: Visualize Point Data

```{r visualize-points}
#| fig-width: 10
#| fig-height: 5

# Simple point map
p1 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = burglaries, color = "#d62828", size = 0.1, alpha = 0.4) +
  labs(
    title = "Burglary Locations",
    subtitle = paste0("Chicago 2017, n = ", nrow(burglaries))
  )

# Density surface using modern syntax
p2 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = data.frame(st_coordinates(burglaries)),
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"  # Modern ggplot2 syntax (not guide = FALSE)
  ) +
  labs(
    title = "Density Surface",
    subtitle = "Kernel density estimation"
  )

# Combine plots using patchwork (modern approach)
p1 + p2 + 
  plot_annotation(
    title = "Spatial Distribution of Burglaries in Chicago",
    tag_levels = 'A'
  )
```

**Question 1.2:** What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?

Burglaries are not evenly distributed across Chicago. There is a clear clustered spatial pattern of burglaries rather than a random distribution.There are areas located in the southern and northern part of the city that have noticeably higher densities than the rest of the city. Peripheral areas show far fewer incidents, which may indicates that burglaries occur often in area with major residential zones, commercial corridors, or transportation routes, rather than peripheral areas.

# Part 2: Create Fishnet Grid

In this step, I create a 500 × 500 meter fishnet grid, which produces 2,458 cells across Chicago. I then aggregate the burglary incidents into these grid cells using a spatial join, resulting in a consistent unit of analysis for modeling and visualization.

## Exercise 2.1: Understanding the Fishnet

A **fishnet grid** converts irregular point data into a regular grid of cells where we can:

-   Aggregate counts
-   Calculate spatial features
-   Apply regression models

Think of it as overlaying graph paper on a map.

```{r create-fishnet}
# Create 500m x 500m grid
fishnet <- st_make_grid(
  chicagoBoundary,
  cellsize = 500,  # 500 meters per cell
  square = TRUE
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Keep only cells that intersect Chicago
fishnet <- fishnet[chicagoBoundary, ]

# View basic info
cat("✓ Created fishnet grid\n")
cat("  - Number of cells:", nrow(fishnet), "\n")
cat("  - Cell size:", 500, "x", 500, "meters\n")
cat("  - Cell area:", round(st_area(fishnet[1,])), "square meters\n")
```

**Question 2.1:** Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach? A regular grid gives uniform, equally sized units that avoid the arbitrary shapes of neighborhoods or census tracts. This improves comparability, reduces boundary-related bias (MAUP), and works better for spatial modeling. But there's a limitation too in using grid, the downside is that grids are less intuitive, may split meaningful areas, and can produce many empty cells at fine resolutions.

## Exercise 2.2: Aggregate Burglaries to Grid

```{r aggregate-burglaries}
# Spatial join: which cell contains each burglary?
burglaries_fishnet <- st_join(burglaries, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(countBurglaries = n())

# Join back to fishnet (cells with 0 burglaries will be NA)
fishnet <- fishnet %>%
  left_join(burglaries_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries = replace_na(countBurglaries, 0))

# Summary statistics
cat("\nBurglary count distribution:\n")
summary(fishnet$countBurglaries)
cat("\nCells with zero burglaries:", 
    sum(fishnet$countBurglaries == 0), 
    "/", nrow(fishnet),
    "(", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), "%)\n")
```

```{r visualize-fishnet}
#| fig-width: 8
#| fig-height: 6

# Visualize aggregated counts
ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "Burglaries",
    option = "plasma",
    trans = "sqrt",  # Square root for better visualization of skewed data
    breaks = c(0, 1, 5, 10, 20, 40)
  ) +
  labs(
    title = "Burglary Counts by Grid Cell",
    subtitle = "500m x 500m cells, Chicago 2017"
  ) +
  theme_crime()
```

**Question 2.2:** What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)

Compared to the spatial distribution density surface map, the grid map shows more precisely where burglaries occur because it uses uniform cells rather than smoothing the data. The mid-range burglary counts (5–10) appear more evenly distributed across space for the same reason. Many cells have zero burglaries because a 500×500 m grid creates many small units where no incidents fall. This leads to overdispersion (variance greater than the mean), meaning the distribution is not well suited to a basic Poisson regression model.

# Part 3: Create a Kernel Density Baseline

Before building complex models, let's create a simple baseline using **Kernel Density Estimation (KDE)**.

**The KDE baseline asks:** "What if crime just happens where it happened before?" (simple spatial smoothing, no predictors)

```{r kde-baseline}
#| message: false

# Convert burglaries to ppp (point pattern) format for spatstat
burglaries_ppp <- as.ppp(
  st_coordinates(burglaries),
  W = as.owin(st_bbox(chicagoBoundary))
)

# Calculate KDE with 1km bandwidth
kde_burglaries <- density.ppp(
  burglaries_ppp,
  sigma = 1000,  # 1km bandwidth
  edge = TRUE    # Edge correction
)

# Convert to terra raster (modern approach, not raster::raster)
kde_raster <- rast(kde_burglaries)

# Extract KDE values to fishnet cells
fishnet <- fishnet %>%
  mutate(
    kde_value = terra::extract(
      kde_raster,
      vect(fishnet),
      fun = mean,
      na.rm = TRUE
    )[, 2]  # Extract just the values column
  )

cat("✓ Calculated KDE baseline\n")
```

```{r visualize-kde}
#| fig-width: 8
#| fig-height: 6

ggplot() +
  geom_sf(data = fishnet, aes(fill = kde_value), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "KDE Value",
    option = "plasma"
  ) +
  labs(
    title = "Kernel Density Estimation Baseline",
    subtitle = "Simple spatial smoothing of burglary locations"
  ) +
  theme_crime()
```

**Question 3.1:** How does the KDE map compare to the count map? What does KDE capture well? What does it miss?

KDE map smooths point locations and highlights general hotspots, making broad spatial patterns easy to see. It reduces noise by blending nearby incidents, so major clusters stand out clearly. But KDE map does not show exact counts or how many crimes occurred in a specific location. It can blur sharp boundaries and make hotspots look larger or smoother than they really are. On the other hand, the grid count map shows precise spatial detail, showing which cells had 0, 1, or 20 burglaries. It also exposes fine-scale variation that KDE smooths away.

# Part 4: Create Spatial Predictor Variables

Now we'll create features that might help predict burglaries. We'll use "broken windows theory" logic: signs of disorder predict crime. In this section, I use vacant and abandoned houses as a predictor of burglary risk. My hypothesis: areas with more vacant or abandoned buildings will have higher burglary counts. These locations often have fewer residents or “eyes on the street,” creating opportunities for offenders. Vacant properties can also signal broader social or economic decline, which may further increase vulnerability to crime.

## Exercise 4.1: Load 311 Vacant and Abandoned House Calls

```{r load-abandoned-house}
#| message: false

abandoned_house <- read_csv(here("assignments/assignment4/scripts/data/vacant_buildings_2018.csv"))%>%
  filter(!is.na(LATITUDE), !is.na(LONGITUDE)) %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326) %>%
  st_transform('ESRI:102271')

cat("✓ Loaded abandoned house calls\n")
cat("  - Number of calls:", nrow(abandoned_house), "\n")
```

::: callout-note
## Data Loading Note

**Consider:** How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?

The 311 reporting system can be biased because it reflects who chooses and is able to report problems, not just where problems actually occur. People with more time, resources, or trust in city services are more likely to call. This means affluent neighborhoods often show higher 311 reporting rates, not necessarily because they have more issues, but because residents have greater awareness, access, and expectations for city responsiveness. In contrast, residents in lower-income areas may be used to chronic problems, may not expect the city to respond, or may face barriers such as limited time, internet access, or language support. As a result, 311 data can underrepresent disorder in disadvantaged neighborhoods.
:::

## Exercise 4.2: Count of Abandoned Houses per Cell

```{r count-abandoned-houses}
# Aggregate abandoned house calls to fishnet
abandoned_fishnet <- st_join(abandoned_house, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(abandoned_house = n())

# Join to fishnet
fishnet <- fishnet %>%
  left_join(abandoned_fishnet, by = "uniqueID") %>%
  mutate(abandoned_house = replace_na(abandoned_house, 0))

cat("Abandoned house distribution:\n")
summary(fishnet$abandoned_house)
```

```{r visualize-abandoned-houses}
#| fig-width: 10
#| fig-height: 4

p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abandoned_house), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "magma") +
  labs(title = "Abandoned House 311 Calls") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma") +
  labs(title = "Burglaries") +
  theme_crime()

p1 + p2 +
  plot_annotation(title = "Are abandoned houses and burglaries correlated?")
```

**Question 4.1:** Do you see a visual relationship between abandoned houses and burglaries? What does this suggest?

Visually, the relationship is not extremely strong, but two of the hotspots of abandoned-house reports, which located in the southeast and southern areas, do overlap with higher burglary counts. This indicates that vacancy may contribute to elevated burglary risk in certain neighborhoods, even if it does not explain the overall citywide pattern.

## Exercise 4.3: Nearest Neighbor Features

Count in a cell is one measure. Distance to the nearest 3 abandoned houses captures local context.

```{r nn-feature}
#| message: false

# Calculate mean distance to 3 nearest abandoned houses
# (Do this OUTSIDE of mutate to avoid sf conflicts)

# Get coordinates
fishnet_coords <- st_coordinates(st_centroid(fishnet))
abandoned_coords <- st_coordinates(abandoned_house)

# Calculate k nearest neighbors and distances
nn_result <- get.knnx(abandoned_coords, fishnet_coords, k = 3)

# Add to fishnet
fishnet <- fishnet %>%
  mutate(
    abandoned_house.nn = rowMeans(nn_result$nn.dist)
  )

cat("✓ Calculated nearest neighbor distances\n")
summary(fishnet$abandoned_house.nn)
```

**Question 4.2:** What does a low value of `abandoned_house.nn` mean? A high value? Why might this be informative?

Low values means the cell is very close to an abandoned house. Possibly adjacent or within a short walking distanceand indicates local concentration or clustering of abandoned houses. On the other hand, high values means the cell is far away from any abandoned house. Indicates areas with few or no abandoned properties nearby and suggests spatial separation from problem properties. Many outcomes (crime, property values, blight spread) depend not just on whether abandoned houses exist, but how close they are. Distance gives nuance beyond a binary yes/no.

## Exercise 4.4: Distance to Hot Spots

Let's identify clusters of abandoned houses using Local Moran's I, then calculate distance to these hot spots.

```{r local-morans-abandoned}
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {
  
  # Create spatial weights
  coords <- st_coordinates(st_centroid(data))
  neighbors <- knn2nb(knearneigh(coords, k = k))
  weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)
  
  # Calculate Local Moran's I
  local_moran <- localmoran(data[[variable]], weights)
  
  # Classify clusters
  mean_val <- mean(data[[variable]], na.rm = TRUE)
  
  data %>%
    mutate(
      local_i = local_moran[, 1],
      p_value = local_moran[, 5],
      is_significant = p_value < 0.05,
      
      moran_class = case_when(
        !is_significant ~ "Not Significant",
        local_i > 0 & .data[[variable]] > mean_val ~ "High-High",
        local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
        local_i < 0 & .data[[variable]] > mean_val ~ "High-Low",
        local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
        TRUE ~ "Not Significant"
      )
    )
}

# Apply to abandoned houses
fishnet <- calculate_local_morans(fishnet, "abandoned_house", k = 5)
```

```{r visualize-morans}
#| fig-width: 8
#| fig-height: 6

# Visualize hot spots
ggplot() +
  geom_sf(
    data = fishnet, 
    aes(fill = moran_class), 
    color = NA
  ) +
  scale_fill_manual(
    values = c(
      "High-High" = "#d7191c",
      "High-Low" = "#fdae61",
      "Low-High" = "#abd9e9",
      "Low-Low" = "#2c7bb6",
      "Not Significant" = "gray90"
    ),
    name = "Cluster Type"
  ) +
  labs(
    title = "Local Moran's I: Abandoned House Clusters",
    subtitle = "High-High = Hot spots of disorder"
  ) +
  theme_crime()
```

```{r distance-to-hotspots}
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
  filter(moran_class == "High-High") %>%
  st_centroid()

# Calculate distance from each cell to nearest hot spot
if (nrow(hotspots) > 0) {
  fishnet <- fishnet %>%
    mutate(
      dist_to_hotspot = as.numeric(
        st_distance(st_centroid(fishnet), hotspots %>% st_union())
      )
    )
  
  cat("✓ Calculated distance to abandoned houses hot spots\n")
  cat("  - Number of hot spot cells:", nrow(hotspots), "\n")
} else {
  fishnet <- fishnet %>%
    mutate(dist_to_hotspot = 0)
  cat("⚠ No significant hot spots found\n")
}
```

**Question 4.3:** Why might distance to a cluster of abandoned houses be more informative than distance to a single abandoned house? What does Local Moran's I tell us?

Distance to a cluster of abandoned houses is more informative than distance to a single abandoned house because clusters reflect real, neighborhood-level decline, while one vacant property may be an isolated or temporary issue. Local Moran’s I helps identify these meaningful patterns by showing where high values group together (hotspots) or where low values group together (stable areas), allowing us to distinguish true problem areas from random outliers.

::: callout-note
**Local Moran's I** identifies:

-   **High-High**: Hot spots (high values surrounded by high values)
-   **Low-Low**: Cold spots (low values surrounded by low values)
-   **High-Low / Low-High**: Spatial outliers
:::

------------------------------------------------------------------------

# Part 5: Join Police Districts for Cross-Validation

In this part, I attach police district information to each fishnet cell. I join the fishnet grid with the police district boundaries so that every cell is labeled with the district it falls into. This step is necessary because later on, I will use these district labels to perform spatial cross-validation.

```{r join-districts}
# Join district information to fishnet
fishnet <- st_join(
  fishnet,
  policeDistricts,
  join = st_within,
  left = TRUE
) %>%
  filter(!is.na(District))  # Remove cells outside districts

cat("✓ Joined police districts\n")
cat("  - Districts:", length(unique(fishnet$District)), "\n")
cat("  - Cells:", nrow(fishnet), "\n")
```

# Part 6: Model Fitting

In this step, I prepared a clean modeling dataset from the fishnet grid and fit statistical models to explain burglary counts. I first used a Poisson regression, which is the standard approach for modeling count data like crime incidents. Then, because Poisson models assume the mean and variance are equal, I checked for overdispersion—a common issue in real crime data where variance is much larger than the mean. Detecting overdispersion is essential because it can lead to biased estimates and misleading significance levels. When overdispersion was detected, I fit a Negative Binomial regression, which relaxes this assumption and provides more reliable estimates. Comparing AIC values allowed me to choose the model that best fits the data. Overall, this step ensures that the final crime prediction model is statistically sound, interpretable, and appropriate for the structure of the data.

## Exercise 6.1: Poisson Regression

Burglary counts are count data (0, 1, 2, 3...). I'll use **Poisson regression**.

```{r prepare-data}
# Create clean modeling dataset
fishnet_model <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(
    uniqueID,
    District,
    countBurglaries,
    abandoned_house,
    abandoned_house.nn,
    dist_to_hotspot
  ) %>%
  na.omit()  # Remove any remaining NAs

cat("✓ Prepared modeling data\n")
cat("  - Observations:", nrow(fishnet_model), "\n")
cat("  - Variables:", ncol(fishnet_model), "\n")
```

```{r fit-poisson}
# Fit Poisson regression
model_poisson <- glm(
  countBurglaries ~ abandoned_house + abandoned_house.nn + 
    dist_to_hotspot,
  data = fishnet_model,
  family = "poisson"
)

# Summary
summary(model_poisson)
```

**Question 6.1:** Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?

All three predictors are statistically significant at the p \< 0.001 level, which means each variables has a meaningful relationship with burglary counts in the fishnet grid.Positive sign means the burglary risk increases as the number of variable increases, and the negative sign tells the opposite, where burglary risk decreases as the number of variable increases. - abandoned_house (positive, significant): More abandoned houses within the cell are associated with higher burglary counts. - abandoned_house.nn (negative, significant): Higher abandoned-house levels in neighboring cells are associated with lower burglaries in this cell. This may seem counterintuitive, but this variable is the neighbor average, not the local count. A negative coefficient suggests burglaries cluster away from areas with very high neighboring abandonment.

Often, extremely distressed surrounding areas may have: Fewer targets, lower residential density, less foot traffic, and fewer occupied homes to burglarize. So this variable picks up spillover effects, not local opportunity. - dist_to_hotspot (negative, significant): Cells closer to abandonment hotspots experience more burglaries; burglary risk decreases as distance increases.

## Exercise 6.2: Check for Overdispersion

Poisson regression assumes mean = variance. Real count data often violates this (overdispersion).

```{r check-overdispersion}
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) / 
              model_poisson$df.residual

cat("Dispersion parameter:", round(dispersion, 2), "\n")
cat("Rule of thumb: >1.5 suggests overdispersion\n")

if (dispersion > 1.5) {
  cat("⚠ Overdispersion detected! Consider Negative Binomial model.\n")
} else {
  cat("✓ Dispersion looks okay for Poisson model.\n")
}
```

## Exercise 6.3: Negative Binomial Regression

If overdispersed, use **Negative Binomial regression** (more flexible).

```{r fit-negbin}
# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ abandoned_house + abandoned_house.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Summary
summary(model_nb)

# Compare AIC (lower is better)
cat("\nModel Comparison:\n")
cat("Poisson AIC:", round(AIC(model_poisson), 1), "\n")
cat("Negative Binomial AIC:", round(AIC(model_nb), 1), "\n")
```

**Question 6.2:** Which model fits better (lower AIC)? What does this tell you about the data?

The Negative Binomial model fits better, because it has a lower AIC (7271.4 vs. 8324.7). This tells us that the burglary data are **overdispersed**—the variance is larger than what the Poisson model can handle—so a Negative Binomial model provides a more accurate and appropriate fit for these crime counts.

*Your answer here:*

# Part 7: Spatial Cross-Validation

Standard cross-validation randomly splits data. But with spatial data, this means training on cells right next to test cells (information leakage!).

In this part, I perform spatial cross-validation to evaluate how well the model generalizes across different parts of the city. I use police districts as the spatial units for cross-validation because they represent meaningful, operational geographic boundaries where conditions tend to be internally similar but different from neighboring districts. Using districts reduces spatial leakage—where nearby locations in the training data accidentally help predict the test data—and therefore provides a more realistic assessment of model performance. Police districts also align the analysis with how the city organizes public safety and resource allocation, making the results more interpretable and actionable for policymakers.

**Leave-One-Group-Out (LOGO) Cross-Validation** trains on all districts except one, then tests on the held-out district.

```{r spatial-cv}
# Get unique districts
districts <- unique(fishnet_model$District)
cv_results <- tibble()

cat("Running LOGO Cross-Validation...\n")

for (i in seq_along(districts)) {
  
  test_district <- districts[i]
  
  # Split data
  train_data <- fishnet_model %>% filter(District != test_district)
  test_data <- fishnet_model %>% filter(District == test_district)
  
  # Fit model on training data
  model_cv <- glm.nb(
    countBurglaries ~ abandoned_house + abandoned_house.nn + 
      dist_to_hotspot,
    data = train_data
  )
  
  # Predict on test data
  test_data <- test_data %>%
    mutate(
      prediction = predict(model_cv, test_data, type = "response")
    )
  
  # Calculate metrics
  mae <- mean(abs(test_data$countBurglaries - test_data$prediction))
  rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
  
  # Store results
  cv_results <- bind_rows(
    cv_results,
    tibble(
      fold = i,
      test_district = test_district,
      n_test = nrow(test_data),
      mae = mae,
      rmse = rmse
    )
  )
  
  cat("  Fold", i, "/", length(districts), "- District", test_district, 
      "- MAE:", round(mae, 2), "\n")
}

# Overall results
cat("\n✓ Cross-Validation Complete\n")
cat("Mean MAE:", round(mean(cv_results$mae), 2), "\n")
cat("Mean RMSE:", round(mean(cv_results$rmse), 2), "\n")
```

```{r cv-results-table}
# Show results
cv_results %>%
  arrange(desc(mae)) %>%
  kable(
    digits = 2,
    caption = "LOGO CV Results by District"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Question 7.1:** Why is spatial CV more appropriate than random CV for this problem? Which districts were hardest to predict?

Spatial CV is more appropriate because crime risk is spatially autocorrelated—nearby locations tend to have similar crime patterns. If we use random CV, training and test samples are often neighbors, so the model cheats by learning from areas that are almost identical to the test set. This leads to overly optimistic accuracy. Using police districts for CV forces the model to predict entire unseen geographic areas, reducing spatial leakage and giving a more realistic measure of generalizability—closer to how the model would perform in new districts. From the result, The hardest districts to predict are those with the highest error values (MAE and RMSE), which are: District 3, District 7, and District 12.

# Part 8: Model Predictions and Comparison

## Exercise 8.1: Generate Final Predictions

```{r final-predictions}
# Fit final model on all data
final_model <- glm.nb(
  countBurglaries ~ abandoned_house + abandoned_house.nn + 
    dist_to_hotspot,
  data = fishnet_model
)

# Add predictions back to fishnet
fishnet <- fishnet %>%
  mutate(
    prediction_nb = predict(final_model, fishnet_model, type = "response")[match(uniqueID, fishnet_model$uniqueID)]
  )

# Also add KDE predictions (normalize to same scale as counts)
kde_sum <- sum(fishnet$kde_value, na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries, na.rm = TRUE)
fishnet <- fishnet %>%
  mutate(
    prediction_kde = (kde_value / kde_sum) * count_sum
  )
```

## Exercise 8.2: Compare Model vs. KDE Baseline

```{r compare-models}
#| fig-width: 12
#| fig-height: 4

# Create three maps
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
  labs(title = "Actual Burglaries") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "Model Predictions (Neg. Binomial)") +
  theme_crime()

p3 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "KDE Baseline Predictions") +
  theme_crime()

p1 + p2 + p3 +
  plot_annotation(
    title = "Actual vs. Predicted Burglaries",
    subtitle = "Does our complex model outperform simple KDE?"
  )
```

```{r model-comparison-metrics}
# Calculate performance metrics
comparison <- fishnet %>%
  st_drop_geometry() %>%
  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
  summarize(
    model_mae = mean(abs(countBurglaries - prediction_nb)),
    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
    kde_mae = mean(abs(countBurglaries - prediction_kde)),
    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))
  )

comparison %>%
  pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
  separate(metric, into = c("approach", "metric"), sep = "_") %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  kable(
    digits = 2,
    caption = "Model Performance Comparison"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Question 8.1:** Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?

The results indicate that the complex regression model does not outperform the simple KDE baseline. MAE: KDE is lower by 0.18 (2.06 vs. 2.24) RMSE: KDE is lower by 0.33 (2.95 vs. 3.28) The results represents roughly an 8-10% gain in predictive accuracy. While the differences are modest, they are consistent across both metrics.

These findings suggest that the added complexity of the model does not translate into better predictive performance. Although the complex model may still offer value for interpretation—such as understanding how specific environmental features relate to crime—its predictive accuracy does not exceed that of the simpler KDE method.

## Exercise 9.3: Where Does the Model Work Well?

```{r prediction-errors}
#| fig-width: 10
#| fig-height: 5

# Calculate errors
fishnet <- fishnet %>%
  mutate(
    error_nb = countBurglaries - prediction_nb,
    error_kde = countBurglaries - prediction_kde,
    abs_error_nb = abs(error_nb),
    abs_error_kde = abs(error_kde)
  )

# Map errors
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
  scale_fill_gradient2(
    name = "Error",
    low = "#2166ac", mid = "white", high = "#b2182b",
    midpoint = 0,
    limits = c(-10, 10)
  ) +
  labs(title = "Model Errors (Actual - Predicted)") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
  scale_fill_viridis_c(name = "Abs. Error", option = "magma") +
  labs(title = "Absolute Model Errors") +
  theme_crime()

p1 + p2
```

**Question 9.2:** Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?

The absolute error map shows the largest errors in: - The far South Side - The Southeast lakefront - A few pockets in the Southwest

On the other hand, the model performs better in: Central and North Side neighborhoods, where burglary counts are lower and more stable.

The errors are not randomly scattered. Errors cluster in specific area. The error pattern may suggest:

1.  The model struggles in places with atypical dynamics Areas with unusually high crime, high vacancy, or rapid neighborhood change produce the largest errors.

2.  311 data may not reflect true conditions everywhere If certain neighborhoods underreport 311 issues (e.g., low civic engagement, distrust of government), the predictor becomes biased - and the model mispredicts.

3.  Burglary is influenced by factors not included in the model, such as:

-   Socioeconomic conditions
-   Policing patterns
-   Land use
-   Housing density
-   Opportunity structures (commercial corridors, transit access)

# Part 10: Summary Statistics and Tables

## Exercise 10.1: Model Summary Table

```{r model-summary-table}
# Create nice summary table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
  mutate(
    across(where(is.numeric), ~round(., 3))
  )

model_summary %>%
  kable(
    caption = "Final Negative Binomial Model Coefficients (Exponentiated)",
    col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  footnote(
    general = "Rate ratios > 1 indicate positive association with burglary counts."
  )
```

## Exercise 10.2: Key Findings Summary

**Technical Performance:**

-   Cross-validation MAE: `r round(mean(cv_results$mae), 2)`
-   Model vs. KDE: \[The KDE baseline performed slightly better, with lower MAE (2.06 vs. 2.24) and RMSE (2.95 vs. 3.28).\]
-   Most predictive variable: \[abandoned_house.nn (large effect, highly significant).\]

**Spatial Patterns:**

-   Burglaries are \[Clustered, not evenly distributed.\]
-   Hot spots are located in \[upper-central and Southeast lakefront neighborhoods\]
-   Model errors show \[systematic\] patterns

**Model Limitations:**

-   Overdispersion: \[Poisson model was overdispersed; Negative Binomial provided a better fit (AIC 7271 vs. 8324).\]
-   Spatial autocorrelation in residuals: \[The residuals show strong and significant spatial autocorrelation. Moran’s I = 0.2883. Expected I ≈ 0 → so 0.288 is much higher than random.\]
-   Cells with zero counts: \[781 / 2458 ( 31.8 % of data)\]

# Conclusion and Next Steps

The model identifies significant spatial clustering of burglaries, with clear hotspots in specific neighborhoods. While the model performs better than a simple KDE baseline, some residual spatial autocorrelation remains, indicating localized patterns not fully captured.

Next Steps: - Refine the model by incorporating temporal patterns (time of day, seasonality) and additional environmental factors (street lighting, security presence). - Monitor changes over time to assess model accuracy and effectiveness of interventions. - Explore integrating predictive outputs with city planning tools to inform broader neighborhood safety strategies.
