[
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Bike-share systems are public transportation services where users can borrow bicycles for short-term use at a low cost. Bicycles are typically parked at stations or designated parking areas (docking stations) and can be picked up from one station and returned to another within the network. These systems are designed for short urban trips and help reduce traffic congestion, and are often part of sustainable transportation strategies in large cities.\n\n\n\nOne A primary challenge in bike share system is managing the uneven distribution of bikes, known as rebalancing challenges. Demand fluctuations often lead to stations being completely full (no space to return a bike) or completely empty (no bikes available to rent). Operators spend a large portion of their budget using vehicles to manually move bikes to meet anticipated demand, and some try to incentivize users to do the rebalancing themselves.\nPhiladelphia has a bike share system called IndeGo. Philadelphia’s Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nImagine you’re an Indego operations manager at 6:00 AM on a Monday morning. You have: - 200 stations across Philadelphia - Limited trucks and staff for moving bikes - 2-3 hours before morning rush hour demand peaks - The question: Which stations will run out of bikes by 8:30 AM?\nThis report presents a predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem.\n\n\n\nThe learning objectives of this assignment are to:\n\nUnderstand panel data structure for space-time analysis\nCreate temporal lag variables to capture demand persistence\nBuild multiple predictive models with increasing complexity\nValidate models temporally (train on past, test on future)\nAnalyze prediction errors in both space and time\nEngineer new features based on error patterns\nCritically evaluate when prediction errors matter most\n\n\n\n\nPart 1: Q1 2025 data - Download Q1 2025 Indego data - Exploratory Analysis - Analyze error patterns - Add 2-3 new features to improve the model\nPart 2: Q3 2024 data - Download Q3 2024 Indego data - Exploratory Analysis - Analyze error patterns - Add 2-3 new features to improve the model - Critical Reflection"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#bike-share-systems",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#bike-share-systems",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Bike-share systems are public transportation services where users can borrow bicycles for short-term use at a low cost. Bicycles are typically parked at stations or designated parking areas (docking stations) and can be picked up from one station and returned to another within the network. These systems are designed for short urban trips and help reduce traffic congestion, and are often part of sustainable transportation strategies in large cities."
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#the-rebalancing-challenge-in-philadelphia",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#the-rebalancing-challenge-in-philadelphia",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "One A primary challenge in bike share system is managing the uneven distribution of bikes, known as rebalancing challenges. Demand fluctuations often lead to stations being completely full (no space to return a bike) or completely empty (no bikes available to rent). Operators spend a large portion of their budget using vehicles to manually move bikes to meet anticipated demand, and some try to incentivize users to do the rebalancing themselves.\nPhiladelphia has a bike share system called IndeGo. Philadelphia’s Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nImagine you’re an Indego operations manager at 6:00 AM on a Monday morning. You have: - 200 stations across Philadelphia - Limited trucks and staff for moving bikes - 2-3 hours before morning rush hour demand peaks - The question: Which stations will run out of bikes by 8:30 AM?\nThis report presents a predictive models that forecast bike share demand across space (different stations) and time (different hours) to help solve this operational problem."
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#learning-objectives",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#learning-objectives",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "The learning objectives of this assignment are to:\n\nUnderstand panel data structure for space-time analysis\nCreate temporal lag variables to capture demand persistence\nBuild multiple predictive models with increasing complexity\nValidate models temporally (train on past, test on future)\nAnalyze prediction errors in both space and time\nEngineer new features based on error patterns\nCritically evaluate when prediction errors matter most"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#assignment-structure",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#assignment-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Part 1: Q1 2025 data - Download Q1 2025 Indego data - Exploratory Analysis - Analyze error patterns - Add 2-3 new features to improve the model\nPart 2: Q3 2024 data - Download Q3 2024 Indego data - Exploratory Analysis - Analyze error patterns - Add 2-3 new features to improve the model - Critical Reflection"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-libraries",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-libraries",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\ninstall.packages(\"tigris\")\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. \noptions(scipen = 999)"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#define-themes",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#define-themes",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#set-census-api-key",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#set-census-api-key",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Set Census API Key",
    "text": "Set Census API Key\n\n\nCode\ncensus_api_key(\"28da3bddd17210421250559be63d8c9379ee190c\", overwrite = TRUE, install = TRUE)"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-indego-trip-data-q1-2025",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-indego-trip-data-q1-2025",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Indego Trip Data (Q1 2025)",
    "text": "Load Indego Trip Data (Q1 2025)\nIn this part, I loaded Q1 2025 Indego trip data that can be retrieved from: https://www.rideindego.com/about/data/\n\n\nCode\n# Read Q1 2025 data\nindego &lt;- read_csv(here(\"assignments/assignment5/data/indego-trips-2025-q1.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n\n\nRows: 201,588\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1123985990, 1123986350, 1124202498, 1123986241, 11…\n$ duration            &lt;dbl&gt; 5, 14, 894, 9, 13, 11, 15, 19, 20, 24, 26, 11, 12,…\n$ start_time          &lt;chr&gt; \"1/1/2025 0:00\", \"1/1/2025 0:04\", \"1/1/2025 0:05\",…\n$ end_time            &lt;chr&gt; \"1/1/2025 0:05\", \"1/1/2025 0:18\", \"1/1/2025 14:59\"…\n$ start_station       &lt;dbl&gt; 3371, 3344, 3021, 3253, 3182, 3346, 3049, 3112, 31…\n$ start_lat           &lt;dbl&gt; 39.95340, 39.95961, 39.95390, 39.93074, 39.95081, …\n$ start_lon           &lt;dbl&gt; -75.15430, -75.23354, -75.16902, -75.18924, -75.16…\n$ end_station         &lt;dbl&gt; 3378, 3294, 3073, 3253, 3249, 3000, 3100, 3035, 33…\n$ end_lat             &lt;dbl&gt; 39.95238, 39.95174, 39.96143, 39.93074, 39.95784, …\n$ end_lon             &lt;dbl&gt; -75.14728, -75.17063, -75.15242, -75.18924, -75.19…\n$ bike_id             &lt;chr&gt; \"22580\", \"31417\", \"31393\", \"31434\", \"22872\", \"1187…\n$ plan_duration       &lt;dbl&gt; 30, 365, 1, 30, 1, 365, 30, 30, 365, 365, 30, 365,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego30\", \"Indego365\", \"Walk-up\", \"Indego30\", \"W…\n$ bike_type           &lt;chr&gt; \"electric\", \"electric\", \"electric\", \"electric\", \"e…"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#examine-the-data-structure",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#examine-the-data-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Examine the Data Structure",
    "text": "Examine the Data Structure\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q1 2025:\", nrow(indego), \"\\n\")\n\n\nTotal trips in Q1 2025: 201588 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n\n\nDate range: 1735689600 to 1743464880 \n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\n\nUnique start stations: 265 \n\n\nCode\n# Trip types\ntable(indego$trip_route_category)\n\n\n\n   One Way Round Trip \n    190792      10796 \n\n\nCode\n# Passholder types\ntable(indego$passholder_type)\n\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex    Walk-up \n      5494      94044      91628          3      10419 \n\n\nCode\n# Bike types\ntable(indego$bike_type)\n\n\n\nelectric standard \n  129561    72027"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#create-time-bins",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#create-time-bins",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Time Bins",
    "text": "Create Time Bins\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\nCode\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2025-01-01 00:00:00 2025-01-01 00:00:00     1 Wed       0       0\n2 2025-01-01 00:04:00 2025-01-01 00:00:00     1 Wed       0       0\n3 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n4 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n5 2025-01-01 00:08:00 2025-01-01 00:00:00     1 Wed       0       0\n6 2025-01-01 00:14:00 2025-01-01 00:00:00     1 Wed       0       0"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#trips-over-time",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#trips-over-time",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Trips Over Time",
    "text": "Trips Over Time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q1 2025\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nQuestion: What patterns do you see? How does ridership change over time?\nAlthough the graph fluctuates days to days, the overall trend (red dashed line) shows a steady increase in daily bike-share trips from January through March. This indicates that demand gradually rises through winter and into early spring. We can also see a major spike on February 14th, 2025—a day that coincided with both Valentine’s Day and the Super Bowl.\nNow, let’s investigate this further by comparing the 14th to all other Fridays in the dataset to see what we find. We’ll keep this observation in mind for future feature engineering.\n\n\nCode\nfly_eagles_fly &lt;- daily_trips %&gt;% filter(date == \"2025-02-14\")\n\ntypical_boring_friday &lt;- indego %&gt;%\n  filter(dotw == \"Fri\", date != \"2025-02-14\") %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) %&gt;%\n  summarize(avg_friday_trips = mean(trips))\n\nprint(fly_eagles_fly)\n\n\n# A tibble: 1 × 2\n  date       trips\n  &lt;date&gt;     &lt;int&gt;\n1 2025-02-14  4192\n\n\nCode\nprint(typical_boring_friday)\n\n\n# A tibble: 1 × 1\n  avg_friday_trips\n             &lt;dbl&gt;\n1            2319."
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#hourly-patterns",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#hourly-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Hourly Patterns",
    "text": "Hourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nQuestion: When are the peak hours? How do weekends differ from weekdays?\nOn weekdays, the peak hours occur around 7 AM and 4 PM. These times align with the typical start and end of the work or school day, showing strong commuter-driven demand.\nOn weekends, the pattern is very different. Ridership rises gradually starting around 5 AM, reaches its peak around 3 PM, and then slowly declines as the day ends. This might show that weekend trips are more leisure-oriented rather than tied to commuting schedules."
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#top-stations",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#top-stations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Top Stations",
    "text": "Top Stations\n\n\nCode\n# Most popular origin stations\ntop_stations &lt;- indego %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 20 Indego Stations by Trip Origins\n\n\nstart_station\nstart_lat\nstart_lon\ntrips\n\n\n\n\n3,010\n39.94711\n-75.16618\n3,999\n\n\n3,032\n39.94527\n-75.17971\n2,842\n\n\n3,359\n39.94888\n-75.16978\n2,699\n\n\n3,020\n39.94855\n-75.19007\n2,673\n\n\n3,208\n39.95048\n-75.19324\n2,503\n\n\n3,244\n39.93865\n-75.16674\n2,486\n\n\n3,066\n39.94561\n-75.17348\n2,396\n\n\n3,362\n39.94816\n-75.16226\n2,387\n\n\n3,012\n39.94218\n-75.17747\n2,361\n\n\n3,028\n39.94061\n-75.14958\n2,348\n\n\n3,161\n39.95486\n-75.18091\n2,278\n\n\n3,101\n39.94295\n-75.15955\n2,274\n\n\n3,295\n39.95028\n-75.16027\n2,160\n\n\n3,054\n39.96250\n-75.17420\n2,123\n\n\n3,185\n39.95169\n-75.15888\n2,116\n\n\n3,038\n39.94781\n-75.19409\n2,111\n\n\n3,203\n39.94077\n-75.17227\n2,106\n\n\n3,059\n39.96244\n-75.16121\n2,027\n\n\n3,022\n39.95472\n-75.18323\n2,014\n\n\n3,063\n39.94633\n-75.16980\n2,014"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-philadelphia-census-data",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#load-philadelphia-census-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Load Philadelphia Census Data",
    "text": "Load Philadelphia Census Data\nIn this part, We’ll get census tract data to add demographic context to our stations.\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\n# Check the data\nglimpse(philly_census)\n\n\nRows: 408\nColumns: 17\n$ GEOID                  &lt;chr&gt; \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   &lt;chr&gt; \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              &lt;dbl&gt; 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            &lt;dbl&gt; 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                &lt;dbl&gt; 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            &lt;dbl&gt; 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        &lt;dbl&gt; 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            &lt;dbl&gt; 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      &lt;dbl&gt; 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            &lt;dbl&gt; 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              &lt;dbl&gt; 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            &lt;dbl&gt; 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         &lt;dbl&gt; 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            &lt;dbl&gt; 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit &lt;dbl&gt; 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          &lt;dbl&gt; 67.2100892, 75.5757576, 64.5279720, 79.9950360,…"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#map-philadelphia-context",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#map-philadelphia-context",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Map Philadelphia Context",
    "text": "Map Philadelphia Context\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#join-census-data-to-stations",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#join-census-data-to-stations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Join Census Data to Stations",
    "text": "Join Census Data to Stations\nWe’ll spatially join census characteristics to each bike station.\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-weather-patterns",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-weather-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Weather Patterns",
    "text": "Visualize Weather Patterns\nWho is ready for a Philly winter?!\n\n\nCode\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q1 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#aggregate-trips-to-station-hour-level",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#aggregate-trips-to-station-hour-level",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Aggregate Trips to Station-Hour Level",
    "text": "Aggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n\n\n[1] 116718\n\n\nCode\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n\n\n[1] 245\n\n\nCode\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n\n\n[1] 2150"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#create-complete-panel-structure",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#create-complete-panel-structure",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Create Complete Panel Structure",
    "text": "Create Complete Panel Structure\nNot every station has trips every hour. We need a complete panel where every station-hour combination exists (even if Trip_Count = 0).\n\n\nCode\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 526,750 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 116,718 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 410,032 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n\n\nComplete panel rows: 526,750"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#add-time-features",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#add-time-features",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Add Time Features",
    "text": "Add Time Features\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#join-weather-data",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#join-weather-data",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Join Weather Data",
    "text": "Join Weather Data\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %&gt;% select(Trip_Count, Temperature, Precipitation))\n\n\n   Trip_Count     Temperature   Precipitation  \n Min.   : 0.00   Min.   :10.0   Min.   :0.000  \n 1st Qu.: 0.00   1st Qu.:30.0   1st Qu.:0.000  \n Median : 0.00   Median :37.0   Median :0.000  \n Mean   : 0.34   Mean   :38.7   Mean   :0.005  \n 3rd Qu.: 0.00   3rd Qu.:47.0   3rd Qu.:0.000  \n Max.   :26.00   Max.   :78.0   Max.   :0.710  \n                 NA's   :5880   NA's   :5880"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#why-lags",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#why-lags",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Why Lags?",
    "text": "Why Lags?\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n\n\nRows after removing NA lags: 635,775"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-lag-correlations",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-lag-correlations",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Lag Correlations",
    "text": "Visualize Lag Correlations\n\n\nCode\n# Sample one station to visualize\nexample_station &lt;- study_panel_complete %&gt;%\n  filter(start_station == first(start_station)) %&gt;%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#why-temporal-validation-matters",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#why-temporal-validation-matters",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Why Temporal Validation Matters",
    "text": "Why Temporal Validation Matters\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn’t happened yet!).\nWrong approach: Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\nCorrect approach: Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\nCode\n# Split by week\n# Q1 has weeks 1-13 (Jan-Mar)\n# Train on weeks 1-9 (Jan 1 - early March)\n# Test on weeks 10-13 (rest of March)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 10) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 10) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 10)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 10)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 428,400 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 189,210 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 20089 to 20151 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 20152 to 20178"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-1-baseline-time-weather",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-1-baseline-time-weather",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 1: Baseline (Time + Weather)",
    "text": "Model 1: Baseline (Time + Weather)\n\n\nCode\n# Create day of week factor with treatment (dummy) coding\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8820 -0.3802 -0.1791  0.0128 18.5540 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1488732  0.0077026 -19.328 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0182693  0.0078250  -2.335               0.0196 *  \nas.factor(hour)2  -0.0401185  0.0078013  -5.143   0.0000002711413737 ***\nas.factor(hour)3  -0.0534144  0.0077758  -6.869   0.0000000000064582 ***\nas.factor(hour)4  -0.0420757  0.0078905  -5.332   0.0000000969595824 ***\nas.factor(hour)5   0.0145021  0.0078916   1.838               0.0661 .  \nas.factor(hour)6   0.1551150  0.0078162  19.845 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.2839418  0.0077451  36.661 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.5012789  0.0077165  64.962 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.3598210  0.0078159  46.037 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2460810  0.0076647  32.106 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2676516  0.0077151  34.692 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3345613  0.0077972  42.908 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3335884  0.0078885  42.288 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3298815  0.0076963  42.862 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.3889267  0.0077546  50.155 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.4812228  0.0077566  62.041 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.5848969  0.0077931  75.053 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.4003396  0.0075825  52.798 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.2521779  0.0077095  32.710 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.1373139  0.0074848  18.346 &lt; 0.0000000000000002 ***\nas.factor(hour)21  0.0880114  0.0075923  11.592 &lt; 0.0000000000000002 ***\nas.factor(hour)22  0.0580815  0.0076060   7.636   0.0000000000000224 ***\nas.factor(hour)23  0.0350222  0.0076227   4.594   0.0000043403255736 ***\ndotw_simple2       0.0651434  0.0043665  14.919 &lt; 0.0000000000000002 ***\ndotw_simple3       0.0430540  0.0043119   9.985 &lt; 0.0000000000000002 ***\ndotw_simple4       0.0505750  0.0041306  12.244 &lt; 0.0000000000000002 ***\ndotw_simple5       0.0434119  0.0041584  10.440 &lt; 0.0000000000000002 ***\ndotw_simple6      -0.0672427  0.0042085 -15.978 &lt; 0.0000000000000002 ***\ndotw_simple7      -0.0631357  0.0041631 -15.166 &lt; 0.0000000000000002 ***\nTemperature        0.0069235  0.0001365  50.722 &lt; 0.0000000000000002 ***\nPrecipitation     -2.4416620  0.0939158 -25.998 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7216 on 428368 degrees of freedom\nMultiple R-squared:  0.07598,   Adjusted R-squared:  0.07592 \nF-statistic:  1136 on 31 and 428368 DF,  p-value: &lt; 0.00000000000000022\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference in expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\nWeekday Pattern (Tue-Fri):\n\nAll weekdays have positive coefficients (0.029 to 0.052)\nTuesday has the highest weekday effect (+0.052)\nWeekdays likely benefit from concentrated commuting patterns\n\nWeekend Pattern (Sat-Sun):\n\nBoth weekend days have negative coefficients (-0.061 and -0.065)\nThis means FEWER trips per station-hour than Monday\n\nHourly Interpretation\nHour Coefficient Interpretation 0 (baseline) 0.000 trips/hour (midnight) 1 -0.018 slightly fewer than midnight … 6 +0.151 morning activity starting 7 +0.276 morning rush building 8 +0.487 PEAK morning rush 9 +0.350 post-rush … 17 +0.568 PEAK evening rush (5 PM!) 18 +0.389 evening declining … 23 +0.034 late night minimal\nIsn’t this fun!"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-2-add-temporal-lags",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-2-add-temporal-lags",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 2: Add Temporal Lags",
    "text": "Model 2: Add Temporal Lags\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1487 -0.2595 -0.0970  0.0235 17.5255 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.0944031  0.0069331 -13.616 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0028329  0.0070412  -0.402             0.687436    \nas.factor(hour)2  -0.0113676  0.0070205  -1.619             0.105403    \nas.factor(hour)3  -0.0233665  0.0069984  -3.339             0.000841 ***\nas.factor(hour)4  -0.0014341  0.0071021  -0.202             0.839977    \nas.factor(hour)5   0.0463719  0.0071041   6.528 0.000000000066943682 ***\nas.factor(hour)6   0.1484482  0.0070401  21.086 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.2157558  0.0069815  30.904 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.3617314  0.0069664  51.925 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.1603371  0.0070640  22.698 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.1024525  0.0069124  14.822 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.1360277  0.0069632  19.535 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.2021688  0.0070303  28.757 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.1900074  0.0071126  26.714 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.1963837  0.0069380  28.305 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.2435050  0.0069932  34.820 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.3042270  0.0070018  43.450 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.3749605  0.0070436  53.234 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.1792367  0.0068606  26.126 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.0973764  0.0069608  13.989 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0389727  0.0067549   5.770 0.000000007955345969 ***\nas.factor(hour)21  0.0320507  0.0068394   4.686 0.000002784344470639 ***\nas.factor(hour)22  0.0310219  0.0068459   4.531 0.000005859991631025 ***\nas.factor(hour)23  0.0221611  0.0068594   3.231             0.001235 ** \ndotw_simple2       0.0271522  0.0039314   6.907 0.000000000004972450 ***\ndotw_simple3       0.0147801  0.0038816   3.808             0.000140 ***\ndotw_simple4       0.0206099  0.0037187   5.542 0.000000029877813199 ***\ndotw_simple5       0.0127648  0.0037443   3.409             0.000652 ***\ndotw_simple6      -0.0521272  0.0037909 -13.750 &lt; 0.0000000000000002 ***\ndotw_simple7      -0.0303227  0.0037485  -8.089 0.000000000000000602 ***\nTemperature        0.0029026  0.0001236  23.486 &lt; 0.0000000000000002 ***\nPrecipitation     -1.3850445  0.0846392 -16.364 &lt; 0.0000000000000002 ***\nlag1Hour           0.3247918  0.0014556 223.135 &lt; 0.0000000000000002 ***\nlag3Hours          0.0975134  0.0014354  67.936 &lt; 0.0000000000000002 ***\nlag1day            0.1659090  0.0013907 119.300 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6493 on 428365 degrees of freedom\nMultiple R-squared:  0.2519,    Adjusted R-squared:  0.2518 \nF-statistic:  4241 on 34 and 428365 DF,  p-value: &lt; 0.00000000000000022\n\n\nQuestion: Did adding lags improve R²? Why or why not?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-3-add-demographics",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-3-add-demographics",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 3: Add Demographics",
    "text": "Model 3: Add Demographics\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0188 -0.4829 -0.2267  0.2771 16.9058 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.9225844734  0.0352000581  26.210\nas.factor(hour)1          0.0037772168  0.0441610229   0.086\nas.factor(hour)2         -0.0955075476  0.0487102008  -1.961\nas.factor(hour)3         -0.1340354597  0.0623637654  -2.149\nas.factor(hour)4         -0.1416017281  0.0554770480  -2.552\nas.factor(hour)5         -0.1300062643  0.0388982897  -3.342\nas.factor(hour)6          0.0360772463  0.0331031436   1.090\nas.factor(hour)7          0.1328270695  0.0317248940   4.187\nas.factor(hour)8          0.3866084171  0.0308539302  12.530\nas.factor(hour)9         -0.0445315405  0.0310696933  -1.433\nas.factor(hour)10        -0.0847033567  0.0313859062  -2.699\nas.factor(hour)11        -0.0290106009  0.0314061584  -0.924\nas.factor(hour)12         0.0385416417  0.0310913865   1.240\nas.factor(hour)13         0.0421197178  0.0311715057   1.351\nas.factor(hour)14         0.0117664453  0.0307921034   0.382\nas.factor(hour)15         0.0826672722  0.0306447205   2.698\nas.factor(hour)16         0.1940771623  0.0304962350   6.364\nas.factor(hour)17         0.3048041726  0.0304046610  10.025\nas.factor(hour)18         0.0002652992  0.0305035090   0.009\nas.factor(hour)19        -0.0634666243  0.0312457890  -2.031\nas.factor(hour)20        -0.1392336862  0.0318719442  -4.369\nas.factor(hour)21        -0.1321162902  0.0327955752  -4.028\nas.factor(hour)22        -0.1054260206  0.0336780868  -3.130\nas.factor(hour)23        -0.0569114464  0.0351511626  -1.619\ndotw_simple2              0.0229986016  0.0122313741   1.880\ndotw_simple3             -0.0175461556  0.0122262223  -1.435\ndotw_simple4             -0.0442688138  0.0115791517  -3.823\ndotw_simple5              0.0027339310  0.0117287820   0.233\ndotw_simple6             -0.1108106236  0.0125102844  -8.858\ndotw_simple7             -0.0398429023  0.0126913144  -3.139\nTemperature               0.0039381085  0.0003773197  10.437\nPrecipitation            -4.4263561137  0.3064039720 -14.446\nlag1Hour                  0.2204571054  0.0028934647  76.191\nlag3Hours                 0.0583071946  0.0031516027  18.501\nlag1day                   0.1447413119  0.0029839179  48.507\nMed_Inc.x                 0.0000004207  0.0000001117   3.767\nPercent_Taking_Transit.y -0.0011705604  0.0004209381  -2.781\nPercent_White.y           0.0018451795  0.0002099438   8.789\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                     0.931838    \nas.factor(hour)2                     0.049914 *  \nas.factor(hour)3                     0.031617 *  \nas.factor(hour)4                     0.010699 *  \nas.factor(hour)5                     0.000832 ***\nas.factor(hour)6                     0.275785    \nas.factor(hour)7               0.000028315277 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                     0.151782    \nas.factor(hour)10                    0.006961 ** \nas.factor(hour)11                    0.355633    \nas.factor(hour)12                    0.215118    \nas.factor(hour)13                    0.176627    \nas.factor(hour)14                    0.702369    \nas.factor(hour)15                    0.006985 ** \nas.factor(hour)16              0.000000000198 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18                    0.993061    \nas.factor(hour)19                    0.042237 *  \nas.factor(hour)20              0.000012523294 ***\nas.factor(hour)21              0.000056188260 ***\nas.factor(hour)22                    0.001746 ** \nas.factor(hour)23                    0.105441    \ndotw_simple2                         0.060071 .  \ndotw_simple3                         0.151255    \ndotw_simple4                         0.000132 ***\ndotw_simple5                         0.815688    \ndotw_simple6             &lt; 0.0000000000000002 ***\ndotw_simple7                         0.001694 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation            &lt; 0.0000000000000002 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                            0.000165 ***\nPercent_Taking_Transit.y             0.005423 ** \nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9136 on 83854 degrees of freedom\n  (344508 observations deleted due to missingness)\nMultiple R-squared:  0.1706,    Adjusted R-squared:  0.1702 \nF-statistic:   466 on 37 and 83854 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-4-add-station-fixed-effects",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-4-add-station-fixed-effects",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 4: Add Station Fixed Effects",
    "text": "Model 4: Add Station Fixed Effects\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\n\nModel 4 R-squared: 0.1922707 \n\n\nCode\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\n\nModel 4 Adj R-squared: 0.1896529 \n\n\nWhat do station fixed effects capture? Baseline differences in demand across stations (some are just busier than others!)."
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-5-add-rush-hour-interaction",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#model-5-add-rush-hour-interaction",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Model 5: Add Rush Hour Interaction",
    "text": "Model 5: Add Rush Hour Interaction\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\n\nModel 5 R-squared: 0.1968772 \n\n\nCode\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\n\nModel 5 Adj R-squared: 0.1942455"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#calculate-predictions-and-mae",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#calculate-predictions-and-mae",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Calculate Predictions and MAE",
    "text": "Calculate Predictions and MAE\n\n\nCode\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.60\n\n\n2. + Temporal Lags\n0.50\n\n\n3. + Demographics\n0.74\n\n\n4. + Station FE\n0.73\n\n\n5. + Rush Hour Interaction\n0.73"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-model-comparison",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#visualize-model-comparison",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Visualize Model Comparison",
    "text": "Visualize Model Comparison\n\n\nCode\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nQuestion: Which features gave us the biggest improvement?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#observed-vs.-predicted",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#observed-vs.-predicted",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Observed vs. Predicted",
    "text": "Observed vs. Predicted\nLet’s use our best model (Model 2) for error analysis.\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nQuestion: Where is the model performing well? Where is it struggling?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#spatial-error-patterns",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#spatial-error-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\nCode\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n\n\n\n\n\n\n\n\n\nCode\np1\n\n\n\n\n\n\n\n\n\nQuestion: Do you see spatial clustering of errors? What neighborhoods have high errors?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#temporal-error-patterns",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#temporal-error-patterns",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\nWhen are we most wrong?\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#errors-and-demographics",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#errors-and-demographics",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Errors and Demographics",
    "text": "Errors and Demographics\nAre prediction errors related to neighborhood characteristics?\n\n\nCode\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(\n    station_attributes %&gt;% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\n\nCritical Question: Are prediction errors systematically higher in certain demographic groups? What are the equity implications?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.1",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.1",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 2.1:",
    "text": "Part 2.1:\n\nDownload data for Q2, Q3, or Q4 2024 from: https://www.rideindego.com/about/data/\nAdapt this code to work with your quarter:\n\nUpdate date ranges for weather data\nCheck for any data structure changes\nCreate the same 5 models\nCalculate MAE for each model\n\nCompare results to Q1 2025:\n\nHow do MAE values compare? Why might they differ?\nAre temporal patterns different (e.g., summer vs. winter)?\nWhich features are most important in your quarter?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.2-error-analysis",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.2-error-analysis",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 2.2: Error Analysis",
    "text": "Part 2.2: Error Analysis\nAnalyze your model’s errors in detail:\n\nSpatial patterns:\n\nCreate error maps\nIdentify neighborhoods with high errors\nHypothesize why (missing features? different demand patterns?)\n\nTemporal patterns:\n\nWhen are errors highest?\nDo certain hours/days have systematic under/over-prediction?\nAre there seasonal patterns?\n\nDemographic patterns:\n\nRelate errors to census characteristics\nAre certain communities systematically harder to predict?\nWhat are the equity implications?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.3-feature-engineering-model-improvement",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-2.3-feature-engineering-model-improvement",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 2.3: Feature Engineering & model improvement",
    "text": "Part 2.3: Feature Engineering & model improvement\nBased on your error analysis, add 2-3 NEW features to improve the model:\nPotential features to consider:\nTemporal features:\n\nHoliday indicators (Memorial Day, July 4th, Labor Day)\nSchool calendar (Penn, Drexel, Temple in session?)\nSpecial events (concerts, sports games, conventions)\nDay of month (payday effects?)\n\nWeather features:\n\nFeels-like temperature (wind chill/heat index)\n“Perfect biking weather” indicator (60-75°F, no rain)\nPrecipitation forecast (not just current)\nWeekend + nice weather interaction\n\nSpatial features:\n\nDistance to Center City\nDistance to nearest university\nDistance to nearest park\nPoints of interest nearby (restaurants, offices, bars)\nStation capacity\nBike lane connectivity\n\nTrip history features:\n\nRolling 7-day average demand\nSame hour last week\nStation “type” clustering (residential, commercial, tourist)\n\nImplementation:\n\nAdd your features to the best model\nCompare MAE before and after\nExplain why you chose these features\nDid they improve predictions? Where?\n\nTry a poisson model for count data\n\nDoes this improve model fit?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-4-critical-reflection",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#part-4-critical-reflection",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 4: Critical Reflection",
    "text": "Part 4: Critical Reflection\nWrite 1-2 paragraphs addressing:\n\nOperational implications:\n\nIs your final MAE “good enough” for Indego to use?\nWhen do prediction errors cause problems for rebalancing?\nWould you recommend deploying this system? Under what conditions?\n\nEquity considerations:\n\nDo prediction errors disproportionately affect certain neighborhoods?\nCould this system worsen existing disparities in bike access?\nWhat safeguards would you recommend?\n\nModel limitations:\n\nWhat patterns is your model missing?\nWhat assumptions might not hold in real deployment?\nHow would you improve this with more time/data?"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#what-to-submit-per-team",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#what-to-submit-per-team",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "What to Submit (per team)",
    "text": "What to Submit (per team)\n\nRmd file with all your code (commented!)\nHTML output with results and visualizations\nBrief report summarizing (with supporting data & visualization):\n\nYour quarter and why you chose it\nModel comparison results\nError analysis insights\nNew features you added and why\nCritical reflection on deployment"
  },
  {
    "objectID": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#tips-for-success",
    "href": "assignments/assignment5/scripts/Indego_SpaceTime_Prediction.html#tips-for-success",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Tips for Success",
    "text": "Tips for Success\n\nStart early - data download and processing takes time\nWork together - pair programming is your friend\nTest incrementally - don’t wait until the end to run code\nDocument everything - explain your choices\nBe creative - the best features come from understanding Philly!\nThink critically - technical sophistication isn’t enough"
  },
  {
    "objectID": "labs/lecture/week10_slides.html",
    "href": "labs/lecture/week10_slides.html",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "Scenario: A state corrections department asks you to help predict:\nWill someone released from prison be arrested again within 3 years?\nNot asking: How many times? How long until?\nAsking: Yes or no? Will it happen or not?\nDiscussion question (1 minute):\n\nHow is this different from predicting home prices?\nWhy might they want this prediction?\nWhat could go wrong?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#a-decision-with-real-consequences",
    "href": "labs/lecture/week10_slides.html#a-decision-with-real-consequences",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "",
    "text": "Scenario: A state corrections department asks you to help predict:\nWill someone released from prison be arrested again within 3 years?\nNot asking: How many times? How long until?\nAsking: Yes or no? Will it happen or not?\nDiscussion question (1 minute):\n\nHow is this different from predicting home prices?\nWhy might they want this prediction?\nWhat could go wrong?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#where-weve-been",
    "href": "labs/lecture/week10_slides.html#where-weve-been",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Where We’ve Been",
    "text": "Where We’ve Been\nWeeks 1-7: Linear regression\n\nPredicting continuous outcomes: home prices, income, population\nY = β₀ + β₁X₁ + β₂X₂ + … + ε\nUsed RMSE to evaluate predictions\n\nLast week: Poisson regression\n\nPredicting count outcomes: number of crimes\nDifferent distribution, but still predicting quantities\n\nToday: A fundamentally different question\n\nNot “how much?” but “will it happen?”\nBinary outcomes: yes/no, 0/1, success/failure\nThis requires a completely different approach"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#what-makes-binary-outcomes-different",
    "href": "labs/lecture/week10_slides.html#what-makes-binary-outcomes-different",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "What Makes Binary Outcomes Different?",
    "text": "What Makes Binary Outcomes Different?\nThe problem with linear regression for binary outcomes:\n\n\n\n\n\n\n\n\n\nProblems:\n\nPredictions can be &gt; 1 or &lt; 0 (makes no sense for probability!)\nAssumes constant effect across range (not realistic)\nViolates regression assumptions (errors aren’t normal)"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#enter-logistic-regression",
    "href": "labs/lecture/week10_slides.html#enter-logistic-regression",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Enter: Logistic Regression",
    "text": "Enter: Logistic Regression\nThe solution: Transform the problem!\nInstead of predicting Y directly, predict the probability that Y = 1\nThe logistic function constrains predictions between 0 and 1:\n\\[p(X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k)}}\\]"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#logistic-vs.-linear-visual-comparison",
    "href": "labs/lecture/week10_slides.html#logistic-vs.-linear-visual-comparison",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Logistic vs. Linear: Visual Comparison",
    "text": "Logistic vs. Linear: Visual Comparison\n\n\n\n\n\n\n\n\n\nKey difference: Logistic regression produces valid probabilities!"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#when-do-we-use-logistic-regression",
    "href": "labs/lecture/week10_slides.html#when-do-we-use-logistic-regression",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "When Do We Use Logistic Regression?",
    "text": "When Do We Use Logistic Regression?\nPerfect for binary classification problems in policy:\nCriminal Justice:\n\nWill someone reoffend? (recidivism)\nWill someone appear for court? (flight risk)\n\nHealth:\n\nWill patient develop disease? (risk assessment)\nWill treatment be successful? (outcome prediction)\n\nEconomics:\n\nWill loan default? (credit risk)\nWill person get hired? (employment prediction)\n\nUrban Planning:\n\nWill building be demolished? (blight prediction)\nWill household participate in program? (uptake prediction)"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-logit-transformation",
    "href": "labs/lecture/week10_slides.html#the-logit-transformation",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The Logit Transformation",
    "text": "The Logit Transformation\nBehind the scenes: We work with log-odds, not probabilities directly\nOdds: \\(\\text{Odds} = \\frac{p}{1-p}\\)\nLog-Odds (Logit): \\(\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\\)\nThis creates a linear relationship: \\[\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...\\]\nWhy this matters:\n\nCoefficients are log-odds (like linear regression!)\nBut we interpret as odds ratios when exponentiated: \\(e^{\\beta}\\)\nOR &gt; 1: predictor increases odds of outcome\nOR &lt; 1: predictor decreases odds of outcome"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#example-email-spam-detection",
    "href": "labs/lecture/week10_slides.html#example-email-spam-detection",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Example: Email Spam Detection",
    "text": "Example: Email Spam Detection\nLet’s build a simple spam detector to understand the mechanics.\nGoal: Predict whether email is spam (1) or legitimate (0)\nPredictors: - Number of exclamation marks - Contains word “free” - Email length\n\n\nCode\n# Create example spam detection data\nset.seed(123)\nn_emails &lt;- 1000\n\nspam_data &lt;- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Look at the data\nhead(spam_data)\n\n\n  exclamation_marks contains_free    length is_spam\n1                 4             1 150.21006       1\n2                 7             1 148.00225       1\n3                 4             1 199.10099       1\n4                 8             0 193.39124       1\n5                 9             0  72.53286       1\n6                 2             1 252.02867       1"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#fitting-the-logistic-model",
    "href": "labs/lecture/week10_slides.html#fitting-the-logistic-model",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Fitting the Logistic Model",
    "text": "Fitting the Logistic Model\nIn R, we use glm() with family = \"binomial\"\n\n\nCode\n# Fit logistic regression\nspam_model &lt;- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\n\n\n\nCall:\nglm(formula = is_spam ~ exclamation_marks + contains_free + length, \n    family = \"binomial\", data = spam_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)         233.048  15015.736   0.016    0.988\nexclamation_marks    55.945  53708.285   0.001    0.999\ncontains_free        46.055  49298.975   0.001    0.999\nlength               -1.273     81.369  -0.016    0.988\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.5017e+02  on 999  degrees of freedom\nResidual deviance: 7.5863e-07  on 996  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#interpreting-coefficients",
    "href": "labs/lecture/week10_slides.html#interpreting-coefficients",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\n\nCode\n# Extract coefficients\ncoefs &lt;- coef(spam_model)\nprint(coefs)\n\n\n      (Intercept) exclamation_marks     contains_free            length \n       233.048051         55.944824         46.055006         -1.272668 \n\n\nCode\n# Convert to odds ratios\nodds_ratios &lt;- exp(coefs)\nprint(odds_ratios)\n\n\n      (Intercept) exclamation_marks     contains_free            length \n    1.627356e+101      1.979376e+24      1.003310e+20      2.800833e-01 \n\n\nInterpretation:\n\nexclamation_marks: Each additional ! multiplies odds of spam by 1.9793761^{24}\ncontains_free: Having “free” multiplies odds by 1.0033099^{20}\n\nlength: Each additional character multiplies odds by 0.2801 (shorter = more likely spam)"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#making-predictions",
    "href": "labs/lecture/week10_slides.html#making-predictions",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Making Predictions",
    "text": "Making Predictions\nThe model outputs probabilities:\n\n\nCode\n# Predict probability for a new email\nnew_email &lt;- data.frame(\n  exclamation_marks = 3,\n  contains_free = 1,\n  length = 150\n)\n\npredicted_prob &lt;- predict(spam_model, newdata = new_email, type = \"response\")\ncat(\"Predicted probability of spam:\", round(predicted_prob, 3))\n\n\nPredicted probability of spam: 1\n\n\nBut now what?\n\nIf probability = 0.723, is this spam or not?\nWe need to choose a threshold (cutoff)\nThreshold = 0.5 is common default, but is it the right choice?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-fundamental-challenge",
    "href": "labs/lecture/week10_slides.html#the-fundamental-challenge",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The Fundamental Challenge",
    "text": "The Fundamental Challenge\nThis is where logistic regression gets interesting (and complicated):\nThe model gives us probabilities, but we need to make binary decisions.\nQuestion: What probability threshold should we use to classify?\n\nThreshold = 0.5? (common default)\nThreshold = 0.3? (more aggressive - flag more as spam)\nThreshold = 0.7? (more conservative - only flag obvious spam)\n\nThe answer depends on:\n\nCost of false positives (marking legitimate email as spam)\nCost of false negatives (missing actual spam)\nThese costs are rarely equal!\n\nThe rest of today: How do we evaluate these predictions and choose thresholds?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#from-probabilities-to-decisions",
    "href": "labs/lecture/week10_slides.html#from-probabilities-to-decisions",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "From Probabilities to Decisions",
    "text": "From Probabilities to Decisions\nWe now have a model that predicts probabilities.\nBut policy decisions require binary choices: spam/not spam, approve/deny, intervene/don’t intervene.\nThis requires two steps:\n\nChoose a threshold to convert probabilities → binary predictions\nEvaluate how good those predictions are\n\nThe confusion matrix helps us with step 2"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-four-outcomes",
    "href": "labs/lecture/week10_slides.html#the-four-outcomes",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The Four Outcomes",
    "text": "The Four Outcomes\nWhen we make binary predictions, four things can happen:\n\n\nModel says “Yes”:\n\nTrue Positive (TP): Correct! ✓\nFalse Positive (FP): Wrong - Type I error\n\nModel says “No”:\n\nTrue Negative (TN): Correct! ✓\n\nFalse Negative (FN): Wrong - Type II error\n\n\n\n\n\nConfusion Matrix Structure\n\n\n\n\nRemember: The model predicts probabilities. WE choose the threshold that converts probabilities to yes/no predictions."
  },
  {
    "objectID": "labs/lecture/week10_slides.html#quick-example-covid-testing",
    "href": "labs/lecture/week10_slides.html#quick-example-covid-testing",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Quick Example: COVID Testing",
    "text": "Quick Example: COVID Testing\nScenario: Testing for COVID-19\n\n\n\nCOVID Test Outcomes\n\n\n\n\n\n\n\n\nTrue_Status\nTest_Result\nOutcome\nConsequence\n\n\n\n\nPositive\nPositive\nTrue Positive (TP)\nQuarantine (correct)\n\n\nPositive\nNegative\nFalse Negative (FN)\nGoes to work, spreads virus\n\n\nNegative\nPositive\nFalse Positive (FP)\nQuarantines unnecessarily\n\n\nNegative\nNegative\nTrue Negative (TN)\nGoes to work (correct)\n\n\n\n\n\nWhich error is worse?\n\nFalse Negative → Virus spreads\nFalse Positive → Unnecessary quarantine\n\nThe answer depends on context! (And changes our threshold choice)"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#calculating-performance-metrics",
    "href": "labs/lecture/week10_slides.html#calculating-performance-metrics",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Calculating Performance Metrics",
    "text": "Calculating Performance Metrics\nFrom the confusion matrix, we derive metrics that emphasize different trade-offs:\nSensitivity (Recall, True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN}\\] “Of all actual positives, how many did we catch?”\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP}\\] “Of all actual negatives, how many did we correctly identify?”\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP}\\] “Of all our positive predictions, how many were correct?”"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#interactive-example-spam-detection",
    "href": "labs/lecture/week10_slides.html#interactive-example-spam-detection",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Interactive Example: Spam Detection",
    "text": "Interactive Example: Spam Detection\nLet’s say we have an email spam filter:\n\n100 actual spam emails\n900 actual legitimate emails\nOur model makes predictions…\n\n\n\nCode\n# Create example predictions\nset.seed(123)\nspam_data &lt;- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %&gt;%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data &lt;- spam_data %&gt;%\n  mutate(predicted_spam = ifelse(predicted_prob &gt; 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat &lt;- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#spam-filter-results",
    "href": "labs/lecture/week10_slides.html#spam-filter-results",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Spam Filter Results",
    "text": "Spam Filter Results\n\n\n          Reference\nPrediction   0   1\n         0 760  14\n         1 140  86\n\n\n\nSensitivity: 0.86 - We caught 86 % of spam\n\n\nSpecificity: 0.844 - We correctly identified 84.4 % of legitimate emails\n\n\nPrecision: 0.381 - Of emails marked spam, 38.1 % actually were spam\n\n\nQuestion: What happens if we change the threshold?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#why-threshold-choice-matters",
    "href": "labs/lecture/week10_slides.html#why-threshold-choice-matters",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Why Threshold Choice Matters",
    "text": "Why Threshold Choice Matters\nRemember: The model gives us probabilities. We decide what probability triggers action.\nThreshold = 0.3 (low bar) - More emails marked as spam - Higher sensitivity (catch more spam) - Lower specificity (more false alarms)\nThreshold = 0.7 (high bar) - Fewer emails marked as spam - Lower sensitivity (miss some spam) - Higher specificity (fewer false alarms)\nThere is no “right” answer - it depends on the costs of each type of error"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-great-sensitivity-specificity-trade-off",
    "href": "labs/lecture/week10_slides.html#the-great-sensitivity-specificity-trade-off",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The Great Sensitivity-Specificity Trade-off",
    "text": "The Great Sensitivity-Specificity Trade-off\n\n\nCode\n# Calculate metrics at different thresholds\nthresholds &lt;- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold &lt;- map_df(thresholds, function(thresh) {\n  preds &lt;- ifelse(spam_data$predicted_prob &gt; thresh, 1, 0)\n  cm &lt;- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#two-policy-scenarios",
    "href": "labs/lecture/week10_slides.html#two-policy-scenarios",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Two Policy Scenarios",
    "text": "Two Policy Scenarios\nScenario A: Rare, deadly disease screening\n\nDisease is rare but fatal if untreated\nTreatment is safe with minor side effects\nGoal: Don’t miss any cases (high sensitivity)\nAcceptable: Some false positives (low threshold)\n\nScenario B: Identifying “high-risk” individuals for intervention\n\nLimited intervention slots\nFalse positives waste resources\nFalse negatives miss opportunities to help\nGoal: Use resources efficiently (high precision)\nDecision depends on: Cost of intervention vs. cost of missed case\n\nClass discussion: Which metrics matter most for each scenario?"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-roc-curve-visualizing-all-thresholds",
    "href": "labs/lecture/week10_slides.html#the-roc-curve-visualizing-all-thresholds",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The ROC Curve: Visualizing All Thresholds",
    "text": "The ROC Curve: Visualizing All Thresholds\nROC = Receiver Operating Characteristic\n(Originally developed for radar signal detection in WWII)\nWhat it shows:\n\nEvery possible threshold\nTrade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)\nOverall model discrimination ability\n\nHow to read it:\n\nX-axis: False Positive Rate (1 - Specificity)\nY-axis: True Positive Rate (Sensitivity)\nDiagonal line: Random guessing\nTop-left corner: Perfect prediction"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#creating-an-roc-curve",
    "href": "labs/lecture/week10_slides.html#creating-an-roc-curve",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Creating an ROC Curve",
    "text": "Creating an ROC Curve\n\n\nCode\n# Create ROC curve for our spam example\nroc_obj &lt;- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nCode\n# Print AUC\nauc_value &lt;- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n\n\n\nArea Under the Curve (AUC): 0.938"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#interpreting-auc",
    "href": "labs/lecture/week10_slides.html#interpreting-auc",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Interpreting AUC",
    "text": "Interpreting AUC\nAUC (Area Under the Curve) summarizes overall model performance:\n\nAUC = 1.0: Perfect classifier\nAUC = 0.9-1.0: Excellent\nAUC = 0.8-0.9: Good\nAUC = 0.7-0.8: Acceptable\nAUC = 0.6-0.7: Poor\nAUC = 0.5: No better than random guessing\nAUC &lt; 0.5: Worse than random (your model is backwards!)\n\nOur spam filter AUC = 0.938\nInterpretation: The model has good discrimination ability, but…\n\nAUC doesn’t tell us which threshold to use\nAUC doesn’t account for class imbalance\nAUC doesn’t show us equity implications"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#understanding-the-roc-curve-points",
    "href": "labs/lecture/week10_slides.html#understanding-the-roc-curve-points",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Understanding the ROC Curve Points",
    "text": "Understanding the ROC Curve Points"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#the-core-problem-disparate-impact",
    "href": "labs/lecture/week10_slides.html#the-core-problem-disparate-impact",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "The Core Problem: Disparate Impact",
    "text": "The Core Problem: Disparate Impact\nA model can be “accurate” overall but perform very differently across groups\nExample metrics from a recidivism model:\n\n\n\nGroup\nSensitivity\nSpecificity\nFalse Positive Rate\n\n\n\n\nOverall\n0.72\n0.68\n0.32\n\n\nGroup A\n0.78\n0.74\n0.26\n\n\nGroup B\n0.64\n0.58\n0.42\n\n\n\nGroup B experiences:\n\nLower sensitivity (more people who will reoffend are missed)\nLower specificity (more people who won’t reoffend are flagged)\nHigher false positive rate (more unjust interventions)\n\nThis is algorithmic bias in action"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#real-world-case-compas",
    "href": "labs/lecture/week10_slides.html#real-world-case-compas",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Real-World Case: COMPAS",
    "text": "Real-World Case: COMPAS\nCOMPAS: Commercial algorithm used in criminal justice to predict recidivism\nProPublica investigation (2016) found:\n\nSimilar overall accuracy for Black and White defendants\nBUT: False positive rates differed dramatically\n\nBlack defendants: 45% false positive rate\nWhite defendants: 23% false positive rate\n\nBlack defendants twice as likely to be incorrectly labeled “high risk”\n\nResult:\n\nDifferent threshold needed for different groups to achieve equity\nBut single-threshold systems are the norm\nKey insight: Overall accuracy masks disparate impact"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#framework-for-threshold-selection",
    "href": "labs/lecture/week10_slides.html#framework-for-threshold-selection",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Framework for Threshold Selection",
    "text": "Framework for Threshold Selection\nStep 1: Understand the consequences\n\nWhat happens with a false positive?\nWhat happens with a false negative?\nAre costs symmetric or asymmetric?\n\nStep 2: Consider stakeholder perspectives\n\nWho is affected by each type of error?\nDo all groups experience consequences equally?\n\nStep 3: Choose your metric priority\n\nMaximize sensitivity? (catch all positives)\nMaximize specificity? (minimize false alarms)\nBalance precision and recall? (F1 score)\nEqualize across groups?\n\nStep 4: Test multiple thresholds\n\nEvaluate performance across thresholds\nLook at group-wise performance\nConsider sensitivity analysis"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#cost-benefit-analysis-approach",
    "href": "labs/lecture/week10_slides.html#cost-benefit-analysis-approach",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Cost-Benefit Analysis Approach",
    "text": "Cost-Benefit Analysis Approach\nAssign concrete costs to errors:\nExample: Disease screening\n\nTrue Positive: Treatment cost $1000, prevent $50,000 in complications\nFalse Positive: Unnecessary treatment $1000\nTrue Negative: No cost\nFalse Negative: Miss disease, $50,000 in complications later\n\nCalculate expected cost at each threshold: \\[E[\\text{Cost}] = C_{FP} \\times FP + C_{FN} \\times FN\\]\nChoose threshold that minimizes expected cost\nNote: This assumes we can quantify all costs, which is often impossible for justice/equity concerns"
  },
  {
    "objectID": "labs/lecture/week10_slides.html#practical-recommendations",
    "href": "labs/lecture/week10_slides.html#practical-recommendations",
    "title": "Logistic Regression for Binary Outcomes",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nReport multiple metrics - not just accuracy\nShow the ROC curve - demonstrates trade-offs\nTest multiple thresholds - document your choice\nEvaluate by sub-group - check for disparate impact\nDocument assumptions - explain why you chose your threshold\nConsider context - what are the real-world consequences?\nProvide uncertainty - confidence intervals, not just point estimates\nEnable recourse - can predictions be challenged?\n\nMost importantly: Be transparent about limitations and potential harms"
  },
  {
    "objectID": "labs/lab6/week-09/lab/week9_test.html",
    "href": "labs/lab6/week-09/lab/week9_test.html",
    "title": "Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "labs/lab6/week-09/lab/week9_test.html#quarto",
    "href": "labs/lab6/week-09/lab/week9_test.html#quarto",
    "title": "Critical Perspectives on Predictive Policing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished presentation. To learn more about Quarto presentations see https://quarto.org/docs/presentations/."
  },
  {
    "objectID": "labs/lab6/week-09/lab/week9_test.html#bullets",
    "href": "labs/lab6/week-09/lab/week9_test.html#bullets",
    "title": "Critical Perspectives on Predictive Policing",
    "section": "Bullets",
    "text": "Bullets\nWhen you click the Render button a document will be generated that includes:\n\nContent authored with markdown\nOutput from executable code"
  },
  {
    "objectID": "labs/lab6/week-09/lab/week9_test.html#code",
    "href": "labs/lab6/week-09/lab/week9_test.html#code",
    "title": "Critical Perspectives on Predictive Policing",
    "section": "Code",
    "text": "Code\nWhen you click the Render button a presentation will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n\n[1] 2"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "assignments/midterm/midterm-final-model.html",
    "href": "assignments/midterm/midterm-final-model.html",
    "title": "MUSA 5080 Midterm",
    "section": "",
    "text": "Goal: Predict 2023-2024 home sale prices accurately while communicating findings clearly to a policy audience."
  },
  {
    "objectID": "assignments/midterm/midterm-final-model.html#musa-5080-midterm",
    "href": "assignments/midterm/midterm-final-model.html#musa-5080-midterm",
    "title": "MUSA 5080 Midterm",
    "section": "",
    "text": "Goal: Predict 2023-2024 home sale prices accurately while communicating findings clearly to a policy audience."
  },
  {
    "objectID": "assignments/midterm/midterm-final-model.html#part-1-data-cleaning-preparation",
    "href": "assignments/midterm/midterm-final-model.html#part-1-data-cleaning-preparation",
    "title": "MUSA 5080 Midterm",
    "section": "Part 1: Data Cleaning & Preparation",
    "text": "Part 1: Data Cleaning & Preparation\n\nPrimary Dataset: Property Sales\nThis dataset contains actual property sales with:\n\nSale price\nSale date\nProperty characteristics (bedrooms, bathrooms, sq ft, etc.)\nProperty location (address, coordinates)\n\n\n\nStep 1: Load and clean Philadelphia sales data\n\nClean it (missing values, outliers, data errors)\nFilter to 2023-2024 residential sales only\n\n\nsales_data &lt;- read_csv(\"data/clean_data.csv\")\nhead(sales_data)\n\n# A tibble: 6 × 51\n  sale_date           sale_price total_area total_livable_area year_built\n  &lt;dttm&gt;                   &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n1 2024-04-16 08:00:00      90000        644                900       1925\n2 2024-10-18 08:00:00      20000       1073                976       1940\n3 2023-09-17 08:00:00     389000       4701               1080       1961\n4 2023-08-01 08:00:00     309000       2050               1792       1925\n5 2023-07-31 08:00:00     185000       1125               1440       1925\n6 2024-06-30 08:00:00     399000       1984               2292       1925\n# ℹ 46 more variables: category_code &lt;dbl&gt;, category_code_description &lt;chr&gt;,\n#   census_tract &lt;dbl&gt;, exterior_condition &lt;dbl&gt;, interior_condition &lt;dbl&gt;,\n#   fireplaces &lt;dbl&gt;, garage_spaces &lt;dbl&gt;, general_construction &lt;chr&gt;,\n#   market_value &lt;dbl&gt;, number_of_bathrooms &lt;dbl&gt;, number_of_bedrooms &lt;dbl&gt;,\n#   basements &lt;chr&gt;, prop_id &lt;dbl&gt;, violent_2blocks &lt;dbl&gt;, petty_1block &lt;dbl&gt;,\n#   neigh_name &lt;chr&gt;, dist_stop_ft &lt;dbl&gt;, dist_bike_ft &lt;dbl&gt;,\n#   dist_hosp_mi &lt;dbl&gt;, service_band_code &lt;dbl&gt;, service_band &lt;chr&gt;, …\n\nglimpse(sales_data)\n\nRows: 19,736\nColumns: 51\n$ sale_date                 &lt;dttm&gt; 2024-04-16 08:00:00, 2024-10-18 08:00:00, 2…\n$ sale_price                &lt;dbl&gt; 90000, 20000, 389000, 309000, 185000, 399000…\n$ total_area                &lt;dbl&gt; 644, 1073, 4701, 2050, 1125, 1984, 1384, 111…\n$ total_livable_area        &lt;dbl&gt; 900, 976, 1080, 1792, 1440, 2292, 1188, 1845…\n$ year_built                &lt;dbl&gt; 1925, 1940, 1961, 1925, 1925, 1925, 1950, 19…\n$ category_code             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ category_code_description &lt;chr&gt; \"SINGLE FAMILY\", \"SINGLE FAMILY\", \"SINGLE FA…\n$ census_tract              &lt;dbl&gt; 41, 287, 356, 184, 191, 158, 313, 356, 255, …\n$ exterior_condition        &lt;dbl&gt; 5, 6, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 4, 6, 4,…\n$ interior_condition        &lt;dbl&gt; 5, 5, 4, 4, 4, 4, 4, 3, 3, 2, 2, 4, 3, 5, 4,…\n$ fireplaces                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ garage_spaces             &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ general_construction      &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ market_value              &lt;dbl&gt; 149300, 66700, 327400, 248300, 167900, 57310…\n$ number_of_bathrooms       &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 0,…\n$ number_of_bedrooms        &lt;dbl&gt; 3, 3, 0, 0, 3, 4, 3, 3, 4, 4, 3, 2, 3, 3, 0,…\n$ basements                 &lt;chr&gt; \"C\", \"F\", \"H\", \"D\", \"H\", \"D\", \"F\", \"A\", \"C\",…\n$ prop_id                   &lt;dbl&gt; 1, 2, 4, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17…\n$ violent_2blocks           &lt;dbl&gt; 47, 90, 1, 5, 65, 14, 63, 2, 8, 100, 53, 70,…\n$ petty_1block              &lt;dbl&gt; 34, 134, 0, 11, 30, 11, 29, 1, 7, 31, 42, 20…\n$ neigh_name                &lt;chr&gt; \"GREENWICH\", \"FELTONVILLE\", \"BUSTLETON\", \"BR…\n$ dist_stop_ft              &lt;dbl&gt; 290.14481, 528.87403, 405.20528, 575.03388, …\n$ dist_bike_ft              &lt;dbl&gt; 690.91700, 399.02685, 510.10879, 512.00309, …\n$ dist_hosp_mi              &lt;dbl&gt; 0.6732689, 1.0606701, 1.8283570, 1.5775431, …\n$ service_band_code         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2,…\n$ service_band              &lt;chr&gt; \"Too Close\", \"Within Service\", \"Within Servi…\n$ dist_park_ft              &lt;dbl&gt; 532.22719, 2162.99199, 458.57125, 1032.98525…\n$ dist_park_mi              &lt;dbl&gt; 0.10080060, 0.40965757, 0.08685062, 0.195641…\n$ dist_school_public_ft     &lt;dbl&gt; 1034.0211, 1402.8327, 1924.2342, 919.1067, 7…\n$ dist_school_charter_ft    &lt;dbl&gt; 4064.3577, 1993.0393, 9331.8429, 2393.9853, …\n$ dist_school_private_ft    &lt;dbl&gt; 4204.2677, 1273.8183, 2949.0531, 8442.3491, …\n$ dist_foodretail_ft        &lt;dbl&gt; 436.9721, 308.0973, 1861.4471, 1772.0048, 38…\n$ foodretail_1mi_count      &lt;dbl&gt; 73, 53, 12, 11, 29, 37, 55, 10, 34, 49, 44, …\n$ dist_police_ft            &lt;dbl&gt; 3534.7457, 5291.8207, 3665.9079, 11101.6058,…\n$ dist_fire_ft              &lt;dbl&gt; 1292.8380, 3479.5780, 3775.9104, 710.2274, 2…\n$ lu2_code                  &lt;dbl&gt; 12, 12, 11, 11, 12, 12, 12, 11, 11, 12, 12, …\n$ GEOID                     &lt;dbl&gt; 42101004101, 42101028700, 42101035601, 42101…\n$ pct_white                 &lt;dbl&gt; 28.67553288, 2.04690832, 65.90263307, 91.860…\n$ pct_black                 &lt;dbl&gt; 12.55237748, 23.07036247, 3.63705247, 0.0439…\n$ pct_hispanic              &lt;dbl&gt; 26.9630169, 72.1961620, 10.7785565, 0.527936…\n$ pct_asian                 &lt;dbl&gt; 26.1249772, 2.5586354, 18.7156659, 0.4399472…\n$ pct_age_25_44             &lt;dbl&gt; 33.52159, 18.46482, 16.15836, 28.86054, 32.0…\n$ pct_age_45_64             &lt;dbl&gt; 23.93879, 25.15991, 26.50123, 24.19710, 21.7…\n$ pct_age_65plus            &lt;dbl&gt; 10.511933, 10.575693, 35.404433, 13.418390, …\n$ vacancy_rate              &lt;dbl&gt; 13.8374899, 13.8952164, 2.2861981, 0.7874016…\n$ med_gross_rent            &lt;dbl&gt; 1540, 1271, 1278, 1456, 1199, 1859, 1383, 12…\n$ med_hh_income             &lt;dbl&gt; 72500, 26707, 61939, 73478, 50238, 116629, 6…\n$ poverty_rate              &lt;dbl&gt; 9.601020, 51.897655, 6.755174, 10.502693, 30…\n$ pct_ba_plus               &lt;dbl&gt; 43.285982, 14.004721, 32.637709, 26.671079, …\n$ pct_hs_only               &lt;dbl&gt; 18.922541, 35.798584, 28.148508, 33.686300, …\n$ unemployment_rate         &lt;dbl&gt; 8.270922, 6.795017, 1.609395, 8.025248, 9.97…\n\n\n\n\nStep 2: Load Secondary Data\n\nCensus data (tidycensus):\nSpatial amenities (OpenDataPhilly)\nJoin to sales data appropriately\nMake sure you have the correct CRS!\n\n\n\n\nStep 3: Feature Engineering\n\nBuffer-based features:\n\nParks within 500ft, 1000ft\nTransit stops within 400ft\nSchools, crime, etc.\n\nk-Nearest Neighbor features:\n\nAverage distance to k nearest parks, transit, etc.\n\nCensus variables:\n\nJoin median income, education, poverty, etc.\n\nInteraction terms:\n\nTheoretically motivated combinations\n\n\n\n\n\nStep 4: Exploratory Data Analysis\nCreate at least 5 professional visualizations:\n\nDistribution of sale prices (histogram)\nGeographic distribution (map)\nPrice vs. structural features (scatter plots)\nPrice vs. spatial features (scatter plots)\nOne creative visualization\n\n\n# Create New Columns for Further Cleaning\nsales_data &lt;- sales_data |&gt; \n  mutate(psf = round(sale_price / total_livable_area, 2),\n         value_multiple = round(market_value / sale_price, 2)) |&gt; \n  arrange(psf)\nsales_data\n\n# A tibble: 19,736 × 53\n   sale_date           sale_price total_area total_livable_area year_built\n   &lt;dttm&gt;                   &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;      &lt;dbl&gt;\n 1 2024-03-11 08:00:00      10000      14895               4200       1900\n 2 2024-02-22 10:00:00      12000       1150               2400       2014\n 3 2024-03-15 08:00:00      10000       1575               1656       1940\n 4 2023-09-06 08:00:00      15000       1201               2451       1915\n 5 2023-10-17 08:00:00      10000       1700               1516       1935\n 6 2024-08-06 08:00:00      14500        946               1892       1915\n 7 2023-11-17 10:00:00      10000        923               1260       1920\n 8 2023-01-17 10:00:00      10000       1410               1260       1940\n 9 2023-07-27 08:00:00      10000        821               1189       1925\n10 2023-07-07 08:00:00      11000       1156               1290       1940\n# ℹ 19,726 more rows\n# ℹ 48 more variables: category_code &lt;dbl&gt;, category_code_description &lt;chr&gt;,\n#   census_tract &lt;dbl&gt;, exterior_condition &lt;dbl&gt;, interior_condition &lt;dbl&gt;,\n#   fireplaces &lt;dbl&gt;, garage_spaces &lt;dbl&gt;, general_construction &lt;chr&gt;,\n#   market_value &lt;dbl&gt;, number_of_bathrooms &lt;dbl&gt;, number_of_bedrooms &lt;dbl&gt;,\n#   basements &lt;chr&gt;, prop_id &lt;dbl&gt;, violent_2blocks &lt;dbl&gt;, petty_1block &lt;dbl&gt;,\n#   neigh_name &lt;chr&gt;, dist_stop_ft &lt;dbl&gt;, dist_bike_ft &lt;dbl&gt;, …\n\ncolnames(sales_data)\n\n [1] \"sale_date\"                 \"sale_price\"               \n [3] \"total_area\"                \"total_livable_area\"       \n [5] \"year_built\"                \"category_code\"            \n [7] \"category_code_description\" \"census_tract\"             \n [9] \"exterior_condition\"        \"interior_condition\"       \n[11] \"fireplaces\"                \"garage_spaces\"            \n[13] \"general_construction\"      \"market_value\"             \n[15] \"number_of_bathrooms\"       \"number_of_bedrooms\"       \n[17] \"basements\"                 \"prop_id\"                  \n[19] \"violent_2blocks\"           \"petty_1block\"             \n[21] \"neigh_name\"                \"dist_stop_ft\"             \n[23] \"dist_bike_ft\"              \"dist_hosp_mi\"             \n[25] \"service_band_code\"         \"service_band\"             \n[27] \"dist_park_ft\"              \"dist_park_mi\"             \n[29] \"dist_school_public_ft\"     \"dist_school_charter_ft\"   \n[31] \"dist_school_private_ft\"    \"dist_foodretail_ft\"       \n[33] \"foodretail_1mi_count\"      \"dist_police_ft\"           \n[35] \"dist_fire_ft\"              \"lu2_code\"                 \n[37] \"GEOID\"                     \"pct_white\"                \n[39] \"pct_black\"                 \"pct_hispanic\"             \n[41] \"pct_asian\"                 \"pct_age_25_44\"            \n[43] \"pct_age_45_64\"             \"pct_age_65plus\"           \n[45] \"vacancy_rate\"              \"med_gross_rent\"           \n[47] \"med_hh_income\"             \"poverty_rate\"             \n[49] \"pct_ba_plus\"               \"pct_hs_only\"              \n[51] \"unemployment_rate\"         \"psf\"                      \n[53] \"value_multiple\"           \n\n\n\n# Histogram of Price\nggplot(sales_data, aes(x = sale_price)) +\n  geom_histogram(fill = \"lightblue\", bins= 500) +\n  labs(title = \"Histogram of Price\", x = \"Price\", y = \"Frequency\")\n\n\n\n\n\n\n\n# Log Sales Price\nlog_sales_data &lt;- mutate(sales_data, log_price = log(sale_price))\nggplot(log_sales_data, aes(x = log_price)) +\n  geom_histogram(fill = \"lightblue\", bins= 500) +\n  labs(title = \"Histogram of Price\", x = \"Log(Price)\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n# Further Cleaning\nselect_data &lt;- sales_data |&gt; \n  select(year_built, number_of_bathrooms, number_of_bedrooms, basements, \n         fireplaces, garage_spaces,\n         general_construction, interior_condition, exterior_condition, \n         census_tract, neigh_name, \n         value_multiple, market_value, sale_price, total_area, total_livable_area, \n         psf, violent_2blocks, petty_1block, dist_park_mi, \n         vacancy_rate, med_gross_rent, med_hh_income, pct_ba_plus, \n         pct_white, pct_black, pct_hispanic, pct_asian,\n         foodretail_1mi_count, \n         dist_school_public_ft, dist_school_charter_ft, \n         pct_age_25_44, pct_age_65plus,\n         poverty_rate, unemployment_rate) |&gt; \n  filter(number_of_bathrooms &gt; 0, number_of_bedrooms &gt; 0) |&gt; \n  mutate(age = 2024 - year_built,\n         basements = dplyr::recode(trimws(as.character(basements)),\n                              \"0\"=\"0\",\"A\"=\"1\",\"B\"=\"2\",\"C\"=\"3\",\"D\"=\"4\",\"E\"=\"5\",\n                              \"F\"=\"6\",\"G\"=\"7\",\"H\"=\"8\",\"I\"=\"9\"),\n         basements = as.integer(basements)) |&gt; \n  rename(\n    baths      = number_of_bathrooms,\n    beds       = number_of_bedrooms,\n    construction = general_construction,\n    interior   = interior_condition,\n    exterior   = exterior_condition,\n    garage     = garage_spaces,\n    tract      = census_tract,\n    neigh      = neigh_name,\n    mkt_value  = market_value,\n    area       = total_area,\n    liv_area   = total_livable_area\n  )\n\n# Intuition: market value should not deviate substantially \n# from the sale price\nvalue_range &lt;- select_data |&gt; \n  summarize(\n    q1 = quantile(value_multiple, 0.25, na.rm = TRUE),\n    q3 = quantile(value_multiple, 0.75, na.rm = TRUE),\n    iqr = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr)\nvalue_range\n\n# A tibble: 1 × 5\n     q1    q3   iqr lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.87  1.16  0.29 0.435  1.59\n\n# Intuition: psf outliers are likely non-market transactions\npsf_range &lt;- select_data |&gt; \n  summarize(\n    q1 = quantile(psf, 0.25, na.rm = TRUE),\n    q3 = quantile(psf, 0.75, na.rm = TRUE),\n    iqr = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr)\npsf_range  \n\n# A tibble: 1 × 5\n     q1    q3   iqr lower upper\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  132.  267.  135. -70.6  469.\n\nclean_data &lt;- select_data |&gt; \n  filter(\n    liv_area &gt; 500,\n    area &gt; 500,\n    between(psf, 40, 800),\n    between(value_multiple, 0.4, 2.0)\n  ) |&gt; \n  mutate(across(where(is.numeric), ~ round(.x, 2))) |&gt; \n  drop_na() |&gt; \n  arrange(liv_area)\nclean_data\n\n# A tibble: 17,174 × 36\n   year_built baths  beds basements fireplaces garage construction interior\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1       1920     1     1         4          0      0 A                   2\n 2       1920     1     2         3          0      0 A                   4\n 3       1915     1     1         4          0      0 A                   4\n 4       1920     1     2         5          0      0 A                   2\n 5       1920     1     2         3          0      0 A                   3\n 6       1920     1     2         1          0      0 A                   2\n 7       1920     1     2         1          0      0 A                   2\n 8       1920     1     2         2          0      0 A                   2\n 9       1875     1     2         3          0      0 A                   4\n10       1875     1     2         3          0      0 A                   3\n# ℹ 17,164 more rows\n# ℹ 28 more variables: exterior &lt;dbl&gt;, tract &lt;dbl&gt;, neigh &lt;chr&gt;,\n#   value_multiple &lt;dbl&gt;, mkt_value &lt;dbl&gt;, sale_price &lt;dbl&gt;, area &lt;dbl&gt;,\n#   liv_area &lt;dbl&gt;, psf &lt;dbl&gt;, violent_2blocks &lt;dbl&gt;, petty_1block &lt;dbl&gt;,\n#   dist_park_mi &lt;dbl&gt;, vacancy_rate &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;,\n#   med_hh_income &lt;dbl&gt;, pct_ba_plus &lt;dbl&gt;, pct_white &lt;dbl&gt;, pct_black &lt;dbl&gt;,\n#   pct_hispanic &lt;dbl&gt;, pct_asian &lt;dbl&gt;, foodretail_1mi_count &lt;dbl&gt;, …\n\n\n\n# Quick Category Check \ncount(clean_data, neigh)\n\n# A tibble: 144 × 2\n   neigh                n\n   &lt;chr&gt;            &lt;int&gt;\n 1 ACADEMY_GARDENS     65\n 2 ALLEGHENY_WEST     160\n 3 ANDORRA             38\n 4 ASTON_WOODBRIDGE    43\n 5 BARTRAM_VILLAGE     14\n 6 BELLA_VISTA         80\n 7 BELMONT             18\n 8 BLUE_BELL_HILL       5\n 9 BREWERYTOWN        147\n10 BRIDESBURG         114\n# ℹ 134 more rows\n\ncount(clean_data, interior)\n\n# A tibble: 7 × 2\n  interior     n\n     &lt;dbl&gt; &lt;int&gt;\n1        1   362\n2        2  3058\n3        3  3831\n4        4  9774\n5        5   103\n6        6    24\n7        7    22\n\ncount(clean_data, exterior)\n\n# A tibble: 7 × 2\n  exterior     n\n     &lt;dbl&gt; &lt;int&gt;\n1        1   368\n2        2   645\n3        3  1833\n4        4 14122\n5        5   161\n6        6    12\n7        7    33\n\ncount(clean_data, basements)\n\n# A tibble: 10 × 2\n   basements     n\n       &lt;dbl&gt; &lt;int&gt;\n 1         0   268\n 2         1  3868\n 3         2   814\n 4         3  4234\n 5         4  3046\n 6         5  1706\n 7         6  1810\n 8         7   483\n 9         8   915\n10         9    30\n\n\nMetadata: Interior and Exterior Condition\n\nNewer Construction\nRehabilitated\nAbove Average\nRehabilitated\nAverage\nBelow Average\nVacant\n\nMetadata: Basements\n\nNone\nA = Full Finished\nB = Full Semi-Finished\nC = Full Unfinished\nD = Full\nE = Partial Finished\nF = Partial Semi-Finished\nG = Partial Unfinished\nH = Partial - Unknown Finish\nI = Unknown Size - Finished\n\n\n\nStep 5: Summary Statistics\n\n# Calculate Summary Stats\nstats_df &lt;- clean_data %&gt;%\n  summarise(across(\n    where(is.numeric),\n    list(\n      mean   = ~ mean(.x, na.rm = TRUE),\n      median = ~ median(.x, na.rm = TRUE),\n      sd     = ~ sd(.x, na.rm = TRUE),\n      q25    = ~ as.numeric(quantile(.x, 0.25, na.rm = TRUE)),\n      q75    = ~ as.numeric(quantile(.x, 0.75, na.rm = TRUE))\n    ),\n    .names = \"{.col}__{.fn}\"\n  )) %&gt;%\n  pivot_longer(everything(),\n               names_to = c(\"variable\",\"stat\"),\n               names_sep = \"__\",\n               values_to = \"value\") %&gt;%\n  pivot_wider(names_from = stat, values_from = value) %&gt;%\n  mutate(across(-variable, ~ round(.x, 2))) %&gt;%\n  arrange(variable)\n\n# Table Output\nlibrary(kableExtra)\nkbl(stats_df,\n    caption   = \"Summary Statistics\",\n    col.names = c(\"Variable\",\"Mean\",\"Median\",\"SD\",\"Q25\",\"Q75\"),\n    align     = c(\"l\",\"r\",\"r\",\"r\",\"r\",\"r\"),\n    booktabs  = TRUE\n) %&gt;%\n  kable_styling(full_width = FALSE,\n                bootstrap_options = c(\"striped\",\"hover\",\"condensed\")) %&gt;%\n  add_header_above(c(\" \" = 1, \"Statistics\" = 5))\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nVariable\nMean\nMedian\nSD\nQ25\nQ75\n\n\n\n\nage\n87.43\n99.00\n28.15\n74.00\n104.00\n\n\narea\n1968.09\n1226.00\n4042.11\n896.00\n1845.75\n\n\nbasements\n3.54\n3.00\n2.04\n2.00\n5.00\n\n\nbaths\n1.46\n1.00\n0.69\n1.00\n2.00\n\n\nbeds\n3.09\n3.00\n0.72\n3.00\n3.00\n\n\ndist_park_mi\n0.16\n0.14\n0.11\n0.08\n0.22\n\n\ndist_school_charter_ft\n3387.13\n2676.00\n2520.44\n1704.38\n4255.10\n\n\ndist_school_public_ft\n1531.73\n1362.89\n902.98\n894.00\n1945.96\n\n\nexterior\n3.77\n4.00\n0.65\n4.00\n4.00\n\n\nfireplaces\n0.07\n0.00\n0.35\n0.00\n0.00\n\n\nfoodretail_1mi_count\n44.55\n45.00\n20.61\n30.00\n57.00\n\n\ngarage\n0.37\n0.00\n0.54\n0.00\n1.00\n\n\ninterior\n3.37\n4.00\n0.87\n3.00\n4.00\n\n\nliv_area\n1379.56\n1224.00\n579.84\n1076.00\n1500.00\n\n\nmed_gross_rent\n1368.47\n1320.00\n312.66\n1155.00\n1561.00\n\n\nmed_hh_income\n69281.93\n63628.00\n30345.38\n46492.00\n86222.00\n\n\nmkt_value\n294129.43\n244000.00\n228503.89\n166725.00\n341075.00\n\n\npct_age_25_44\n34.08\n30.18\n11.98\n26.18\n40.20\n\n\npct_age_65plus\n13.76\n12.59\n6.78\n8.33\n17.38\n\n\npct_asian\n7.29\n4.58\n8.53\n1.08\n10.05\n\n\npct_ba_plus\n34.37\n28.22\n22.78\n15.84\n51.34\n\n\npct_black\n33.57\n20.15\n31.87\n6.56\n54.57\n\n\npct_hispanic\n15.51\n8.68\n17.81\n4.48\n18.80\n\n\npct_white\n38.82\n36.08\n29.69\n8.50\n67.45\n\n\npetty_1block\n29.09\n25.00\n29.48\n15.00\n36.00\n\n\npoverty_rate\n19.63\n17.26\n12.56\n10.30\n26.58\n\n\npsf\n213.18\n200.22\n101.73\n142.00\n266.67\n\n\nsale_price\n304050.36\n250000.00\n243673.32\n170000.00\n359000.00\n\n\ntract\n200.52\n202.00\n118.79\n85.00\n314.00\n\n\nunemployment_rate\n8.38\n7.19\n5.70\n3.89\n11.90\n\n\nvacancy_rate\n8.85\n7.84\n5.74\n4.77\n11.90\n\n\nvalue_multiple\n1.01\n0.99\n0.26\n0.86\n1.12\n\n\nviolent_2blocks\n44.85\n35.00\n39.37\n17.00\n61.00\n\n\nyear_built\n1936.57\n1925.00\n28.15\n1920.00\n1950.00\n\n\n\n\n\nCorrelation Matrix\n\nhedonic_vars &lt;- clean_data[, c(\"age\", \"baths\", \"beds\", \"basements\", \"interior\", \"exterior\", \"area\", \"liv_area\")]\n\ncor_matrix &lt;- cor(hedonic_vars, use = \"pairwise.complete.obs\", method = \"pearson\")\n\nggcorrplot(cor_matrix,\n           method = \"square\",\n           type = \"lower\",\n           lab = TRUE,\n           lab_size = 3,\n           colors = c(\"#a4133c\", \"white\", \"#a4133c\"))+\n    labs(title = \"Correlation Matrix for Hedonic Variables\") +\n    theme(plot.subtitle = element_text(size = 9, face = \"italic\"),\n        plot.title = element_text(size = 12, face = \"bold\"),\n        axis.text.x = element_text(size = 7),\n        axis.text.y = element_text(size = 7),\n        axis.title = element_text(size = 8))\n\n\n\n\n\n\n\n\n\nspatial_vars &lt;- clean_data[, c(\n  \"violent_2blocks\", \"petty_1block\", \"dist_park_mi\",\n  \"vacancy_rate\", \"med_gross_rent\", \"med_hh_income\", \"pct_ba_plus\",\n  \"pct_white\", \"pct_black\", \"pct_hispanic\", \"pct_asian\",\n  \"foodretail_1mi_count\",\n  \"dist_school_public_ft\", \"dist_school_charter_ft\",\n  \"pct_age_25_44\", \"pct_age_65plus\",\n  \"poverty_rate\", \"unemployment_rate\"\n)]\n\nspatial_matrix &lt;- cor(spatial_vars, use = \"pairwise.complete.obs\", method = \"pearson\")\n\nggcorrplot(spatial_matrix,\n           method = \"square\",\n           type = \"lower\",\n           lab = TRUE,\n           lab_size = 3,\n           colors = c(\"#a4133c\", \"white\", \"#a4133c\"))+\n    labs(title = \"Correlation Matrix for Spatial Variables\") +\n    theme(plot.subtitle = element_text(size = 9, face = \"italic\"),\n        plot.title = element_text(size = 12, face = \"bold\"),\n        axis.text.x = element_text(size = 7),\n        axis.text.y = element_text(size = 7),\n        axis.title = element_text(size = 8))"
  },
  {
    "objectID": "assignments/midterm/midterm-final-model.html#part-2-model-building-evaluation-using-cross-validation",
    "href": "assignments/midterm/midterm-final-model.html#part-2-model-building-evaluation-using-cross-validation",
    "title": "MUSA 5080 Midterm",
    "section": "Part 2: Model Building & Evaluation using Cross-Validation",
    "text": "Part 2: Model Building & Evaluation using Cross-Validation\n\nEvaluate the model fit and predictive reliability using key metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared.\n\nclean_data$neigh &lt;- as.factor(clean_data$neigh)\nclean_data$interior &lt;- as.factor(clean_data$interior)\nclean_data$exterior &lt;- as.factor(clean_data$exterior)\nclean_data$basements &lt;- as.factor(clean_data$basements)\n                                 \ncount(clean_data, neigh)\n\n# A tibble: 144 × 2\n   neigh                n\n   &lt;fct&gt;            &lt;int&gt;\n 1 ACADEMY_GARDENS     65\n 2 ALLEGHENY_WEST     160\n 3 ANDORRA             38\n 4 ASTON_WOODBRIDGE    43\n 5 BARTRAM_VILLAGE     14\n 6 BELLA_VISTA         80\n 7 BELMONT             18\n 8 BLUE_BELL_HILL       5\n 9 BREWERYTOWN        147\n10 BRIDESBURG         114\n# ℹ 134 more rows\n\ncount(clean_data, interior)\n\n# A tibble: 7 × 2\n  interior     n\n  &lt;fct&gt;    &lt;int&gt;\n1 1          362\n2 2         3058\n3 3         3831\n4 4         9774\n5 5          103\n6 6           24\n7 7           22\n\ncount(clean_data, exterior)\n\n# A tibble: 7 × 2\n  exterior     n\n  &lt;fct&gt;    &lt;int&gt;\n1 1          368\n2 2          645\n3 3         1833\n4 4        14122\n5 5          161\n6 6           12\n7 7           33\n\ncount(clean_data, basements)\n\n# A tibble: 10 × 2\n   basements     n\n   &lt;fct&gt;     &lt;int&gt;\n 1 0           268\n 2 1          3868\n 3 2           814\n 4 3          4234\n 5 4          3046\n 6 5          1706\n 7 6          1810\n 8 7           483\n 9 8           915\n10 9            30\n\n\n\nset.seed(5080) # Set seed for reproducibility\nctrl &lt;- trainControl(method = \"cv\", number = 10) #10-Fold CV\n\n# Model 1: Structural\ncv_m1 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2),\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft\n  + vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white\n  + poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus,\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Neighborhood Fixed Effects\ncv_m3 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus +\n  neigh, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 4: + Small Neighborhoods + Mkt Value\nclean_data &lt;- clean_data |&gt; \n  add_count(neigh) |&gt; \n  mutate(\n    neigh_cv = if_else(\n      n &lt; 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(neigh)           # Keep original\n    ),\n    neigh_cv = as.factor(neigh_cv)\n  )\n\ncv_m4 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  violent_2blocks + dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus + \n  mkt_value +\n  neigh_cv, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 5: + Simpler + Mkt Value\ncv_m5 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  mkt_value +\n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 6: Simpler Only\ncv_m6 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Compare\ndata.frame(\n  Model = c(\"Structural\", \"Spatial\", \"Fixed Effects\", \"Small Neighborhoods + Mkt Value\", \"Simpler + Mkt Value\", \"Simpler Only\"),\n  Rsquared = c(cv_m1$results$Rsquared, cv_m2$results$Rsquared, cv_m3$results$Rsquared, \n               cv_m4$results$Rsquared, cv_m5$results$Rsquared, cv_m6$results$Rsquared),\n  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE, \n           cv_m4$results$RMSE, cv_m5$results$RMSE, cv_m6$results$RMSE),\n  MAE = c(cv_m1$results$MAE, cv_m2$results$MAE, cv_m3$results$MAE, \n          cv_m4$results$MAE, cv_m5$results$MAE, cv_m6$results$MAE)\n)\n\n                            Model  Rsquared      RMSE      MAE\n1                      Structural 0.6734982 139097.27 91009.99\n2                         Spatial 0.7910435 111374.22 67559.44\n3                   Fixed Effects 0.8489664  95369.77 56768.27\n4 Small Neighborhoods + Mkt Value 0.9266800  65382.05 42496.50\n5             Simpler + Mkt Value 0.9282046  65402.58 42527.28\n6                    Simpler Only 0.8415987  96901.51 58377.83\n\n\n\nlm_model3 &lt;- cv_m3$finalModel\nvif_model3 &lt;- vif(lm_model3)\nvif_m3 &lt;- data.frame(Variable = names(vif_model3), # convert to data frame\n                     VIF = as.vector(vif_model3))\nkable(vif_m3, caption = \"Variance Inflation Factors for Model 3: Fixed Effects\")\n\n\nVariance Inflation Factors for Model 3: Fixed Effects\n\n\nVariable\nVIF\n\n\n\n\nbeds\n2.049265\n\n\nbaths\n2.214022\n\n\nbasements1\n14.301690\n\n\nbasements2\n4.494605\n\n\nbasements3\n15.741349\n\n\nbasements4\n12.630752\n\n\nbasements5\n7.769069\n\n\nbasements6\n8.457106\n\n\nbasements7\n3.162235\n\n\nbasements8\n4.963679\n\n\nbasements9\n1.148218\n\n\ninterior2\n433.595343\n\n\ninterior3\n511.190303\n\n\ninterior4\n725.322963\n\n\ninterior5\n18.775469\n\n\ninterior6\n5.373467\n\n\ninterior7\n6.004538\n\n\nexterior2\n107.584125\n\n\nexterior3\n280.208816\n\n\nexterior4\n430.490151\n\n\nexterior5\n28.543354\n\n\nexterior6\n3.252689\n\n\nexterior7\n7.935073\n\n\nliv_area\n2.840232\n\n\npoly(age, 2)1\n2.474318\n\n\npoly(age, 2)2\n2.113020\n\n\ndist_park_mi\n1.449398\n\n\nfoodretail_1mi_count\n10.018463\n\n\ndist_school_public_ft\n1.949979\n\n\nvacancy_rate\n3.542389\n\n\nmed_gross_rent\n4.496201\n\n\nmed_hh_income\n9.248076\n\n\npct_ba_plus\n19.605835\n\n\npct_black\n21.261745\n\n\npct_white\n19.614209\n\n\npoverty_rate\n5.199561\n\n\nunemployment_rate\n2.417667\n\n\npct_age_25_44\n6.518760\n\n\npct_age_65plus\n3.712982\n\n\nneighALLEGHENY_WEST\n4.682976\n\n\nneighANDORRA\n1.729573\n\n\nneighASTON_WOODBRIDGE\n1.754919\n\n\nneighBARTRAM_VILLAGE\n1.324190\n\n\nneighBELLA_VISTA\n3.019564\n\n\nneighBELMONT\n1.433535\n\n\nneighBLUE_BELL_HILL\n1.126132\n\n\nneighBREWERYTOWN\n4.323633\n\n\nneighBRIDESBURG\n2.884220\n\n\nneighBURHOLME\n1.378201\n\n\nneighBUSTLETON\n4.917405\n\n\nneighCARROLL_PARK\n4.085431\n\n\nneighCEDAR_PARK\n3.092098\n\n\nneighCEDARBROOK\n3.928959\n\n\nneighCHESTNUT_HILL\n4.253643\n\n\nneighCLEARVIEW\n1.709857\n\n\nneighCOBBS_CREEK\n9.434200\n\n\nneighCRESCENTVILLE\n1.088240\n\n\nneighDEARNLEY_PARK\n1.207802\n\n\nneighDICKINSON_NARROWS\n4.163236\n\n\nneighDUNLAP\n1.562388\n\n\nneighEAST_FALLS\n4.717792\n\n\nneighEAST_KENSINGTON\n4.738747\n\n\nneighEAST_OAK_LANE\n2.392539\n\n\nneighEAST_PARK\n1.023240\n\n\nneighEAST_PARKSIDE\n1.553629\n\n\nneighEAST_PASSYUNK\n5.950768\n\n\nneighEAST_POPLAR\n1.068445\n\n\nneighEASTWICK\n1.626392\n\n\nneighELMWOOD\n5.022162\n\n\nneighFAIRHILL\n1.495718\n\n\nneighFAIRMOUNT\n6.092245\n\n\nneighFELTONVILLE\n3.739039\n\n\nneighFERN_ROCK\n1.742725\n\n\nneighFISHTOWN\n11.059002\n\n\nneighFITLER_SQUARE\n1.729380\n\n\nneighFOX_CHASE\n4.434731\n\n\nneighFRANCISVILLE\n2.296713\n\n\nneighFRANKFORD\n5.117177\n\n\nneighFRANKLINVILLE\n1.937673\n\n\nneighGARDEN_COURT\n1.372308\n\n\nneighGERMANTOWN_EAST\n3.434410\n\n\nneighGERMANTOWN_MORTON\n2.171998\n\n\nneighGERMANTOWN_PENN_KNOX\n1.248988\n\n\nneighGERMANTOWN_SOUTHWEST\n2.540385\n\n\nneighGERMANTOWN_WEST_CENT\n1.974975\n\n\nneighGERMANTOWN_WESTSIDE\n1.411555\n\n\nneighGERMANY_HILL\n2.167678\n\n\nneighGIRARD_ESTATES\n4.573653\n\n\nneighGLENWOOD\n2.036976\n\n\nneighGRADUATE_HOSPITAL\n8.437837\n\n\nneighGRAYS_FERRY\n6.580249\n\n\nneighGREENWICH\n2.222646\n\n\nneighHADDINGTON\n5.933569\n\n\nneighHARROWGATE\n5.564070\n\n\nneighHARTRANFT\n2.471315\n\n\nneighHAVERFORD_NORTH\n1.319760\n\n\nneighHAWTHORNE\n2.105040\n\n\nneighHOLMESBURG\n6.727475\n\n\nneighHUNTING_PARK\n3.499835\n\n\nneighJUNIATA_PARK\n5.201612\n\n\nneighKINGSESSING\n6.253649\n\n\nneighLAWNDALE\n5.408713\n\n\nneighLEXINGTON_PARK\n1.831462\n\n\nneighLOGAN\n4.799513\n\n\nneighLOGAN_SQUARE\n1.528557\n\n\nneighLOWER_MOYAMENSING\n6.782643\n\n\nneighLUDLOW\n1.088475\n\n\nneighMANAYUNK\n5.774279\n\n\nneighMANTUA\n2.021614\n\n\nneighMAYFAIR\n11.766631\n\n\nneighMCGUIRE\n1.400103\n\n\nneighMECHANICSVILLE\n1.038126\n\n\nneighMELROSE_PARK_GARDENS\n1.823393\n\n\nneighMILL_CREEK\n2.349579\n\n\nneighMILLBROOK\n2.398227\n\n\nneighMODENA\n3.418338\n\n\nneighMORRELL_PARK\n3.112724\n\n\nneighMOUNT_AIRY_EAST\n5.973708\n\n\nneighMOUNT_AIRY_WEST\n4.712264\n\n\nneighNEWBOLD\n4.188107\n\n\nneighNICETOWN\n1.255634\n\n\nneighNORMANDY_VILLAGE\n1.215620\n\n\nneighNORTH_CENTRAL\n2.968110\n\n\nneighNORTHERN_LIBERTIES\n3.731773\n\n\nneighNORTHWOOD\n2.190233\n\n\nneighOGONTZ\n3.559671\n\n\nneighOLD_CITY\n1.291230\n\n\nneighOLD_KENSINGTON\n3.024091\n\n\nneighOLNEY\n8.078969\n\n\nneighOVERBROOK\n6.772090\n\n\nneighOXFORD_CIRCLE\n12.558674\n\n\nneighPACKER_PARK\n1.850966\n\n\nneighPARKWOOD_MANOR\n4.730816\n\n\nneighPASCHALL\n3.254948\n\n\nneighPASSYUNK_SQUARE\n5.041187\n\n\nneighPENNSPORT\n3.749722\n\n\nneighPENNYPACK\n2.970297\n\n\nneighPENNYPACK_WOODS\n1.373448\n\n\nneighPENROSE\n1.844699\n\n\nneighPOINT_BREEZE\n13.274515\n\n\nneighPOWELTON\n1.170423\n\n\nneighQUEEN_VILLAGE\n3.163962\n\n\nneighRHAWNHURST\n4.316231\n\n\nneighRICHMOND\n10.008276\n\n\nneighRITTENHOUSE\n3.477658\n\n\nneighRIVERFRONT\n1.055313\n\n\nneighROXBOROUGH\n5.747575\n\n\nneighROXBOROUGH_PARK\n1.461860\n\n\nneighSHARSWOOD\n1.357025\n\n\nneighSOCIETY_HILL\n2.992026\n\n\nneighSOMERTON\n5.688279\n\n\nneighSOUTHWEST_SCHUYLKILL\n3.296779\n\n\nneighSPRING_GARDEN\n2.370461\n\n\nneighSPRUCE_HILL\n1.949239\n\n\nneighSTADIUM_DISTRICT\n2.144494\n\n\nneighSTANTON\n4.070961\n\n\nneighSTRAWBERRY_MANSION\n3.887767\n\n\nneighSUMMERDALE\n3.192886\n\n\nneighTACONY\n5.456290\n\n\nneighTIOGA\n2.857180\n\n\nneighTORRESDALE\n2.599110\n\n\nneighUNIVERSITY_CITY\n1.040846\n\n\nneighUPPER_KENSINGTON\n5.669766\n\n\nneighUPPER_ROXBOROUGH\n3.483232\n\n\nneighWALNUT_HILL\n1.856734\n\n\nneighWASHINGTON_SQUARE\n2.668939\n\n\nneighWEST_KENSINGTON\n3.310909\n\n\nneighWEST_OAK_LANE\n10.682921\n\n\nneighWEST_PARKSIDE\n1.097074\n\n\nneighWEST_PASSYUNK\n5.442566\n\n\nneighWEST_POPLAR\n1.286419\n\n\nneighWEST_POWELTON\n1.462303\n\n\nneighWHITMAN\n4.043836\n\n\nneighWINCHESTER_PARK\n1.438090\n\n\nneighWISSAHICKON\n3.143050\n\n\nneighWISSAHICKON_HILLS\n1.528008\n\n\nneighWISSINOMING\n7.531934\n\n\nneighWISTER\n1.774620\n\n\nneighWOODLAND_TERRACE\n1.169262\n\n\nneighWYNNEFIELD\n3.978670\n\n\nneighWYNNEFIELD_HEIGHTS\n1.255465\n\n\nneighYORKTOWN\n1.265892\n\n\n\n\n\n\nlm_model5 &lt;- cv_m5$finalModel\nvif_model5 &lt;- vif(lm_model5)\nvif_m5 &lt;- data.frame(Variable = names(vif_model5), # convert to data frame\n                     VIF = as.vector(vif_model5))\nkable(vif_m5, caption = \"Variance Inflation Factors for Model 5: Simpler + Mkt Value\")\n\n\nVariance Inflation Factors for Model 5: Simpler + Mkt Value\n\n\nVariable\nVIF\n\n\n\n\nbeds\n2.079469\n\n\nbaths\n2.217788\n\n\nbasements1\n14.222509\n\n\nbasements2\n4.474744\n\n\nbasements3\n15.660980\n\n\nbasements4\n12.568746\n\n\nbasements5\n7.734044\n\n\nbasements6\n8.411127\n\n\nbasements7\n3.146126\n\n\nbasements8\n4.939574\n\n\nbasements9\n1.146520\n\n\ninterior2\n433.428669\n\n\ninterior3\n511.003786\n\n\ninterior4\n725.126130\n\n\ninterior5\n18.777399\n\n\ninterior6\n5.373677\n\n\ninterior7\n6.003274\n\n\nexterior2\n107.549402\n\n\nexterior3\n280.161895\n\n\nexterior4\n430.473573\n\n\nexterior5\n28.540147\n\n\nexterior6\n3.250687\n\n\nexterior7\n7.933660\n\n\nliv_area\n5.104796\n\n\npoly(age, 2)1\n2.469017\n\n\npoly(age, 2)2\n2.099108\n\n\ndist_park_mi\n1.429027\n\n\nfoodretail_1mi_count\n9.095552\n\n\ndist_school_public_ft\n1.876059\n\n\nmkt_value\n7.768525\n\n\nneigh_cvALLEGHENY_WEST\n3.661075\n\n\nneigh_cvANDORRA\n1.620863\n\n\nneigh_cvASTON_WOODBRIDGE\n1.688578\n\n\nneigh_cvBARTRAM_VILLAGE\n1.232886\n\n\nneigh_cvBELLA_VISTA\n2.648721\n\n\nneigh_cvBELMONT\n1.313586\n\n\nneigh_cvBREWERYTOWN\n3.542230\n\n\nneigh_cvBRIDESBURG\n2.820062\n\n\nneigh_cvBURHOLME\n1.329891\n\n\nneigh_cvBUSTLETON\n4.535277\n\n\nneigh_cvCARROLL_PARK\n3.274566\n\n\nneigh_cvCEDAR_PARK\n2.637660\n\n\nneigh_cvCEDARBROOK\n2.787442\n\n\nneigh_cvCHESTNUT_HILL\n3.339501\n\n\nneigh_cvCLEARVIEW\n1.462231\n\n\nneigh_cvCOBBS_CREEK\n7.549782\n\n\nneigh_cvDEARNLEY_PARK\n1.180732\n\n\nneigh_cvDICKINSON_NARROWS\n3.760063\n\n\nneigh_cvDUNLAP\n1.414517\n\n\nneigh_cvEAST_FALLS\n3.614578\n\n\nneigh_cvEAST_KENSINGTON\n4.034785\n\n\nneigh_cvEAST_OAK_LANE\n2.021801\n\n\nneigh_cvEAST_PARKSIDE\n1.391240\n\n\nneigh_cvEAST_PASSYUNK\n5.400342\n\n\nneigh_cvEASTWICK\n1.408075\n\n\nneigh_cvELMWOOD\n3.929218\n\n\nneigh_cvFAIRHILL\n1.401604\n\n\nneigh_cvFAIRMOUNT\n4.800691\n\n\nneigh_cvFELTONVILLE\n3.276236\n\n\nneigh_cvFERN_ROCK\n1.553823\n\n\nneigh_cvFISHTOWN\n9.366947\n\n\nneigh_cvFITLER_SQUARE\n1.594588\n\n\nneigh_cvFOX_CHASE\n4.204728\n\n\nneigh_cvFRANCISVILLE\n1.969606\n\n\nneigh_cvFRANKFORD\n4.669218\n\n\nneigh_cvFRANKLINVILLE\n1.801362\n\n\nneigh_cvGARDEN_COURT\n1.297023\n\n\nneigh_cvGERMANTOWN_EAST\n2.774685\n\n\nneigh_cvGERMANTOWN_MORTON\n1.866439\n\n\nneigh_cvGERMANTOWN_PENN_KNOX\n1.193124\n\n\nneigh_cvGERMANTOWN_SOUTHWEST\n2.126860\n\n\nneigh_cvGERMANTOWN_WEST_CENT\n1.719776\n\n\nneigh_cvGERMANTOWN_WESTSIDE\n1.304456\n\n\nneigh_cvGERMANY_HILL\n1.864648\n\n\nneigh_cvGIRARD_ESTATES\n4.409647\n\n\nneigh_cvGLENWOOD\n1.776352\n\n\nneigh_cvGRADUATE_HOSPITAL\n6.765058\n\n\nneigh_cvGRAYS_FERRY\n5.620112\n\n\nneigh_cvGREENWICH\n2.031752\n\n\nneigh_cvHADDINGTON\n4.733530\n\n\nneigh_cvHARROWGATE\n4.776293\n\n\nneigh_cvHARTRANFT\n2.274215\n\n\nneigh_cvHAVERFORD_NORTH\n1.238809\n\n\nneigh_cvHAWTHORNE\n1.902921\n\n\nneigh_cvHOLMESBURG\n6.204381\n\n\nneigh_cvHUNTING_PARK\n3.088752\n\n\nneigh_cvJUNIATA_PARK\n4.510213\n\n\nneigh_cvKINGSESSING\n5.023030\n\n\nneigh_cvLAWNDALE\n4.695615\n\n\nneigh_cvLEXINGTON_PARK\n1.740326\n\n\nneigh_cvLOGAN\n3.852107\n\n\nneigh_cvLOGAN_SQUARE\n1.419454\n\n\nneigh_cvLOWER_MOYAMENSING\n6.288348\n\n\nneigh_cvMANAYUNK\n4.719119\n\n\nneigh_cvMANTUA\n1.752579\n\n\nneigh_cvMAYFAIR\n10.856754\n\n\nneigh_cvMCGUIRE\n1.321714\n\n\nneigh_cvMELROSE_PARK_GARDENS\n1.542063\n\n\nneigh_cvMILL_CREEK\n2.026251\n\n\nneigh_cvMILLBROOK\n2.337292\n\n\nneigh_cvMODENA\n3.364393\n\n\nneigh_cvMORRELL_PARK\n2.998080\n\n\nneigh_cvMOUNT_AIRY_EAST\n4.535431\n\n\nneigh_cvMOUNT_AIRY_WEST\n3.538709\n\n\nneigh_cvNEWBOLD\n3.792868\n\n\nneigh_cvNORMANDY_VILLAGE\n1.200600\n\n\nneigh_cvNORTH_CENTRAL\n2.671706\n\n\nneigh_cvNORTHERN_LIBERTIES\n2.952639\n\n\nneigh_cvNORTHWOOD\n2.019738\n\n\nneigh_cvOGONTZ\n2.971028\n\n\nneigh_cvOLD_CITY\n1.250390\n\n\nneigh_cvOLD_KENSINGTON\n2.574254\n\n\nneigh_cvOLNEY\n6.886447\n\n\nneigh_cvOVERBROOK\n5.008870\n\n\nneigh_cvOXFORD_CIRCLE\n11.088613\n\n\nneigh_cvPACKER_PARK\n1.784012\n\n\nneigh_cvPARKWOOD_MANOR\n4.605025\n\n\nneigh_cvPASCHALL\n2.727475\n\n\nneigh_cvPASSYUNK_SQUARE\n4.509742\n\n\nneigh_cvPENNSPORT\n3.369305\n\n\nneigh_cvPENNYPACK\n2.798121\n\n\nneigh_cvPENNYPACK_WOODS\n1.343561\n\n\nneigh_cvPENROSE\n1.609110\n\n\nneigh_cvPOINT_BREEZE\n10.964432\n\n\nneigh_cvQUEEN_VILLAGE\n2.691401\n\n\nneigh_cvRHAWNHURST\n3.995112\n\n\nneigh_cvRICHMOND\n9.510735\n\n\nneigh_cvRITTENHOUSE\n3.154039\n\n\nneigh_cvROXBOROUGH\n5.139763\n\n\nneigh_cvROXBOROUGH_PARK\n1.387774\n\n\nneigh_cvSHARSWOOD\n1.255374\n\n\nneigh_cvSmall_Neighborhoods\n1.812647\n\n\nneigh_cvSOCIETY_HILL\n2.522007\n\n\nneigh_cvSOMERTON\n5.278331\n\n\nneigh_cvSOUTHWEST_SCHUYLKILL\n2.754400\n\n\nneigh_cvSPRING_GARDEN\n2.034743\n\n\nneigh_cvSPRUCE_HILL\n1.695846\n\n\nneigh_cvSTADIUM_DISTRICT\n2.069908\n\n\nneigh_cvSTANTON\n3.281597\n\n\nneigh_cvSTRAWBERRY_MANSION\n3.128114\n\n\nneigh_cvSUMMERDALE\n2.751373\n\n\nneigh_cvTACONY\n5.249567\n\n\nneigh_cvTIOGA\n2.331449\n\n\nneigh_cvTORRESDALE\n2.476609\n\n\nneigh_cvUPPER_KENSINGTON\n4.897645\n\n\nneigh_cvUPPER_ROXBOROUGH\n3.098878\n\n\nneigh_cvWALNUT_HILL\n1.689462\n\n\nneigh_cvWASHINGTON_SQUARE\n2.334592\n\n\nneigh_cvWEST_KENSINGTON\n2.950022\n\n\nneigh_cvWEST_OAK_LANE\n7.652986\n\n\nneigh_cvWEST_PASSYUNK\n4.729158\n\n\nneigh_cvWEST_POPLAR\n1.220534\n\n\nneigh_cvWEST_POWELTON\n1.355769\n\n\nneigh_cvWHITMAN\n3.831367\n\n\nneigh_cvWINCHESTER_PARK\n1.403076\n\n\nneigh_cvWISSAHICKON\n2.732331\n\n\nneigh_cvWISSAHICKON_HILLS\n1.455756\n\n\nneigh_cvWISSINOMING\n7.039081\n\n\nneigh_cvWISTER\n1.595599\n\n\nneigh_cvWYNNEFIELD\n2.974526\n\n\nneigh_cvWYNNEFIELD_HEIGHTS\n1.167013\n\n\nneigh_cvYORKTOWN\n1.219956"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html",
    "title": "Philadelphia Housing Price Prediction",
    "section": "",
    "text": "How accurately can we predict 2023–2024 residential sale prices in Philadelphia using property characteristics and neighborhood/contextual features, in order to improve the city’s automated property tax assessment model?"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#data-overview",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#data-overview",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Overview",
    "text": "Data Overview\n\nOur Data Foundation\n\nPhiladelphia Property Sales (2023–2024)\nOver 17,000 residential transactions citywide capturing sale price, location, and property details.\nNeighborhood & Contextual Data\nCombined city (OpenDataPhilly) and census (ACS 2023) information on:\n\nCommunity factors: race, employment rate, median rent, education level, neighborhood\nPublic safety: proximity to vioelnt crime\nAccessibility: bike routes\nAmenities & services: parks, public schools, fire department, food retail"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#what-do-home-prices-look-like-in-philadelphia",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#what-do-home-prices-look-like-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Do Home Prices Look Like in Philadelphia?",
    "text": "What Do Home Prices Look Like in Philadelphia?\n![Histogram of Sale Prices]\nKey Findings - Most homes sell for under $400,000\n- A small number of luxury properties push up the high end\n- Reflects strong housing inequality across neighborhoods"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#henry-please-edit-key-findings-and-slides-layout-for-the-three-visualization-slides",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#henry-please-edit-key-findings-and-slides-layout-for-the-three-visualization-slides",
    "title": "Philadelphia Housing Price Prediction",
    "section": "(Henry, please edit key findings and slides layout for the three visualization slides)",
    "text": "(Henry, please edit key findings and slides layout for the three visualization slides)"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#where-are-expensive-homes-in-philadelphia",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#where-are-expensive-homes-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Where Are Expensive Homes in Philadelphia?",
    "text": "Where Are Expensive Homes in Philadelphia?\n![Map of Sale Prices]\nKey Findings\n\nCenter City and Northwest Philadelphia command premium prices\n\nRiver Wards show emerging appreciation\n\nOuter neighborhoods remain most affordable"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#visualization-3",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#visualization-3",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Visualization 3",
    "text": "Visualization 3\n(Add figure or placeholder content here)"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#how-does-model-performance-improve",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#how-does-model-performance-improve",
    "title": "Philadelphia Housing Price Prediction",
    "section": "How Does Model Performance Improve?",
    "text": "How Does Model Performance Improve?\n\n\n\nModel\nCV RMSE\nR²\n\n\n\n\nStructural Only\n139,097\n0.67\n\n\nSpatial\n111,374\n0.79\n\n\nFixed Effects\n95,370\n0.85\n\n\nHyper-model\n65,382\n0.93\n\n\n\nBottom Line\nEach additional data layer improves accuracy, with neighborhood / market-value features producing the largest jump (lowest RMSE and highest R²)."
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#what-drives-the-improvement",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#what-drives-the-improvement",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Drives the Improvement?",
    "text": "What Drives the Improvement?\n\nFrom taking into accounts only the structural/building quality variables in our first model;\nAdding Spatial Data accounts for proximity to parks, public schools, food retail, and violent crime.\nAdding Census & Socioeconomic Context captures income, education, and housing quality\n\nAdding Neighborhood Fixed Effects controls for unobserved local traits\n\nGrouping Small Neighborhoods stabilizes estimates where data are sparse (sales&lt;10)\nIncluding Market Value Benchmarks aligns predictions with broader price trends\n\nTakeaway:\nPhiladelphia’s housing market is highly localized — accuracy improves most when models reflect neighborhood structure and market context. The Hyper model which optimized by groups small neighborhoods, is more parsimonious while maintaining high predictive performance."
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#best-models",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#best-models",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Best Models",
    "text": "Best Models\n\n\n\nModel\nRMSE ($)\nR²\n\n\n\n\nModel 3: Fixed Effects\n$95,370\n0.85\n\n\nModel 4: Hyper Model\n$65,382\n0.93\n\n\n\nBoth models capture structural, spatial, census, and socioeconomic factors,\nwhile incorporating neighborhood effects to reflect local market variation.\nKey Insight:\nAdding small-neighborhood grouping and market value signals delivers a major leap in accuracy — boosting R² from 0.85 → 0.93. By going down to the block level, our model captures fine-grained local variation, showing that neighborhood and market context are essential for fair property assessments."
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#key-findings",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#key-findings",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Key Findings",
    "text": "Key Findings\nModel Accuracy RMSE = $65,382 & R² = 0.93\nTop Predictors - Neighborhoods (Prestige/unobserved characteristics) - Market Value\n(Mohamed, Please interpret coefficient)"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#hardest-to-predict",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#hardest-to-predict",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Hardest To Predict",
    "text": "Hardest To Predict\n (Jinheng, please update from this slide to end) —"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#policy-recommendations",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#policy-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Policy Recommendations",
    "text": "Policy Recommendations\nKey Actions - ✓ Current AVM undervalues transit-accessible properties\n- ✓ Model struggles in rapidly gentrifying neighborhoods\n- ✓ Incorporate updated neighborhood effects to improve fairness\n- ✓ Use regular data refreshes to maintain accuracy over time"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#limitations-next-steps",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nLimitations - Placeholder text — refine with data or discussion\nNext Steps - ✓ A\n- ✓ B\n- ✓ C"
  },
  {
    "objectID": "assignments/midterm/presentation/Midterm_slides_v2.html#thank-you",
    "href": "assignments/midterm/presentation/Midterm_slides_v2.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "THANK YOU",
    "text": "THANK YOU"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#about-this-exercise",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#about-this-exercise",
    "title": "Predictive Policing - Technical Implementation",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.1-load-chicago-spatial-data",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.1-load-chicago-spatial-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"✓ Loaded spatial boundaries\\n\")\n\n\n✓ Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe’re using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.2-load-burglary-data",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.2-load-burglary-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(here(\"data\", \"burglaries.shp\")) %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `C:\\NATAN\\PENN\\2025 FALL\\Public Policy Analytics\\portfolio-setup-itsnatani-humaira\\labs\\lab6\\week-09\\data\\burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n✓ Loaded burglary data\\n\")\n\n\n\n✓ Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$date, na.rm = TRUE), \"to\", \n    max(burglaries$date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: Inf to -Inf \n\n\nQuestion 1.1: How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis?\nYour answer here:\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn’t call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.3-visualize-point-data",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-1.3-visualize-point-data",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nQuestion 1.2: What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-2.1-understanding-the-fishnet",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-2.1-understanding-the-fishnet",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\nThink of it as overlaying graph paper on a map.\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nQuestion 2.1: Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nQuestion 2.2: What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.1: Load 311 Abandoned Vehicle Calls",
    "text": "Exercise 4.1: Load 311 Abandoned Vehicle Calls\n\n\nCode\nabandoned_cars &lt;- read_csv(here(\"data/abandoned_cars_2017.csv\"))%&gt;%\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"✓ Loaded abandoned vehicle calls\\n\")\n\n\n✓ Loaded abandoned vehicle calls\n\n\nCode\ncat(\"  - Number of calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of calls: 31390 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago’s Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.2: Count of Abandoned Cars per Cell",
    "text": "Exercise 4.2: Count of Abandoned Cars per Cell\n\n\nCode\n# Aggregate abandoned car calls to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\ncat(\"Abandoned car distribution:\\n\")\n\n\nAbandoned car distribution:\n\n\nCode\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle 311 Calls\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(title = \"Are abandoned cars and burglaries correlated?\")\n\n\n\n\n\n\n\n\n\nQuestion 4.1: Do you see a visual relationship between abandoned cars and burglaries? What does this suggest?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.3-nearest-neighbor-features",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.3-nearest-neighbor-features",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 abandoned cars captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    abandoned_cars.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"✓ Calculated nearest neighbor distances\\n\")\n\n\n✓ Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.284 2195.753 \n\n\nQuestion 4.2: What does a low value of abandoned_cars.nn mean? A high value? Why might this be informative?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.4-distance-to-hot-spots",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-4.4-distance-to-hot-spots",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet’s identify clusters of abandoned cars using Local Moran’s I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"✓ Calculated distance to abandoned car hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to abandoned car hot spots\n  - Number of hot spot cells: 275 \n\n\nQuestion 4.3: Why might distance to a cluster of abandoned cars be more informative than distance to a single abandoned car? What does Local Moran’s I tell us?\nYour answer here:\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran’s I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.1-poisson-regression",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.1-poisson-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3…). We’ll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        1.976262369  0.042512701  46.486 &lt;0.0000000000000002 ***\nabandoned_cars    -0.001360741  0.001089805  -1.249               0.212    \nabandoned_cars.nn -0.004965200  0.000198914 -24.962 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000002874  0.000006206   0.463               0.643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 5070.6  on 1704  degrees of freedom\nAIC: 9138.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nQuestion 6.1: Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.2-check-for-overdispersion",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.2-check-for-overdispersion",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 3.38 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"✓ Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n⚠ Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.3-negative-binomial-regression",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-6.3-negative-binomial-regression",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.603099596, \n    link = log)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        2.092907737  0.077469423  27.016 &lt;0.0000000000000002 ***\nabandoned_cars    -0.002006352  0.002091851  -0.959               0.337    \nabandoned_cars.nn -0.005844829  0.000321389 -18.186 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000006861  0.000011049   0.621               0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6031) family taken to be 1)\n\n    Null deviance: 2534.0  on 1707  degrees of freedom\nResidual deviance: 1796.6  on 1704  degrees of freedom\nAIC: 7522.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.6031 \n          Std. Err.:  0.0888 \n\n 2 x log-likelihood:  -7512.5850 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 9138.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7522.6 \n\n\nQuestion 6.2: Which model fits better (lower AIC)? What does this tell you about the data?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-8.1-generate-final-predictions",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-8.1-generate-final-predictions",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 8.2: Compare Model vs. KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs. KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.48\n3.59\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nQuestion 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-9.3-where-does-the-model-work-well",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-9.3-where-does-the-model-work-well",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 9.3: Where Does the Model Work Well?",
    "text": "Exercise 9.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nQuestion 9.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\nYour answer here:"
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-10.1-model-summary-table",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-10.1-model-summary-table",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.1: Model Summary Table",
    "text": "Exercise 10.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n8.108\n0.077\n27.016\n0.000\n\n\nabandoned_cars\n0.998\n0.002\n-0.959\n0.337\n\n\nabandoned_cars.nn\n0.994\n0.000\n-18.186\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n0.621\n0.535\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-10.2-key-findings-summary",
    "href": "labs/lab6/week-09/lab/predictive_policing_exercise.html#exercise-10.2-key-findings-summary",
    "title": "Predictive Policing - Technical Implementation",
    "section": "Exercise 10.2: Key Findings Summary",
    "text": "Exercise 10.2: Key Findings Summary\nBased on your analysis, complete this summary:\nTechnical Performance:\n\nCross-validation MAE: 2.7\nModel vs. KDE: [Which performed better?]\nMost predictive variable: [Which had largest effect?]\n\nSpatial Patterns:\n\nBurglaries are [evenly distributed / clustered]\nHot spots are located in [describe]\nModel errors show [random / systematic] patterns\n\nModel Limitations:\n\nOverdispersion: [Yes/No]\nSpatial autocorrelation in residuals: [Test this!]\nCells with zero counts: [What % of data?]"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner.\n\n\n\n\n\n\nRun the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC?\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#your-role",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#your-role",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner."
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#instructions",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#instructions",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "Run the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC?\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#consulting-team-information",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#consulting-team-information",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "Consulting Team Information",
    "text": "Consulting Team Information\nClever Team Name: _____________\nTeam Members:"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#technical-assessment",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#technical-assessment",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "1. TECHNICAL ASSESSMENT",
    "text": "1. TECHNICAL ASSESSMENT\n\nModel Performance Metrics\nAUC (Area Under ROC Curve): __________\nAt threshold = 0.50:\n\nSensitivity (True Positive Rate): __________\nSpecificity (True Negative Rate): __________\nPrecision (Positive Predictive Value): __________\nOverall Accuracy: __________\n\n\n\nTechnical Quality Rating\nSelect one:\n\nExcellent (AUC &gt; 0.90)\nGood (AUC 0.80-0.90)\nAcceptable (AUC 0.70-0.80)\nPoor (AUC &lt; 0.70)\n\n\n\nBrief Technical Summary (2-3 sentences)\nIs the model accurate enough for high-stakes decision-making?"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#equity-analysis",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#equity-analysis",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "2. EQUITY ANALYSIS",
    "text": "2. EQUITY ANALYSIS\n\nFalse Positive Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Positive Rate\nSample Size\n\n\n\n\nGroup 1:\n\n\n\n\nGroup 2:\n\n\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nFalse Negative Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Negative Rate\nSample Size\n\n\n\n\nGroup 1:\n\n\n\n\nGroup 2:\n\n\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nDisparity Analysis\nLargest disparity identified:\nGroup _____________ has ______% higher false positive rate than Group _____________\nOR\nGroup _____________ has ______% higher false negative rate than Group _____________\n\n\nEquity Concerns Summary (3-4 sentences)\nWhat are the implications of these disparities? Who is harmed?"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "3. THRESHOLD RECOMMENDATION",
    "text": "3. THRESHOLD RECOMMENDATION\n\nIf we deploy this model, we recommend:\nSelect one:\n\nThreshold = 0.30 (Aggressive - prioritize catching recidivists)\nThreshold = 0.50 (Balanced - default)\nThreshold = 0.70 (Conservative - minimize false accusations)\nOther: ________\n\n\n\nRationale for Threshold Choice (3-4 sentences)\nWhy this threshold? What does it optimize for? What are the trade-offs?\n\n\n\n\n\n\nThis threshold prioritizes:\nSelect one:\n\nHigh Sensitivity - Catch more people who will reoffend (accept more false positives)\nHigh Specificity - Avoid false accusations (accept more false negatives)\nBalance - Try to minimize both types of errors"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "4. DEPLOYMENT RECOMMENDATION",
    "text": "4. DEPLOYMENT RECOMMENDATION\n\nOur recommendation to Georgia DOC:\nSelect one:\n\nDEPLOY - Use this model to inform parole decisions\nDO NOT DEPLOY - Do not use this model\nCONDITIONAL DEPLOY - Deploy only with specific safeguards in place\n\n\n\nKey Reasons for Our Recommendation\nProvide 3-5 bullet points supporting your decision:\n\n\n\n\n\n\n\n\n\nWhat about the equity concerns?\nHow do you justify your recommendation given the disparate impact you identified?"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "5. SAFEGUARDS OR ALTERNATIVES",
    "text": "5. SAFEGUARDS OR ALTERNATIVES\n\nIf DEPLOY - Required Safeguards\nWhat protections must be in place before deployment?\n\n\n\n\n\n\nOR\n\n\nIf DO NOT DEPLOY - Alternative Approaches\nWhat should Georgia DOC do instead?"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "6. LIMITATIONS & UNCERTAINTIES",
    "text": "6. LIMITATIONS & UNCERTAINTIES\n\nWhat we don’t know (but wish we did)\nWhat additional information would strengthen your recommendation?\n\n\n\n\n\nWeaknesses in our recommendation\nWhat’s the strongest argument AGAINST your recommendation?"
  },
  {
    "objectID": "labs/lecture/script/In_Class_Exercise_Instruction.html#bottom-line",
    "href": "labs/lecture/script/In_Class_Exercise_Instruction.html#bottom-line",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "7. BOTTOM LINE",
    "text": "7. BOTTOM LINE\n\nOne-Sentence Recommendation\nIf the Commissioner only reads one thing, what should it be?"
  }
]