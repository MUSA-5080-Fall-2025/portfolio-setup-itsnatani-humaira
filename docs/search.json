[
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html",
    "href": "assignments/assignment1/scripts/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Mississippi Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#scenario",
    "href": "assignments/assignment1/scripts/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Mississippi Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#learning-objectives",
    "href": "assignments/assignment1/scripts/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#submission-instructions",
    "href": "assignments/assignment1/scripts/assignment1_template.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#data-retrieval",
    "href": "assignments/assignment1/scripts/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nms_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"MS\",\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\nms_data\n\n# A tibble: 82 × 6\n   GEOID NAME                total_popE total_popM median_incomeE median_incomeM\n   &lt;chr&gt; &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 28001 Adams County, Miss…      29425         NA          37271           4671\n 2 28003 Alcorn County, Mis…      34717         NA          47716           4160\n 3 28005 Amite County, Miss…      12683         NA          34866           3839\n 4 28007 Attala County, Mis…      17842         NA          42680           5034\n 5 28009 Benton County, Mis…       7637         NA          38750           3034\n 6 28011 Bolivar County, Mi…      30688         NA          37845           3176\n 7 28013 Calhoun County, Mi…      13193         NA          44505           3793\n 8 28015 Carroll County, Mi…       9930         NA          42285          14597\n 9 28017 Chickasaw County, …      17024         NA          40224           3081\n10 28019 Choctaw County, Mi…       8208         NA          41887           4482\n# ℹ 72 more rows\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nms_clean &lt;- ms_data %&gt;%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \" County, Mississippi\")\n  )\n# Display the first few rows\nhead(ms_clean)\n\n# A tibble: 6 × 7\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM county_name\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;      \n1 28001 Adams C…      29425         NA          37271           4671 Adams      \n2 28003 Alcorn …      34717         NA          47716           4160 Alcorn     \n3 28005 Amite C…      12683         NA          34866           3839 Amite      \n4 28007 Attala …      17842         NA          42680           5034 Attala     \n5 28009 Benton …       7637         NA          38750           3034 Benton     \n6 28011 Bolivar…      30688         NA          37845           3176 Bolivar"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#data-quality-assessment",
    "href": "assignments/assignment1/scripts/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nms_reliability &lt;- ms_clean %&gt;%\n  mutate(\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 0),\n    reliability = case_when(\n      moe_percentage &lt; 5 ~ \"High Confidence\",\n      moe_percentage &gt;= 5 & moe_percentage &lt;= 10 ~ \"Moderate Confidence\",\n      moe_percentage &gt; 10 ~ \"Low Confidence\",\n    ),\n    unreliable_estimates = moe_percentage &gt; 10\n  )\nms_reliability\n\n# A tibble: 82 × 10\n   GEOID NAME    total_popE total_popM median_incomeE median_incomeM county_name\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;      \n 1 28001 Adams …      29425         NA          37271           4671 Adams      \n 2 28003 Alcorn…      34717         NA          47716           4160 Alcorn     \n 3 28005 Amite …      12683         NA          34866           3839 Amite      \n 4 28007 Attala…      17842         NA          42680           5034 Attala     \n 5 28009 Benton…       7637         NA          38750           3034 Benton     \n 6 28011 Boliva…      30688         NA          37845           3176 Bolivar    \n 7 28013 Calhou…      13193         NA          44505           3793 Calhoun    \n 8 28015 Carrol…       9930         NA          42285          14597 Carroll    \n 9 28017 Chicka…      17024         NA          40224           3081 Chickasaw  \n10 28019 Chocta…       8208         NA          41887           4482 Choctaw    \n# ℹ 72 more rows\n# ℹ 3 more variables: moe_percentage &lt;dbl&gt;, reliability &lt;chr&gt;,\n#   unreliable_estimates &lt;lgl&gt;\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nreliability_summary &lt;- ms_reliability %&gt;%\n  group_by(reliability) %&gt;%\n  summarize(\n    counties = n()) %&gt;%\n    mutate(percentage = round((counties / sum(counties)) * 100, 0))"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#high-uncertainty-counties",
    "href": "assignments/assignment1/scripts/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nms_high_uncertainty &lt;- ms_reliability %&gt;%\n  arrange(desc(moe_percentage)) %&gt;%\n  slice_head(n = 5) %&gt;% \n  select(county_name, median_incomeE, median_incomeM, moe_percentage, reliability)\n# Format as table with kable() - include appropriate column names and caption\nkable(ms_high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nCounties with Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\n\n\n\n\nCarroll\n42,285\n14,597\n35\nLow Confidence\n\n\nIssaquena\n17,900\n5,191\n29\nLow Confidence\n\n\nSharkey\n41,000\n10,196\n25\nLow Confidence\n\n\nTunica\n41,676\n10,036\n24\nLow Confidence\n\n\nWilkinson\n34,928\n8,245\n24\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\nOut of Mississippi’s 82 counties, 43 have low confidence reliability, with a margin of error (MOE) greater than 10%. This represents 52% of all counties, indicating that a significant portion of the available data may not be reliable for algorithmic decision-making. Relying on such data to inform policies carries a high risk of policy bias, which could result in inaccurate targeting of resources and services.\nSome counties with the highest MOE include Carroll, Issaquena, Sharkey, Tunica, and Wilkinson, where income data has MOE values ranging from 23% to 34%. Algorithms dependent on this census income data may therefore misrepresent the actual economic conditions in these areas, potentially leading to communities being underfunded or overlooked.\nSeveral factors may contribute to these high levels of uncertainty, such as: - Small population sizes, which increase sampling variability. - Limited survey response rates. - Data collection challenges, including underreporting or outdated information. - Economic and demographic fluctuations, which are harder to capture accurately with small sample sizes."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#focus-area-selection",
    "href": "assignments/assignment1/scripts/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- ms_reliability %&gt;% filter(county_name == \"Carroll\" | county_name == \"Chickasaw\" | county_name == \"DeSoto\")\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselect(selected_counties, county_name, median_incomeE, moe_percentage, reliability)\n\n# A tibble: 3 × 4\n  county_name median_incomeE moe_percentage reliability        \n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;              \n1 Carroll              42285             35 Low Confidence     \n2 Chickasaw            40224              8 Moderate Confidence\n3 DeSoto               79666              3 High Confidence    \n\n\nComment on the output: These three counties represent different levels of data reliability, with a significant gap in their margins of error ranging from 3% to 35%. This poses a challenge for state-level algorithmic decision-making, as counties with lower data reliability may be underrepresented or misrepresented."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#tract-level-demographics",
    "href": "assignments/assignment1/scripts/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nms_counties_data &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    white_alone = \"B03002_003\",\n    african_american = \"B03002_004\",\n    hispanic_latino = \"B03002_012\"\n  ),\n  state = \"MS\",\n  county = c(\"015\", \"017\", \"033\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n# Calculate percentage of each group using mutate()\n\n# Create percentages for white, Black, and Hispanic populations\nms_counties_data &lt;- ms_counties_data %&gt;%\n  mutate(\n    pct_white = round((white_aloneE / total_popE) * 100, 0),          # % White alone\n    pct_black = round((african_americanE / total_popE) * 100, 0),     # % African American\n    pct_hispanic = round((hispanic_latinoE / total_popE) * 100, 0)    # % Hispanic or Latino\n  )\n# Add readable tract and county name columns using str_extract() or similar\nms_counties_clean &lt;- ms_counties_data %&gt;%\n  mutate(\n    # Remove \"Census \" at the start\n    tract_name = str_remove(NAME, \"^Census\\\\s+\"),\n    \n    # Remove \"; Mississippi\" at the end\n    tract_name = str_remove(tract_name, \"; Mississippi$\")\n  ) %&gt;%\n  mutate(\n    county_name = str_extract(NAME, \"(?&lt;=; )[^;]+ County\")\n  )"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#demographic-analysis",
    "href": "assignments/assignment1/scripts/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhighest_hispanic_tract &lt;- ms_counties_clean %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice_head(n = 1)\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ncounty_demographics &lt;- ms_counties_clean %&gt;%\n  group_by(county_name) %&gt;%                  # Group data by county\n  summarize(\n    num_tracts = n(),                        # Count number of tracts in each county\n    avg_pct_white = round(mean(pct_white, na.rm = TRUE), 0),  # Average % White\n    avg_pct_black = round(mean(pct_black, na.rm = TRUE), 0),  # Average % Black\n    avg_pct_hispanic = round(mean(pct_hispanic, na.rm = TRUE), 0) # Average % Hispanic Latino\n  ) %&gt;%\n  arrange(desc(num_tracts))                   # Sort counties by number of tracts\n\n# Create a nicely formatted table of your results using kable()\nkable(county_demographics,\n      col.names = c(\"County\", \"Number of Tracts\", \"White % Average\", \"Black % Average\", \"Hispanic / Latino % Average\"),\n      caption = \"County demographics comparison\",\n      format.args = list(big.mark = \",\"))\n\n\nCounty demographics comparison\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nWhite % Average\nBlack % Average\nHispanic / Latino % Average\n\n\n\n\nDeSoto County\n41\n60\n30\n5\n\n\nChickasaw County\n5\n49\n42\n7\n\n\nCarroll County\n3\n64\n35\n0"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/scripts/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\nms_counties_clean &lt;- ms_counties_clean %&gt;%\n  mutate(\n    moe_pct_white = round((white_aloneM / white_aloneE) * 100, 0),\n    moe_pct_black = round((african_americanM / african_americanE) * 100, 0),\n    moe_pct_hispanic = round((hispanic_latinoM / hispanic_latinoE) * 100, 0)\n  )\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nms_counties_clean &lt;- ms_counties_clean %&gt;%\n  mutate(\n    reliability = ifelse(\n      moe_pct_white &gt; 15 | moe_pct_black &gt; 15 | moe_pct_hispanic &gt; 15,\n      \"Low Confidence\",\n      \"High Confidence\"\n    )\n  )\n# Create summary statistics showing how many tracts have data quality issues\ntract_moe_summary &lt;- ms_counties_clean %&gt;%\n  group_by(reliability) %&gt;%                  # Group data by reliability\n  summarize(\n    num_tracts = n())"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#pattern-analysis",
    "href": "assignments/assignment1/scripts/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\nmoe_summary &lt;- ms_counties_clean %&gt;%\n  group_by(reliability) %&gt;%\n  summarize(\n    num_tracts = n(),                                  # Count of tracts in each group\n    avg_total_pop = round(mean(total_popE, na.rm = TRUE), 0),    # Average population\n    avg_pct_white = round(mean((white_aloneE / total_popE) * 100, na.rm = TRUE), 0),\n    avg_pct_black = round(mean((african_americanE / total_popE) * 100, na.rm = TRUE), 0),\n    avg_pct_hispanic = round(mean((hispanic_latinoE / total_popE) * 100, na.rm = TRUE), 0)\n  ) %&gt;%\n  arrange(desc(num_tracts))\n# Create a professional table showing the patterns\nkable(\n  moe_summary,\n  caption = \"Comparison of Census Tracts by MOE Reliability\",\n  col.names = c(\"MOE Group\", \"Number of Tracts\", \"Avg Population\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nComparison of Census Tracts by MOE Reliability\n\n\n\n\n\n\n\n\n\n\nMOE Group\nNumber of Tracts\nAvg Population\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nLow Confidence\n49\n4,350\n59\n32\n5\n\n\n\n\n\nPattern Analysis: I see no clear pattern of lower data reliability related to the demographic composition of the communities. The data quality appears to be randomly distributed, regardless of which racial or ethnic group dominates a particular census tract.\nA possible reason why some tracts have low confidence estimates could be due to small population sizes in those tracts. Smaller populations tend to produce higher margins of error in survey-based estimates, because sampling variability is greater when fewer people are surveyed."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/scripts/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nMississippi is one of the hungriest and poorest states in the U.S. To ensure that aid programs are implemented effectively and reach the communities most in need, decisions about which areas receive resources must be based on reliable and accurate data. Unfortunately, our analysis found data quality issues in many parts of Mississippi, creating challenges for equitable program delivery.\nOur analysis revealed no clear geographic or demographic pattern to the distribution of low-quality data. Census tracts with high margins of error (MOE) were randomly distributed across the three counties analyzed — Carroll, Chickasaw, and DeSoto — and did not consistently align with the racial or ethnic composition of the population. This suggests that data reliability is not systematically related to whether a tract is majority White, Black, or Hispanic.\nBecause low data reliability is randomly distributed, all communities are equally at risk of algorithmic bias if unreliable data is used in decision-making.High MOE tracts, regardless of demographic makeup, could be overlooked or misclassified, resulting in unequal access to resources. This risk is particularly concerning in vulnerable areas with high poverty rates, where unreliable data could hide urgent needs and exacerbate existing inequities.\nOur findings suggest several underlying factors driving both data quality issues and algorithmic bias risks: 1. Small Population Sizes. Rural tracts have fewer survey respondents, leading to higher sampling variability and larger margins of error. 2. Survey Coverage Gaps. Certain areas may have limited census outreach or lower response rates, especially in hard-to-reach communities.\nThese factors combine to make rural and low-population tracts particularly vulnerable to unreliable data, even though our analysis showed no demographic-specific pattern. To ensure aid programs are accurately targeted and equitable, we recommend the Department take the following actions:\n\nIntegrate Data Reliability Checks\nFlag tracts with unreliable data for manual review before decisions are made.\nInvest in Rural Data Collection\nStrengthen survey outreach efforts in small or hard-to-reach tracts to increase response rates and improve reliability over time.\n\nThe lack of a clear demographic pattern in data reliability suggests that no single community is disproportionately affected by poor data quality. However, the random distribution of unreliable data, especially in rural areas, creates a systemic risk of misallocation and algorithmic bias.\nBy implementing data reliability safeguards and strengthening rural survey outreach, the Department can improve equity in resource distribution and ensure that aid reaches the communities who need it most."
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#specific-recommendations",
    "href": "assignments/assignment1/scripts/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\ndecision_framework &lt;- ms_reliability %&gt;%\n  select(county_name, median_incomeE, moe_percentage, reliability) %&gt;%\n  mutate(\n    # Add algorithm decision recommendations\n    algorithm_recommendation = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\",\n      TRUE ~ \"Not Classified\"\n    )\n  )\n# Format as a professional table with kable()\nkable(\n  decision_framework,\n  caption = \"Algorithm Decision Framework for County-Level Data\",\n  col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Algorithm Recommendation\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nAlgorithm Decision Framework for County-Level Data\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE %\nReliability\nAlgorithm Recommendation\n\n\n\n\nAdams\n37,271\n13\nLow Confidence\nRequires manual review or additional data\n\n\nAlcorn\n47,716\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nAmite\n34,866\n11\nLow Confidence\nRequires manual review or additional data\n\n\nAttala\n42,680\n12\nLow Confidence\nRequires manual review or additional data\n\n\nBenton\n38,750\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nBolivar\n37,845\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCalhoun\n44,505\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCarroll\n42,285\n35\nLow Confidence\nRequires manual review or additional data\n\n\nChickasaw\n40,224\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nChoctaw\n41,887\n11\nLow Confidence\nRequires manual review or additional data\n\n\nClaiborne\n34,282\n11\nLow Confidence\nRequires manual review or additional data\n\n\nClarke\n46,329\n20\nLow Confidence\nRequires manual review or additional data\n\n\nClay\n37,412\n10\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCoahoma\n36,075\n7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCopiah\n46,889\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nCovington\n40,164\n14\nLow Confidence\nRequires manual review or additional data\n\n\nDeSoto\n79,666\n3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nForrest\n49,340\n6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nFranklin\n43,942\n14\nLow Confidence\nRequires manual review or additional data\n\n\nGeorge\n51,349\n10\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n50,000\n15\nLow Confidence\nRequires manual review or additional data\n\n\nGrenada\n45,745\n11\nLow Confidence\nRequires manual review or additional data\n\n\nHancock\n63,623\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHarrison\n55,211\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHinds\n48,596\n6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHolmes\n28,818\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHumphreys\n31,907\n12\nLow Confidence\nRequires manual review or additional data\n\n\nIssaquena\n17,900\n29\nLow Confidence\nRequires manual review or additional data\n\n\nItawamba\n57,252\n13\nLow Confidence\nRequires manual review or additional data\n\n\nJackson\n60,045\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nJasper\n43,914\n12\nLow Confidence\nRequires manual review or additional data\n\n\nJefferson\n31,544\n13\nLow Confidence\nRequires manual review or additional data\n\n\nJefferson Davis\n36,473\n15\nLow Confidence\nRequires manual review or additional data\n\n\nJones\n49,451\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKemper\n42,947\n10\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLafayette\n59,748\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLamar\n67,972\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLauderdale\n45,649\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLawrence\n41,096\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLeake\n46,669\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLee\n64,479\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLeflore\n33,115\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLincoln\n47,069\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLowndes\n53,687\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadison\n79,105\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarion\n38,399\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMarshall\n51,431\n14\nLow Confidence\nRequires manual review or additional data\n\n\nMonroe\n51,190\n5\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMontgomery\n36,845\n16\nLow Confidence\nRequires manual review or additional data\n\n\nNeshoba\n47,400\n12\nLow Confidence\nRequires manual review or additional data\n\n\nNewton\n49,160\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nNoxubee\n42,298\n11\nLow Confidence\nRequires manual review or additional data\n\n\nOktibbeha\n42,953\n11\nLow Confidence\nRequires manual review or additional data\n\n\nPanola\n47,894\n16\nLow Confidence\nRequires manual review or additional data\n\n\nPearl River\n54,220\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPerry\n48,333\n11\nLow Confidence\nRequires manual review or additional data\n\n\nPike\n40,131\n6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPontotoc\n54,414\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPrentiss\n51,578\n12\nLow Confidence\nRequires manual review or additional data\n\n\nQuitman\n31,192\n12\nLow Confidence\nRequires manual review or additional data\n\n\nRankin\n76,460\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nScott\n44,968\n16\nLow Confidence\nRequires manual review or additional data\n\n\nSharkey\n41,000\n25\nLow Confidence\nRequires manual review or additional data\n\n\nSimpson\n50,867\n7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSmith\n51,983\n21\nLow Confidence\nRequires manual review or additional data\n\n\nStone\n55,894\n11\nLow Confidence\nRequires manual review or additional data\n\n\nSunflower\n37,403\n11\nLow Confidence\nRequires manual review or additional data\n\n\nTallahatchie\n35,428\n12\nLow Confidence\nRequires manual review or additional data\n\n\nTate\n61,286\n8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTippah\n47,968\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTishomingo\n45,545\n15\nLow Confidence\nRequires manual review or additional data\n\n\nTunica\n41,676\n24\nLow Confidence\nRequires manual review or additional data\n\n\nUnion\n55,970\n9\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWalthall\n37,145\n17\nLow Confidence\nRequires manual review or additional data\n\n\nWarren\n54,117\n4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n38,394\n11\nLow Confidence\nRequires manual review or additional data\n\n\nWayne\n34,875\n13\nLow Confidence\nRequires manual review or additional data\n\n\nWebster\n55,657\n12\nLow Confidence\nRequires manual review or additional data\n\n\nWilkinson\n34,928\n24\nLow Confidence\nRequires manual review or additional data\n\n\nWinston\n45,516\n13\nLow Confidence\nRequires manual review or additional data\n\n\nYalobusha\n47,006\n11\nLow Confidence\nRequires manual review or additional data\n\n\nYazoo\n41,867\n11\nLow Confidence\nRequires manual review or additional data\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\n\nThese counties have high-confidence data with low margins of error (MOE), meaning the information is statistically reliable and accurate. - The population is well-represented in surveys. - Data is consistent and stable over time. - Minimal risk of algorithmic bias caused by data errors.\nCounties: DeSoto, Harrison, Jackson, Lowndes, Madison, Rankin, Warren\n\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed] These counties have moderate-confidence data, indicating some risk of errors or gaps. While the data is generally usable, there are enough limitations that algorithmic decisions must be closely supervised.\n\nMonitoring actions needed:\n\nHuman review of algorithm outputs to catch anomalies or unfair patterns.\nRegular audits of decisions, especially in high-impact areas such as resource allocation or eligibility for aid programs.\nContinuous data tracking, updating the system as new survey data becomes available.\nCommunity feedback loops to validate results and identify potential issues early.\n\nCounties: Alcorn, Benton, Bolivar, Calhoun, Chickasaw, Clay, Coahoma, Copiah, Forrest, George, Hancock, Hinds, Holmes, Jones, Kemper, Lafayette, Lamar, Lauderdale, Lawrence, Leake, Lee, Leflore, Lincoln, Marion, Monroe, Newton, Pearl River, Pike, Pontotoc, Simpson, Tate, Tippah, Union\n\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.] These counties have low-confidence data, often due to very high margins of error or incomplete reporting.\n\n\nData may be missing or misrepresent entire communities.\nThere is a high risk of algorithmic bias if decisions rely solely on this data.\nAutomated decisions could lead to misallocation of resources or inequitable outcomes.\n\nRecommended alternative approaches:\n\nManual review of decisions by local experts to account for nuances not captured by the data.\nSupplementary data collection, such as targeted community surveys or administrative records.\nBlended approaches, combining algorithmic analysis with on-the-ground insights from social workers, local agencies, or nonprofits.\nPilot programs to test algorithms on a small scale before wider rollout.\n\nCounties: Adams, Amite, Attala, Carroll, Choctaw, Claiborne, Clarke, Covington, Franklin, Greene, Grenada, Humphreys, Issaquena, Itawamba, Jasper, Jefferson, Jefferson Davis, Marshall, Montgomery, Neshoba, Noxubee, Oktibbeha, Panola, Perry, Prentiss, Quitman, Scott, Sharkey, Smith, Stone, Sunflower, Tallahatchie, Tishomingo, Tunica, Walthall, Washington, Wayne, Webster, Wilkinson, Winston, Yalobusha, Yazoo"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#questions-for-further-investigation",
    "href": "assignments/assignment1/scripts/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nDo rural areas or border regions consistently show higher margins of error compared to urban centers like Jackson or Gulfport?\nWhat spatial factors (e.g., population density, survey reach, infrastructure) drive these reliability differences?\nHas the data reliability improved or worsened over time in Mississippi?\nAre certain counties consistently underrepresented in ACS data year after year, indicating a systemic issue in data collection?"
  },
  {
    "objectID": "assignments/assignment1/scripts/assignment1_template.html#submission-checklist",
    "href": "assignments/assignment1/scripts/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "",
    "text": "Part 1: Algorithmic Decision Making\nKey Terms → Data Science: Computer science/engineering focus on algorithm and methods → Data Analytics: Application of Data Science methods to other disciplines → Machine Learning: Algorithm for classification and prediction that learn from data → AI: Algorithms that adjust and improve across iterations (neural networks, etc)\nGovernment data collection history: Civic registration system Census Data Administrative records Operations Research (Post-WWII) What’s new in data collection: More data that are official and accidental Accidental → we turned on our location when updating on instagram Focus on prediction rather than explanation Harder to interpret and explain\nAlgorithmic decision making is especially appealing because it promises: Efficiency - process more cases faster Consistency - same rules applied to everyone Objectivity - removes human bias Cost savings - fewer staff needed\nData Analytics is subjective because every step involves human choices. For example during Data cleaning decisions; Data coding or classification; Data collection - use of imperfect proxies; How you interpret results; What variables you put in the model. These choices embed human values and biases.\nPart 2: Active Learning Exercise\nSmall group challenge scenario: School enrollment assignment Proxy: zipcodes stand in for “assignment-distance” Blindspots: wealthy/lower economic class neighborhood tend to be grouped together Harm + guiderail: low-income neighborhood get low quality education -&gt; we’d add more demographic data into consideration like racial data and income data into account when assigning school rather than just distance to achieve racial and economic diversity in certain neighborhood.\nPart 3: Census Data Foundations\n\nThe Decennial Census counts every person in the U.S. every 10 years to determine population counts for congressional apportionment and redistricting, asking basic questions about age, sex, and race.\nIn contrast, the American Community Survey (ACS) is a continuous, year-round survey sent to a sample of addresses that replaces the decennial “long form” by gathering detailed, timely socioeconomic, housing, and transportation data for communities to inform planning and funding for services and programs.\nACS Data 1-Year Estimates (areas &gt; 65,000 people) Most current data, smallest sample\n\n5-Year Estimates (all areas including census tracts) Most reliable data, largest sample What you’ll use most often\nCensus Geographic Hierarchy : Nation&gt;Regions&gt;States&gt;Counties&gt;Census Tracts (1,500-8,000 people)&gt;Block Groups (600-3,000 people)&gt;Blocks (~85 people, Decennial only)\nMost policy analysis happens at: County level - state and regional planning Census tract level - neighborhood analysis Block group level - very local analysis (tempting, but big MOEs)\nAccessing Census Data in R Traditional approach: Download CSV files from Census website Modern approach: Use R packages to access data directly –&gt; use the tidycensus package\nBenefits of programmatic access:\nAlways get latest data Reproducible workflows Automatic geographic boundaries Built-in error handling\nWhen New Data Comes Out - ACS 1-year estimates: Released in September (previous year’s data) - ACS 5-year estimates: Released in December - Decennial Census: Released on rolling schedule over 2-3 years\nHelpful Data Sources\nTIGER/Line Files - Geographic boundaries (shapefiles) - Census tracts, counties, states - Now released as shapefiles (easier to use!)\nHistorical Data Sources: - NHGIS (nhgis.org) - Historical census data - Neighborhood Change Database - Longitudinal Tract Database - Track changes over time\nPart 4: Census Data with R\ntotal_pop table –&gt; B01003_001 median_income table –&gt; B19013_001"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "",
    "text": "Part 1: Algorithmic Decision Making\nKey Terms → Data Science: Computer science/engineering focus on algorithm and methods → Data Analytics: Application of Data Science methods to other disciplines → Machine Learning: Algorithm for classification and prediction that learn from data → AI: Algorithms that adjust and improve across iterations (neural networks, etc)\nGovernment data collection history: Civic registration system Census Data Administrative records Operations Research (Post-WWII) What’s new in data collection: More data that are official and accidental Accidental → we turned on our location when updating on instagram Focus on prediction rather than explanation Harder to interpret and explain\nAlgorithmic decision making is especially appealing because it promises: Efficiency - process more cases faster Consistency - same rules applied to everyone Objectivity - removes human bias Cost savings - fewer staff needed\nData Analytics is subjective because every step involves human choices. For example during Data cleaning decisions; Data coding or classification; Data collection - use of imperfect proxies; How you interpret results; What variables you put in the model. These choices embed human values and biases.\nPart 2: Active Learning Exercise\nSmall group challenge scenario: School enrollment assignment Proxy: zipcodes stand in for “assignment-distance” Blindspots: wealthy/lower economic class neighborhood tend to be grouped together Harm + guiderail: low-income neighborhood get low quality education -&gt; we’d add more demographic data into consideration like racial data and income data into account when assigning school rather than just distance to achieve racial and economic diversity in certain neighborhood.\nPart 3: Census Data Foundations\n\nThe Decennial Census counts every person in the U.S. every 10 years to determine population counts for congressional apportionment and redistricting, asking basic questions about age, sex, and race.\nIn contrast, the American Community Survey (ACS) is a continuous, year-round survey sent to a sample of addresses that replaces the decennial “long form” by gathering detailed, timely socioeconomic, housing, and transportation data for communities to inform planning and funding for services and programs.\nACS Data 1-Year Estimates (areas &gt; 65,000 people) Most current data, smallest sample\n\n5-Year Estimates (all areas including census tracts) Most reliable data, largest sample What you’ll use most often\nCensus Geographic Hierarchy : Nation&gt;Regions&gt;States&gt;Counties&gt;Census Tracts (1,500-8,000 people)&gt;Block Groups (600-3,000 people)&gt;Blocks (~85 people, Decennial only)\nMost policy analysis happens at: County level - state and regional planning Census tract level - neighborhood analysis Block group level - very local analysis (tempting, but big MOEs)\nAccessing Census Data in R Traditional approach: Download CSV files from Census website Modern approach: Use R packages to access data directly –&gt; use the tidycensus package\nBenefits of programmatic access:\nAlways get latest data Reproducible workflows Automatic geographic boundaries Built-in error handling\nWhen New Data Comes Out - ACS 1-year estimates: Released in September (previous year’s data) - ACS 5-year estimates: Released in December - Decennial Census: Released on rolling schedule over 2-3 years\nHelpful Data Sources\nTIGER/Line Files - Geographic boundaries (shapefiles) - Census tracts, counties, states - Now released as shapefiles (easier to use!)\nHistorical Data Sources: - NHGIS (nhgis.org) - Historical census data - Neighborhood Change Database - Longitudinal Tract Database - Track changes over time\nPart 4: Census Data with R\ntotal_pop table –&gt; B01003_001 median_income table –&gt; B19013_001"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Algorithmic Decision Making",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "GitHub Git is a collaboration tool for teams. It tracks changes for our code projects. GitHub is a cloud hosting for Git repositories. It backs up work in the cloud. Through GitHub, we can share project with other people and collaborate on code projects.\nTerminology Repository is the folder that contains the project files Commit is snapshot of our work at a point in time Push Send changes to GitHub cloud Pull Get latest changes from GitHub cloud\nQuarto is a publishing system that combines code, text, and output into professional documents."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "GitHub Git is a collaboration tool for teams. It tracks changes for our code projects. GitHub is a cloud hosting for Git repositories. It backs up work in the cloud. Through GitHub, we can share project with other people and collaborate on code projects.\nTerminology Repository is the folder that contains the project files Commit is snapshot of our work at a point in time Push Send changes to GitHub cloud Pull Get latest changes from GitHub cloud\nQuarto is a publishing system that combines code, text, and output into professional documents."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\n[New R functions or approaches]\n[Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  }
]