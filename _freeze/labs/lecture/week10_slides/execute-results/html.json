{
  "hash": "837e7006b1ddb4b9dcd2316702efc9b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression for Binary Outcomes\"\nsubtitle: \"Week 10: Introduction to Logistic Regression\"\nauthor: \"Dr. Elizabeth Delmelle\"\ndate: \"November 10, 2025\"\nformat: \n  revealjs:\n    theme: simple\n    slide-number: true\n    chalkboard: true\n    code-line-numbers: true\n    incremental: false\n    smaller: true\n    scrollable: true\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\n\n\n# Opening: A New Kind of Prediction {.center}\n\n## A Decision with Real Consequences\n\n**Scenario:** A state corrections department asks you to help predict:\n\n*Will someone released from prison be arrested again within 3 years?*\n\n**Not asking:** How many times? How long until?  \n**Asking:** Yes or no? Will it happen or not?\n\n**Discussion question (1 minute):**  \n\n- How is this different from predicting home prices?\n- Why might they want this prediction?\n- What could go wrong?\n\n---\n\n# Part 1: Introduction to Logistic Regression {.center}\n\n## Where We've Been\n\n**Weeks 1-7: Linear regression**\n\n- Predicting **continuous outcomes**: home prices, income, population\n- Y = β₀ + β₁X₁ + β₂X₂ + ... + ε\n- Used RMSE to evaluate predictions\n\n**Last week: Poisson regression**\n\n- Predicting **count outcomes**: number of crimes\n- Different distribution, but still predicting quantities\n\n**Today: A fundamentally different question**\n\n- Not \"how much?\" but \"**will it happen?**\"\n- Binary outcomes: yes/no, 0/1, success/failure\n- This requires a completely different approach\n\n## What Makes Binary Outcomes Different?\n\n**The problem with linear regression for binary outcomes:**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week10_slides_files/figure-revealjs/binary-problem-1.png){width=960}\n:::\n:::\n\n\n**Problems:**\n\n- Predictions can be > 1 or < 0 (makes no sense for probability!)\n- Assumes constant effect across range (not realistic)\n- Violates regression assumptions (errors aren't normal)\n\n## Enter: Logistic Regression\n\n**The solution:** Transform the problem!\n\nInstead of predicting Y directly, predict the **probability** that Y = 1\n\n**The logistic function** constrains predictions between 0 and 1:\n\n$$p(X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k)}}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week10_slides_files/figure-revealjs/logistic-curve-1.png){width=960}\n:::\n:::\n\n\n## Logistic vs. Linear: Visual Comparison\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week10_slides_files/figure-revealjs/linear-vs-logistic-1.png){width=960}\n:::\n:::\n\n\n**Key difference:** Logistic regression produces valid probabilities!\n\n## When Do We Use Logistic Regression?\n\n**Perfect for binary classification problems in policy:**\n\n**Criminal Justice:**\n\n- Will someone reoffend? (recidivism)\n- Will someone appear for court? (flight risk)\n\n**Health:**\n\n- Will patient develop disease? (risk assessment)\n- Will treatment be successful? (outcome prediction)\n\n**Economics:**\n\n- Will loan default? (credit risk)\n- Will person get hired? (employment prediction)\n\n**Urban Planning:**\n\n- Will building be demolished? (blight prediction)\n- Will household participate in program? (uptake prediction)\n\n## The Logit Transformation\n\n**Behind the scenes:** We work with **log-odds**, not probabilities directly\n\n**Odds:** $\\text{Odds} = \\frac{p}{1-p}$\n\n**Log-Odds (Logit):** $\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)$\n\n**This creates a linear relationship:**\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...$$\n\n**Why this matters:**\n\n- Coefficients are log-odds (like linear regression!)\n- But we interpret as **odds ratios** when exponentiated: $e^{\\beta}$\n- OR > 1: predictor increases odds of outcome\n- OR < 1: predictor decreases odds of outcome\n\n---\n\n# Part 2: Building Our First Logistic Model {.center}\n\n## Example: Email Spam Detection\n\nLet's build a simple spam detector to understand the mechanics.\n\n**Goal:** Predict whether email is spam (1) or legitimate (0)\n\n**Predictors:**\n- Number of exclamation marks\n- Contains word \"free\"\n- Email length\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example spam detection data\nset.seed(123)\nn_emails <- 1000\n\nspam_data <- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Look at the data\nhead(spam_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  exclamation_marks contains_free    length is_spam\n1                 4             1 150.21006       1\n2                 7             1 148.00225       1\n3                 4             1 199.10099       1\n4                 8             0 193.39124       1\n5                 9             0  72.53286       1\n6                 2             1 252.02867       1\n```\n\n\n:::\n:::\n\n\n## Fitting the Logistic Model\n\n**In R, we use `glm()` with `family = \"binomial\"`**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit logistic regression\nspam_model <- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = is_spam ~ exclamation_marks + contains_free + length, \n    family = \"binomial\", data = spam_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)\n(Intercept)         233.048  15015.736   0.016    0.988\nexclamation_marks    55.945  53708.285   0.001    0.999\ncontains_free        46.055  49298.975   0.001    0.999\nlength               -1.273     81.369  -0.016    0.988\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.5017e+02  on 999  degrees of freedom\nResidual deviance: 7.5863e-07  on 996  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n```\n\n\n:::\n:::\n\n\n## Interpreting Coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficients\ncoefs <- coef(spam_model)\nprint(coefs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept) exclamation_marks     contains_free            length \n       233.048051         55.944824         46.055006         -1.272668 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Convert to odds ratios\nodds_ratios <- exp(coefs)\nprint(odds_ratios)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      (Intercept) exclamation_marks     contains_free            length \n    1.627356e+101      1.979376e+24      1.003310e+20      2.800833e-01 \n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\n- `exclamation_marks`: Each additional ! multiplies odds of spam by 1.9793761\\times 10^{24}\n- `contains_free`: Having \"free\" multiplies odds by 1.0033099\\times 10^{20}  \n- `length`: Each additional character multiplies odds by 0.2801 (shorter = more likely spam)\n\n## Making Predictions\n\n**The model outputs probabilities:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict probability for a new email\nnew_email <- data.frame(\n  exclamation_marks = 3,\n  contains_free = 1,\n  length = 150\n)\n\npredicted_prob <- predict(spam_model, newdata = new_email, type = \"response\")\ncat(\"Predicted probability of spam:\", round(predicted_prob, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted probability of spam: 1\n```\n\n\n:::\n:::\n\n\n**But now what?**\n\n- If probability = 0.723, is this spam or not?\n- We need to choose a **threshold** (cutoff)\n- Threshold = 0.5 is common default, but is it the right choice?\n\n## The Fundamental Challenge\n\n**This is where logistic regression gets interesting (and complicated):**\n\nThe model gives us probabilities, but we need to make **binary decisions**.\n\n**Question:** What probability threshold should we use to classify?\n\n- Threshold = 0.5? (common default)\n- Threshold = 0.3? (more aggressive - flag more as spam)\n- Threshold = 0.7? (more conservative - only flag obvious spam)\n\n**The answer depends on:**\n\n- Cost of false positives (marking legitimate email as spam)\n- Cost of false negatives (missing actual spam)\n- **These costs are rarely equal!**\n\n**The rest of today:** How do we evaluate these predictions and choose thresholds?\n\n---\n\n# Part 3: Evaluating Binary Predictions {.center}\n\n## From Probabilities to Decisions\n\n**We now have a model that predicts probabilities.**\n\nBut policy decisions require binary choices: spam/not spam, approve/deny, intervene/don't intervene.\n\n**This requires two steps:**\n\n1. Choose a threshold to convert probabilities → binary predictions\n2. Evaluate how good those predictions are\n\n**The confusion matrix helps us with step 2**\n\n---\n\n# Part 3a: Confusion Matrices {.center}\n\n## The Four Outcomes\n\nWhen we make binary predictions, four things can happen:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Model says \"Yes\":**\n\n- **True Positive (TP):** Correct! ✓\n- **False Positive (FP):** Wrong - Type I error\n\n**Model says \"No\":**\n\n- **True Negative (TN):** Correct! ✓  \n- **False Negative (FN):** Wrong - Type II error\n:::\n\n::: {.column width=\"50%\"}\n![Confusion Matrix Structure](images/confusion_matrix.png){width=80%}\n:::\n:::\n\n**Remember:** The model predicts probabilities. WE choose the threshold that converts probabilities to yes/no predictions.\n\n## Quick Example: COVID Testing\n\n**Scenario:** Testing for COVID-19\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\nTable: COVID Test Outcomes\n\n|True_Status |Test_Result |Outcome             |Consequence                 |\n|:-----------|:-----------|:-------------------|:---------------------------|\n|Positive    |Positive    |True Positive (TP)  |Quarantine (correct)        |\n|Positive    |Negative    |False Negative (FN) |Goes to work, spreads virus |\n|Negative    |Positive    |False Positive (FP) |Quarantines unnecessarily   |\n|Negative    |Negative    |True Negative (TN)  |Goes to work (correct)      |\n\n\n:::\n:::\n\n\n**Which error is worse?**  \n\n- False Negative → Virus spreads\n- False Positive → Unnecessary quarantine\n\n**The answer depends on context!** (And changes our threshold choice)\n\n## Calculating Performance Metrics\n\nFrom the confusion matrix, we derive metrics that emphasize different trade-offs:\n\n**Sensitivity (Recall, True Positive Rate):**\n$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n*\"Of all actual positives, how many did we catch?\"*\n\n**Specificity (True Negative Rate):**\n$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n*\"Of all actual negatives, how many did we correctly identify?\"*\n\n**Precision (Positive Predictive Value):**\n$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n*\"Of all our positive predictions, how many were correct?\"*\n\n## Interactive Example: Spam Detection\n\nLet's say we have an email spam filter:\n\n- 100 actual spam emails\n- 900 actual legitimate emails\n- Our model makes predictions...\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example predictions\nset.seed(123)\nspam_data <- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %>%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data <- spam_data %>%\n  mutate(predicted_spam = ifelse(predicted_prob > 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat <- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)\n```\n:::\n\n\n## Spam Filter Results\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n          Reference\nPrediction   0   1\n         0 760  14\n         1 140  86\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSensitivity: 0.86 - We caught 86 % of spam\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSpecificity: 0.844 - We correctly identified 84.4 % of legitimate emails\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPrecision: 0.381 - Of emails marked spam, 38.1 % actually were spam\n```\n\n\n:::\n:::\n\n\n**Question:** What happens if we change the threshold?\n\n---\n\n# Part 4: The Threshold Decision {.center}\n\n## Why Threshold Choice Matters\n\n**Remember:** The model gives us probabilities. We decide what probability triggers action.\n\n**Threshold = 0.3** (low bar)\n- More emails marked as spam\n- Higher sensitivity (catch more spam)\n- Lower specificity (more false alarms)\n\n**Threshold = 0.7** (high bar)\n- Fewer emails marked as spam\n- Lower sensitivity (miss some spam)\n- Higher specificity (fewer false alarms)\n\n**There is no \"right\" answer - it depends on the costs of each type of error**\n\n## The Great Sensitivity-Specificity Trade-off\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate metrics at different thresholds\nthresholds <- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold <- map_df(thresholds, function(thresh) {\n  preds <- ifelse(spam_data$predicted_prob > thresh, 1, 0)\n  cm <- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](week10_slides_files/figure-revealjs/threshold-demo-1.png){width=960}\n:::\n:::\n\n\n## Two Policy Scenarios\n\n**Scenario A: Rare, deadly disease screening**\n\n- Disease is rare but fatal if untreated\n- Treatment is safe with minor side effects\n- **Goal:** Don't miss any cases (high sensitivity)\n- **Acceptable:** Some false positives (low threshold)\n\n**Scenario B: Identifying \"high-risk\" individuals for intervention**\n\n- Limited intervention slots\n- False positives waste resources\n- False negatives miss opportunities to help\n- **Goal:** Use resources efficiently (high precision)\n- **Decision depends on:** Cost of intervention vs. cost of missed case\n\n**Class discussion:** Which metrics matter most for each scenario?\n\n---\n\n# Part 5: ROC Curves {.center}\n\n## The ROC Curve: Visualizing All Thresholds\n\n**ROC = Receiver Operating Characteristic**  \n\n(Originally developed for radar signal detection in WWII)\n\n**What it shows:**\n\n- Every possible threshold\n- Trade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)\n- Overall model discrimination ability\n\n**How to read it:**\n\n- X-axis: False Positive Rate (1 - Specificity)\n- Y-axis: True Positive Rate (Sensitivity)\n- Diagonal line: Random guessing\n- Top-left corner: Perfect prediction\n\n## Creating an ROC Curve\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ROC curve for our spam example\nroc_obj <- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](week10_slides_files/figure-revealjs/roc-curve-1.png){width=960}\n:::\n\n```{.r .cell-code}\n# Print AUC\nauc_value <- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nArea Under the Curve (AUC): 0.938\n```\n\n\n:::\n:::\n\n\n## Interpreting AUC\n\n**AUC (Area Under the Curve)** summarizes overall model performance:\n\n- **AUC = 1.0:** Perfect classifier\n- **AUC = 0.9-1.0:** Excellent\n- **AUC = 0.8-0.9:** Good\n- **AUC = 0.7-0.8:** Acceptable\n- **AUC = 0.6-0.7:** Poor\n- **AUC = 0.5:** No better than random guessing\n- **AUC < 0.5:** Worse than random (your model is backwards!)\n\n**Our spam filter AUC = 0.938**\n\n**Interpretation:** The model has good discrimination ability, but...\n\n- AUC doesn't tell us which threshold to use\n- AUC doesn't account for class imbalance\n- AUC doesn't show us equity implications\n\n## Understanding the ROC Curve Points\n\n![](images/roc.png)\n\n---\n\n# Part 6: Equity Considerations {.center}\n\n## The Core Problem: Disparate Impact\n\n**A model can be \"accurate\" overall but perform very differently across groups**\n\nExample metrics from a recidivism model:\n\n| Group | Sensitivity | Specificity | False Positive Rate |\n|-------|-------------|-------------|---------------------|\n| Overall | 0.72 | 0.68 | 0.32 |\n| Group A | 0.78 | 0.74 | 0.26 |\n| Group B | 0.64 | 0.58 | 0.42 |\n\n**Group B experiences:**\n\n- Lower sensitivity (more people who will reoffend are missed)\n- Lower specificity (more people who won't reoffend are flagged)\n- Higher false positive rate (more unjust interventions)\n\n**This is algorithmic bias in action**\n\n## Real-World Case: COMPAS\n\n**COMPAS:** Commercial algorithm used in criminal justice to predict recidivism\n\n**ProPublica investigation (2016) found:**\n\n- Similar overall accuracy for Black and White defendants\n- BUT: False positive rates differed dramatically\n  - Black defendants: 45% false positive rate\n  - White defendants: 23% false positive rate\n- Black defendants twice as likely to be incorrectly labeled \"high risk\"\n\n**Result:** \n\n- Different threshold needed for different groups to achieve equity\n- But single-threshold systems are the norm\n- **Key insight:** Overall accuracy masks disparate impact\n\n---\n\n# How to Choose a Threshold {.center}\n\n## Framework for Threshold Selection\n\n**Step 1: Understand the consequences**\n\n- What happens with a false positive?\n- What happens with a false negative?\n- Are costs symmetric or asymmetric?\n\n**Step 2: Consider stakeholder perspectives**\n\n- Who is affected by each type of error?\n- Do all groups experience consequences equally?\n\n**Step 3: Choose your metric priority**\n\n- Maximize sensitivity? (catch all positives)\n- Maximize specificity? (minimize false alarms)\n- Balance precision and recall? (F1 score)\n- Equalize across groups?\n\n**Step 4: Test multiple thresholds**\n\n- Evaluate performance across thresholds\n- Look at group-wise performance\n- Consider sensitivity analysis\n\n## Cost-Benefit Analysis Approach\n\n**Assign concrete costs to errors:**\n\nExample: Disease screening\n\n- True Positive: Treatment cost $1000, prevent $50,000 in complications\n- False Positive: Unnecessary treatment $1000\n- True Negative: No cost\n- False Negative: Miss disease, $50,000 in complications later\n\n**Calculate expected cost at each threshold:**\n$$E[\\text{Cost}] = C_{FP} \\times FP + C_{FN} \\times FN$$\n\n**Choose threshold that minimizes expected cost**\n\n*Note: This assumes we can quantify all costs, which is often impossible for justice/equity concerns*\n\n---\n\n## Practical Recommendations\n\n1. **Report multiple metrics** - not just accuracy\n2. **Show the ROC curve** - demonstrates trade-offs\n3. **Test multiple thresholds** - document your choice\n4. **Evaluate by sub-group** - check for disparate impact\n5. **Document assumptions** - explain why you chose your threshold\n6. **Consider context** - what are the real-world consequences?\n7. **Provide uncertainty** - confidence intervals, not just point estimates\n8. **Enable recourse** - can predictions be challenged?\n\n**Most importantly: Be transparent about limitations and potential harms**\n\n---\n\n\n",
    "supporting": [
      "week10_slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}