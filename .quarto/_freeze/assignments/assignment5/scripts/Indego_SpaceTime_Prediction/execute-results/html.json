{
  "hash": "199ce5ec99fab267fc61aca413fb0818",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Itsnatani Humaira Anaqami\"\ndate: \"2025-12-01\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Introduction to Bike Share and Rebalancing Plan\n\n## Bike Share Systems\nBike-share systems are public transportation services where users can borrow bicycles for short-term use at a low cost. Bicycles are typically parked at stations or designated parking areas (docking stations) and can be picked up from one station and returned to another within the network. These systems are designed for short urban trips and help reduce traffic congestion, and are often part of sustainable transportation strategies in large cities.\n\n## The Rebalancing Challenge in Philadelphia\nOne A primary challenge in bike share system is managing the uneven distribution of bikes, known as rebalancing challenges. Demand fluctuations often lead to stations being completely full (no space to return a bike) or completely empty (no bikes available to rent). Operators spend a large portion of their budget using vehicles to manually move bikes to meet anticipated demand, and some try to incentivize users to do the rebalancing themselves. \n\nPhiladelphia has a bike share system called IndeGo. Philadelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. \n\nImagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have:\n- 200 stations across Philadelphia\n- Limited trucks and staff for moving bikes\n- 2-3 hours before morning rush hour demand peaks\n- **The question:** Which stations will run out of bikes by 8:30 AM?\n\nThis report presents a predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.\n\n## Learning Objectives\n\nThe learning objectives of this assignment are to:\n\n1. **Understand panel data structure** for space-time analysis\n2. **Create temporal lag variables** to capture demand persistence\n3. **Build multiple predictive models** with increasing complexity\n4. **Validate models temporally** (train on past, test on future)\n5. **Analyze prediction errors** in both space and time\n6. **Engineer new features** based on error patterns\n7. **Critically evaluate** when prediction errors matter most\n\n## Assignment Structure\n\n**Part 1:** Create prediction model with Q1 2025 data\n- Download Q1 2025 Indego data\n- Exploratory Analysis\n- Analyze error patterns\n\n**Part 2:** Create prediction model with Q3 2024 data\n- Download Q3 2024 Indego data\n- Exploratory Analysis\n- Analyze error patterns\n- Add 2-3 new features to improve the model\n- Critical Reflection\n\n---\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. \noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"28da3bddd17210421250559be63d8c9379ee190c\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n# PART 1: Data Import & Preparation (Q1 2025)\n\n## Load Indego Trip Data (Q1 2025)\n\nIn this part, I loaded Q1 2025 Indego trip data that can be retrieved from: https://www.rideindego.com/about/data/\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q1 2025 data\nindego <- read_csv(here(\"assignments/assignment5/data/indego-trips-2025-q1.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 201,588\nColumns: 15\n$ trip_id             <dbl> 1123985990, 1123986350, 1124202498, 1123986241, 11…\n$ duration            <dbl> 5, 14, 894, 9, 13, 11, 15, 19, 20, 24, 26, 11, 12,…\n$ start_time          <chr> \"1/1/2025 0:00\", \"1/1/2025 0:04\", \"1/1/2025 0:05\",…\n$ end_time            <chr> \"1/1/2025 0:05\", \"1/1/2025 0:18\", \"1/1/2025 14:59\"…\n$ start_station       <dbl> 3371, 3344, 3021, 3253, 3182, 3346, 3049, 3112, 31…\n$ start_lat           <dbl> 39.95340, 39.95961, 39.95390, 39.93074, 39.95081, …\n$ start_lon           <dbl> -75.15430, -75.23354, -75.16902, -75.18924, -75.16…\n$ end_station         <dbl> 3378, 3294, 3073, 3253, 3249, 3000, 3100, 3035, 33…\n$ end_lat             <dbl> 39.95238, 39.95174, 39.96143, 39.93074, 39.95784, …\n$ end_lon             <dbl> -75.14728, -75.17063, -75.15242, -75.18924, -75.19…\n$ bike_id             <chr> \"22580\", \"31417\", \"31393\", \"31434\", \"22872\", \"1187…\n$ plan_duration       <dbl> 30, 365, 1, 30, 1, 365, 30, 30, 365, 365, 30, 365,…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Indego365\", \"Walk-up\", \"Indego30\", \"W…\n$ bike_type           <chr> \"electric\", \"electric\", \"electric\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q1 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q1 2025: 201588 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1735689600 to 1743464880 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 265 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    190792      10796 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex    Walk-up \n      5494      94044      91628          3      10419 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  129561    72027 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-01-01 00:00:00 2025-01-01 00:00:00     1 Wed       0       0\n2 2025-01-01 00:04:00 2025-01-01 00:00:00     1 Wed       0       0\n3 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n4 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n5 2025-01-01 00:08:00 2025-01-01 00:00:00     1 Wed       0       0\n6 2025-01-01 00:14:00 2025-01-01 00:00:00     1 Wed       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q1 2025\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Question:** What patterns do you see? How does ridership change over time?\n\nThe overall trend (red dashed line) shows a steady increase in daily bike-share trips from January through March, although the graph fluctuates days to days. This indicates that demand gradually rises through winter and into early spring. We can also see a major spike on February 14th, 2025—a day that coincided with both Valentine’s Day and the Super Bowl.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfly_eagles_fly <- daily_trips %>% filter(date == \"2025-02-14\")\n\ntypical_boring_friday <- indego %>%\n  filter(dotw == \"Fri\", date != \"2025-02-14\") %>%\n  group_by(date) %>%\n  summarize(trips = n()) %>%\n  summarize(avg_friday_trips = mean(trips))\n\nprint(fly_eagles_fly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  date       trips\n  <date>     <int>\n1 2025-02-14  4192\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(typical_boring_friday)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  avg_friday_trips\n             <dbl>\n1            2319.\n```\n\n\n:::\n:::\n\n\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Question:** When are the peak hours? How do weekends differ from weekdays?\n\nOn weekdays, the peak hours occur around 7 AM and 4 PM. These times align with the typical start and end of the work or school day, showing strong commuter-driven demand.\n\nOn weekends, the pattern is very different. Ridership rises gradually starting around 5 AM, reaches its peak around 3 PM, and then slowly declines as the day ends. This might show that weekend trips are more leisure-oriented rather than tied to commuting schedules.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 3,999 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 2,842 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 2,699 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 2,673 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 2,503 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 2,486 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 2,396 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 2,387 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 2,361 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 2,348 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 2,278 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 2,274 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 2,160 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 2,123 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 2,116 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,038 </td>\n   <td style=\"text-align:right;\"> 39.94781 </td>\n   <td style=\"text-align:right;\"> -75.19409 </td>\n   <td style=\"text-align:right;\"> 2,111 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,203 </td>\n   <td style=\"text-align:right;\"> 39.94077 </td>\n   <td style=\"text-align:right;\"> -75.17227 </td>\n   <td style=\"text-align:right;\"> 2,106 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 2,027 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 2,014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 2,014 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nIn this part, We'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  show_call = FALSE,\n  progress = FALSE\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-01-01\",\n  date_end = \"2025-03-31\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :10.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:30.00   1st Qu.:0.000000   1st Qu.: 5.000  \n Median :37.00   Median :0.000000   Median : 8.000  \n Mean   :38.66   Mean   :0.005459   Mean   : 9.047  \n 3rd Qu.:47.00   3rd Qu.:0.000000   3rd Qu.:12.000  \n Max.   :78.00   Max.   :0.710000   Max.   :30.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q1 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 116718\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 245\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2150\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 526,750 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 116,718 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 410,032 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 526,750 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count     Temperature   Precipitation  \n Min.   : 0.00   Min.   :10.0   Min.   :0.000  \n 1st Qu.: 0.00   1st Qu.:30.0   1st Qu.:0.000  \n Median : 0.00   Median :37.0   Median :0.000  \n Mean   : 0.34   Mean   :38.7   Mean   :0.005  \n 3rd Qu.: 0.00   3rd Qu.:47.0   3rd Qu.:0.000  \n Max.   :26.00   Max.   :78.0   Max.   :0.710  \n                 NA's   :5880   NA's   :5880   \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 635,775 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data.\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q1 has weeks 1-13 (Jan-Mar)\n# Train on weeks 1-9 (Jan 1 - early March)\n# Test on weeks 10-13 (rest of March)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 10) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 10) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 10)\n\ntest <- study_panel_complete %>%\n  filter(week >= 10)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 428,400 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 189,210 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20089 to 20151 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20152 to 20178 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8820 -0.3802 -0.1791  0.0128 18.5540 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.1488732  0.0077026 -19.328 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0182693  0.0078250  -2.335               0.0196 *  \nas.factor(hour)2  -0.0401185  0.0078013  -5.143   0.0000002711413737 ***\nas.factor(hour)3  -0.0534144  0.0077758  -6.869   0.0000000000064582 ***\nas.factor(hour)4  -0.0420757  0.0078905  -5.332   0.0000000969595824 ***\nas.factor(hour)5   0.0145021  0.0078916   1.838               0.0661 .  \nas.factor(hour)6   0.1551150  0.0078162  19.845 < 0.0000000000000002 ***\nas.factor(hour)7   0.2839418  0.0077451  36.661 < 0.0000000000000002 ***\nas.factor(hour)8   0.5012789  0.0077165  64.962 < 0.0000000000000002 ***\nas.factor(hour)9   0.3598210  0.0078159  46.037 < 0.0000000000000002 ***\nas.factor(hour)10  0.2460810  0.0076647  32.106 < 0.0000000000000002 ***\nas.factor(hour)11  0.2676516  0.0077151  34.692 < 0.0000000000000002 ***\nas.factor(hour)12  0.3345613  0.0077972  42.908 < 0.0000000000000002 ***\nas.factor(hour)13  0.3335884  0.0078885  42.288 < 0.0000000000000002 ***\nas.factor(hour)14  0.3298815  0.0076963  42.862 < 0.0000000000000002 ***\nas.factor(hour)15  0.3889267  0.0077546  50.155 < 0.0000000000000002 ***\nas.factor(hour)16  0.4812228  0.0077566  62.041 < 0.0000000000000002 ***\nas.factor(hour)17  0.5848969  0.0077931  75.053 < 0.0000000000000002 ***\nas.factor(hour)18  0.4003396  0.0075825  52.798 < 0.0000000000000002 ***\nas.factor(hour)19  0.2521779  0.0077095  32.710 < 0.0000000000000002 ***\nas.factor(hour)20  0.1373139  0.0074848  18.346 < 0.0000000000000002 ***\nas.factor(hour)21  0.0880114  0.0075923  11.592 < 0.0000000000000002 ***\nas.factor(hour)22  0.0580815  0.0076060   7.636   0.0000000000000224 ***\nas.factor(hour)23  0.0350222  0.0076227   4.594   0.0000043403255736 ***\ndotw_simple2       0.0651434  0.0043665  14.919 < 0.0000000000000002 ***\ndotw_simple3       0.0430540  0.0043119   9.985 < 0.0000000000000002 ***\ndotw_simple4       0.0505750  0.0041306  12.244 < 0.0000000000000002 ***\ndotw_simple5       0.0434119  0.0041584  10.440 < 0.0000000000000002 ***\ndotw_simple6      -0.0672427  0.0042085 -15.978 < 0.0000000000000002 ***\ndotw_simple7      -0.0631357  0.0041631 -15.166 < 0.0000000000000002 ***\nTemperature        0.0069235  0.0001365  50.722 < 0.0000000000000002 ***\nPrecipitation     -2.4416620  0.0939158 -25.998 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7216 on 428368 degrees of freedom\nMultiple R-squared:  0.07598,\tAdjusted R-squared:  0.07592 \nF-statistic:  1136 on 31 and 428368 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays have positive coefficients (0.029 to 0.052)\n- Tuesday has the highest weekday effect (+0.052)\n- Weekdays likely benefit from concentrated commuting patterns\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.061 and -0.065)\n- This means FEWER trips per station-hour than Monday\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1      -0.018       slightly fewer than midnight\n...\n6      +0.151       morning activity starting\n7      +0.276       morning rush building\n8      +0.487       PEAK morning rush\n9      +0.350       post-rush\n...\n17     +0.568       PEAK evening rush (5 PM!)\n18     +0.389       evening declining\n...\n23     +0.034       late night minimal\n\nIsn't this fun!\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1487 -0.2595 -0.0970  0.0235 17.5255 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.0944031  0.0069331 -13.616 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0028329  0.0070412  -0.402             0.687436    \nas.factor(hour)2  -0.0113676  0.0070205  -1.619             0.105403    \nas.factor(hour)3  -0.0233665  0.0069984  -3.339             0.000841 ***\nas.factor(hour)4  -0.0014341  0.0071021  -0.202             0.839977    \nas.factor(hour)5   0.0463719  0.0071041   6.528 0.000000000066943682 ***\nas.factor(hour)6   0.1484482  0.0070401  21.086 < 0.0000000000000002 ***\nas.factor(hour)7   0.2157558  0.0069815  30.904 < 0.0000000000000002 ***\nas.factor(hour)8   0.3617314  0.0069664  51.925 < 0.0000000000000002 ***\nas.factor(hour)9   0.1603371  0.0070640  22.698 < 0.0000000000000002 ***\nas.factor(hour)10  0.1024525  0.0069124  14.822 < 0.0000000000000002 ***\nas.factor(hour)11  0.1360277  0.0069632  19.535 < 0.0000000000000002 ***\nas.factor(hour)12  0.2021688  0.0070303  28.757 < 0.0000000000000002 ***\nas.factor(hour)13  0.1900074  0.0071126  26.714 < 0.0000000000000002 ***\nas.factor(hour)14  0.1963837  0.0069380  28.305 < 0.0000000000000002 ***\nas.factor(hour)15  0.2435050  0.0069932  34.820 < 0.0000000000000002 ***\nas.factor(hour)16  0.3042270  0.0070018  43.450 < 0.0000000000000002 ***\nas.factor(hour)17  0.3749605  0.0070436  53.234 < 0.0000000000000002 ***\nas.factor(hour)18  0.1792367  0.0068606  26.126 < 0.0000000000000002 ***\nas.factor(hour)19  0.0973764  0.0069608  13.989 < 0.0000000000000002 ***\nas.factor(hour)20  0.0389727  0.0067549   5.770 0.000000007955345969 ***\nas.factor(hour)21  0.0320507  0.0068394   4.686 0.000002784344470639 ***\nas.factor(hour)22  0.0310219  0.0068459   4.531 0.000005859991631025 ***\nas.factor(hour)23  0.0221611  0.0068594   3.231             0.001235 ** \ndotw_simple2       0.0271522  0.0039314   6.907 0.000000000004972450 ***\ndotw_simple3       0.0147801  0.0038816   3.808             0.000140 ***\ndotw_simple4       0.0206099  0.0037187   5.542 0.000000029877813199 ***\ndotw_simple5       0.0127648  0.0037443   3.409             0.000652 ***\ndotw_simple6      -0.0521272  0.0037909 -13.750 < 0.0000000000000002 ***\ndotw_simple7      -0.0303227  0.0037485  -8.089 0.000000000000000602 ***\nTemperature        0.0029026  0.0001236  23.486 < 0.0000000000000002 ***\nPrecipitation     -1.3850445  0.0846392 -16.364 < 0.0000000000000002 ***\nlag1Hour           0.3247918  0.0014556 223.135 < 0.0000000000000002 ***\nlag3Hours          0.0975134  0.0014354  67.936 < 0.0000000000000002 ***\nlag1day            0.1659090  0.0013907 119.300 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6493 on 428365 degrees of freedom\nMultiple R-squared:  0.2519,\tAdjusted R-squared:  0.2518 \nF-statistic:  4241 on 34 and 428365 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Did adding lags improve R²? Why or why not?\nYes, it improves R² because adding lags captures past patterns in the data, reducing unexplained variance and making the model fit the observed values better.\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0188 -0.4829 -0.2267  0.2771 16.9058 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.9225844734  0.0352000581  26.210\nas.factor(hour)1          0.0037772168  0.0441610229   0.086\nas.factor(hour)2         -0.0955075476  0.0487102008  -1.961\nas.factor(hour)3         -0.1340354597  0.0623637654  -2.149\nas.factor(hour)4         -0.1416017281  0.0554770480  -2.552\nas.factor(hour)5         -0.1300062643  0.0388982897  -3.342\nas.factor(hour)6          0.0360772463  0.0331031436   1.090\nas.factor(hour)7          0.1328270695  0.0317248940   4.187\nas.factor(hour)8          0.3866084171  0.0308539302  12.530\nas.factor(hour)9         -0.0445315405  0.0310696933  -1.433\nas.factor(hour)10        -0.0847033567  0.0313859062  -2.699\nas.factor(hour)11        -0.0290106009  0.0314061584  -0.924\nas.factor(hour)12         0.0385416417  0.0310913865   1.240\nas.factor(hour)13         0.0421197178  0.0311715057   1.351\nas.factor(hour)14         0.0117664453  0.0307921034   0.382\nas.factor(hour)15         0.0826672722  0.0306447205   2.698\nas.factor(hour)16         0.1940771623  0.0304962350   6.364\nas.factor(hour)17         0.3048041726  0.0304046610  10.025\nas.factor(hour)18         0.0002652992  0.0305035090   0.009\nas.factor(hour)19        -0.0634666243  0.0312457890  -2.031\nas.factor(hour)20        -0.1392336862  0.0318719442  -4.369\nas.factor(hour)21        -0.1321162902  0.0327955752  -4.028\nas.factor(hour)22        -0.1054260206  0.0336780868  -3.130\nas.factor(hour)23        -0.0569114464  0.0351511626  -1.619\ndotw_simple2              0.0229986016  0.0122313741   1.880\ndotw_simple3             -0.0175461556  0.0122262223  -1.435\ndotw_simple4             -0.0442688138  0.0115791517  -3.823\ndotw_simple5              0.0027339310  0.0117287820   0.233\ndotw_simple6             -0.1108106236  0.0125102844  -8.858\ndotw_simple7             -0.0398429023  0.0126913144  -3.139\nTemperature               0.0039381085  0.0003773197  10.437\nPrecipitation            -4.4263561137  0.3064039720 -14.446\nlag1Hour                  0.2204571054  0.0028934647  76.191\nlag3Hours                 0.0583071946  0.0031516027  18.501\nlag1day                   0.1447413119  0.0029839179  48.507\nMed_Inc.x                 0.0000004207  0.0000001117   3.767\nPercent_Taking_Transit.y -0.0011705604  0.0004209381  -2.781\nPercent_White.y           0.0018451795  0.0002099438   8.789\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.931838    \nas.factor(hour)2                     0.049914 *  \nas.factor(hour)3                     0.031617 *  \nas.factor(hour)4                     0.010699 *  \nas.factor(hour)5                     0.000832 ***\nas.factor(hour)6                     0.275785    \nas.factor(hour)7               0.000028315277 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9                     0.151782    \nas.factor(hour)10                    0.006961 ** \nas.factor(hour)11                    0.355633    \nas.factor(hour)12                    0.215118    \nas.factor(hour)13                    0.176627    \nas.factor(hour)14                    0.702369    \nas.factor(hour)15                    0.006985 ** \nas.factor(hour)16              0.000000000198 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18                    0.993061    \nas.factor(hour)19                    0.042237 *  \nas.factor(hour)20              0.000012523294 ***\nas.factor(hour)21              0.000056188260 ***\nas.factor(hour)22                    0.001746 ** \nas.factor(hour)23                    0.105441    \ndotw_simple2                         0.060071 .  \ndotw_simple3                         0.151255    \ndotw_simple4                         0.000132 ***\ndotw_simple5                         0.815688    \ndotw_simple6             < 0.0000000000000002 ***\ndotw_simple7                         0.001694 ** \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.000165 ***\nPercent_Taking_Transit.y             0.005423 ** \nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9136 on 83854 degrees of freedom\n  (344508 observations deleted due to missingness)\nMultiple R-squared:  0.1706,\tAdjusted R-squared:  0.1702 \nF-statistic:   466 on 37 and 83854 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.1922707 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.1896529 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?** \nStation fixed effects capture baseline differences in demand across stations—essentially accounting for the fact that some stations are inherently busier or quieter, independent of other variables in the model.\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.1968772 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.1942455 \n```\n\n\n:::\n:::\n\n\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\nOf all the five models, the largest improvement in MAE comes from adding temporal lags.\n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Question:** Where is the model performing well? Where is it struggling?\nBased on the scatterplots and the green model-fit lines, the model performs well for low trip volumes and during stable periods like overnight and mid-day.\nHpwever, it struggles during AM/PM rush hours and evening, where trips spike. The model underpredicts high-demand situations.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/spatial_errors-2.png){width=672}\n:::\n:::\n\n\n**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?\nYes, errors are spatially clustered. The highest MAE values (darker purple points) appear around Center City, University City, and nearby high-activity areas. These are busy neighborhoods with more variable trip patterns, so that might be the reason why the model struggles more in there.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\n\nYes. Based on the MAE plots, they show small but systematic differences in prediction errors across demographic groups:\n\n-     Higher-income areas: slightly higher errors.\n-     High-transit areas: noticeably lower errors; low-transit areas have higher errors.\n-     Areas with more White residents: slightly higher errors.\n\nEquity implications:\nUneven accuracy means some communities—especially low-transit or less urban areas—may receive less reliable predictions, which can lead to unequal service quality or resource allocation.\n\n---\n\n# Part 2: Data Import & Preparation (Q3 2024)\n   \n## Load Indego Trip Data (Q3 2024)\n\nIn this section, I use Q3 Indego data because I want to examine space–time patterns of bike-share demand during a different season other than winter (Q1). By using summer data, I can assess how significant the seasonal differences are—especially since many students in Philadelphia are on a long break during this period. I adapt the same code as in Part 1, then I compare the result of the analysis of Q1 2025 data vs Q3 2024 data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q3 2024 data\nindego2024 <- read_csv(here(\"assignments/assignment5/data/indego-trips-2024-q3.csv\"))\n\n# Quick look at the data\nglimpse(indego2024)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408,408\nColumns: 15\n$ trip_id             <dbl> 953311130, 953308580, 953309085, 953334269, 953334…\n$ duration            <dbl> 5, 3, 2, 12, 22, 47, 111, 7, 1, 8, 6, 2, 20, 246, …\n$ start_time          <chr> \"7/1/2024 0:02\", \"7/1/2024 0:03\", \"7/1/2024 0:04\",…\n$ end_time            <chr> \"7/1/2024 0:07\", \"7/1/2024 0:06\", \"7/1/2024 0:06\",…\n$ start_station       <dbl> 3271, 3304, 3052, 3323, 3209, 3101, 3338, 3373, 33…\n$ start_lat           <dbl> 39.94760, 39.94234, 39.94732, 40.02796, 39.94900, …\n$ start_lon           <dbl> -75.22946, -75.15399, -75.15695, -75.22770, -75.21…\n$ end_station         <dbl> 3338, 3028, 3007, 3323, 3344, 3296, 3258, 3201, 33…\n$ end_lat             <dbl> 39.95098, 39.94061, 39.94517, 40.02796, 39.95961, …\n$ end_lon             <dbl> -75.23035, -75.14958, -75.15993, -75.22770, -75.23…\n$ bike_id             <chr> \"03439\", \"11905\", \"26031\", \"19971\", \"20921\", \"2241…\n$ plan_duration       <dbl> 30, 30, 30, 1, 30, 1, 30, 30, 30, 30, 30, 30, 30, …\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Day Pass\", \"I…\n$ bike_type           <chr> \"standard\", \"standard\", \"electric\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q3 2024:\", nrow(indego2024), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q3 2024: 408408 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego2024$start_time)), \"to\", \n    max(mdy_hm(indego2024$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1719792120 to 1727740740 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations in Q3 2024:\", length(unique(indego2024$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations in Q3 2024: 261 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego2024$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    380939      27469 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego2024$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Day Pass  Indego30 Indego365   Walk-up \n    22885    239857    132358     13308 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego2024$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  236839   171569 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego2024 <- indego2024 %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego2024 %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-07-01 00:02:00 2024-07-01 00:00:00    27 Mon       0       0\n2 2024-07-01 00:03:00 2024-07-01 00:00:00    27 Mon       0       0\n3 2024-07-01 00:04:00 2024-07-01 00:00:00    27 Mon       0       0\n4 2024-07-01 00:05:00 2024-07-01 00:00:00    27 Mon       0       0\n5 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n6 2024-07-01 00:06:00 2024-07-01 00:00:00    27 Mon       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips2024 <- indego2024 %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips2024, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q3 2024\",\n    subtitle = \"Summer demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/trips_over_time2024-1.png){width=672}\n:::\n:::\n\n\n**Question:** What patterns do you see? How does ridership change over time?\n\nOverall daily trips generally range between 3,000 and 5,500. The trend line (red dash) shows a slight decline through July, a leveling in August, and then a gradual rise into early September before tapering toward October. This pattern might show typical summer usage with fluctuations driven by weekly rhythms and seasonal transitions.\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns2024 <- indego2024 %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns2024, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Summer Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/hourly_patterns2024-1.png){width=672}\n:::\n:::\n\n\n**Question:** When are the peak hours? How do weekends differ from weekdays?\n\nSimiliar to Q1, on weekdays, the peak hours are around 7 AM and 4 PM. These times align with the typical start and end of the work or school day, showing strong commuter-driven demand.\nOn weekends, the pattern is different. Ridership rises gradually starting around 5 AM, reaches its peak around 3 PM, and then slowly declines as the day ends.\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations2024 <- indego2024 %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations2024, \n      caption = \"Top 20 Indego Stations by Trip Origins Summer 2024\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins Summer 2024</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,654 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 5,436 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 4,421 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,305 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 4,252 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 4,226 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 4,154 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 4,144 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 4,114 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 4,022 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,954 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,948 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 3,942 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 3,921 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 3,919 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,796 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,046 </td>\n   <td style=\"text-align:right;\"> 39.95012 </td>\n   <td style=\"text-align:right;\"> -75.14472 </td>\n   <td style=\"text-align:right;\"> 3,734 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,726 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,678 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nIn this part, We'll get census tract data to add demographic context to the Q3 2024 stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  show_call = FALSE,\n  progress = FALSE\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego2024,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/map_philly2024-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations2024_sf <- indego2024 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations2024_census <- st_join(stations2024_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations2024_for_map <- indego2024 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations2024_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego2024_census <- indego2024 %>%\n  left_join(\n    stations2024_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations2024_for_map <- indego2024 %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations2024_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations2024_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations2024_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/join_census_to_stations2024-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\nSame like Part 1, we need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations2024 <- stations2024_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego2024_census <- indego2024 %>%\n  filter(start_station %in% valid_stations2024) %>%\n  left_join(\n    stations2024_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q3 2024: July 1 - September 30\nweather_data2024 <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-07-01\",\n  date_end = \"2024-09-30\"\n)\n\n# Process weather data\nweather2024_processed <- weather_data2024 %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather2024_complete <- weather2024_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather2024_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :55.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:70.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :76.00   Median :0.000000   Median : 7.000  \n Mean   :75.59   Mean   :0.007896   Mean   : 6.893  \n 3rd Qu.:81.00   3rd Qu.:0.000000   3rd Qu.: 9.000  \n Max.   :98.00   Max.   :1.250000   Max.   :44.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly summer?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather2024_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q3 2024\",\n    subtitle = \"Summer to early fall transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/visualize_weather2024-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel2024 <- indego2024_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel2024)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 193072\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel2024$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 241\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel2024$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2202\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations2024 <- length(unique(trips_panel2024$start_station))\nn_hours2024 <- length(unique(trips_panel2024$interval60))\nexpected_rows2024 <- n_stations2024 * n_hours2024\n\ncat(\"Expected panel rows:\", format(expected_rows2024, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 530,682 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 193,072 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows2024 - nrow(trips_panel2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 337,610 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel2024 <- expand.grid(\n  interval60 = unique(trips_panel2024$interval60),\n  start_station = unique(trips_panel2024$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel2024, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes2024 <- trips_panel2024 %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel2024 <- study_panel2024 %>%\n  left_join(station_attributes2024, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 530,682 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel2024 <- study_panel2024 %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel2024 <- study_panel2024 %>%\n  left_join(weather2024_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel2024 %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation  \n Min.   : 0.0000   Min.   :55.00   Min.   :0.000  \n 1st Qu.: 0.0000   1st Qu.:70.00   1st Qu.:0.000  \n Median : 0.0000   Median :76.00   Median :0.000  \n Mean   : 0.7068   Mean   :75.59   Mean   :0.008  \n 3rd Qu.: 1.0000   3rd Qu.:81.00   3rd Qu.:0.000  \n Max.   :24.0000   Max.   :98.00   Max.   :1.250  \n                   NA's   :5784    NA's   :5784   \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel2024 <- study_panel2024 %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel2024 <- study_panel2024 %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete2024 <- study_panel2024 %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 646,362 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station2024 <- study_panel_complete2024 %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station2024, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station Q3 2024\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/lag_correlations2024-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Temporal Train/Test Split\n\nWe will train on PAST data and test on FUTURE data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q3 has weeks 27-40 (Jul-Sep)\n# Train on weeks 27-36 (Jul 1 - early September)\n# Test on weeks 37-40 (rest of September)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations2024 <- study_panel_complete2024 %>%\n  filter(week < 37) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations2024 <- study_panel_complete2024 %>%\n  filter(week >= 37) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations2024 <- intersect(early_stations2024, late_stations2024)\n\n# Filter panel to only common stations\nstudy_panel_complete2024 <- study_panel_complete2024 %>%\n  filter(start_station %in% common_stations2024)\n\n# NOW create train/test split\ntrain2024 <- study_panel_complete2024 %>%\n  filter(week < 37)\n\ntest2024 <- study_panel_complete2024 %>%\n  filter(week >= 37)\n\ncat(\"Training observations:\", format(nrow(train2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 470,940 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test2024), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 159,330 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train2024$date), \"to\", max(train2024$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19905 to 19974 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test2024$date), \"to\", max(test2024$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 19975 to 19996 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nLike in part 1, we'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model A: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain2024 <- train2024 %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train2024$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel_a <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train2024\n)\n\nsummary(model_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train2024)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6920 -0.7833 -0.1971  0.1820 23.4842 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.3229123  0.0259386  12.449 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0962853  0.0121814  -7.904   0.0000000000000027 ***\nas.factor(hour)2  -0.1268306  0.0123872 -10.239 < 0.0000000000000002 ***\nas.factor(hour)3  -0.1825815  0.0126803 -14.399 < 0.0000000000000002 ***\nas.factor(hour)4  -0.1666676  0.0124958 -13.338 < 0.0000000000000002 ***\nas.factor(hour)5  -0.0380285  0.0123998  -3.067             0.002163 ** \nas.factor(hour)6   0.2156086  0.0125010  17.247 < 0.0000000000000002 ***\nas.factor(hour)7   0.5023523  0.0125359  40.073 < 0.0000000000000002 ***\nas.factor(hour)8   0.9175652  0.0124731  73.564 < 0.0000000000000002 ***\nas.factor(hour)9   0.6584226  0.0125894  52.300 < 0.0000000000000002 ***\nas.factor(hour)10  0.5391414  0.0123822  43.542 < 0.0000000000000002 ***\nas.factor(hour)11  0.5668137  0.0125180  45.280 < 0.0000000000000002 ***\nas.factor(hour)12  0.6907433  0.0124549  55.460 < 0.0000000000000002 ***\nas.factor(hour)13  0.6901722  0.0122185  56.486 < 0.0000000000000002 ***\nas.factor(hour)14  0.7335880  0.0122197  60.033 < 0.0000000000000002 ***\nas.factor(hour)15  0.7974676  0.0122500  65.100 < 0.0000000000000002 ***\nas.factor(hour)16  1.0183442  0.0125627  81.061 < 0.0000000000000002 ***\nas.factor(hour)17  1.3647585  0.0126393 107.977 < 0.0000000000000002 ***\nas.factor(hour)18  1.0673821  0.0124813  85.519 < 0.0000000000000002 ***\nas.factor(hour)19  0.8576101  0.0124960  68.631 < 0.0000000000000002 ***\nas.factor(hour)20  0.6099223  0.0121730  50.104 < 0.0000000000000002 ***\nas.factor(hour)21  0.3848980  0.0122492  31.422 < 0.0000000000000002 ***\nas.factor(hour)22  0.2710948  0.0120278  22.539 < 0.0000000000000002 ***\nas.factor(hour)23  0.1479533  0.0126837  11.665 < 0.0000000000000002 ***\ndotw_simple2       0.0763268  0.0070091  10.890 < 0.0000000000000002 ***\ndotw_simple3       0.0794375  0.0068830  11.541 < 0.0000000000000002 ***\ndotw_simple4       0.0170159  0.0069075   2.463             0.013763 *  \ndotw_simple5      -0.0314724  0.0068601  -4.588   0.0000044815445072 ***\ndotw_simple6      -0.0574657  0.0068917  -8.338 < 0.0000000000000002 ***\ndotw_simple7      -0.1030374  0.0068825 -14.971 < 0.0000000000000002 ***\nTemperature       -0.0010578  0.0002994  -3.533             0.000412 ***\nPrecipitation     -0.1998269  0.0312335  -6.398   0.0000000001577433 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.216 on 470908 degrees of freedom\nMultiple R-squared:  0.1085,\tAdjusted R-squared:  0.1085 \nF-statistic:  1849 on 31 and 470908 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays have positive coefficients except Friday (0.017 to 0.079)\n- Wednesday has the highest weekday effect (+0.079)\n- Weekdays likely benefit from concentrated commuting patterns\n- Friday has negative coefficient -0.031 likely because it is a transition day between weekdays and weekend travel.\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.057 and -0.103)\n- This means FEWER trips per station-hour than Monday\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.322 trips/hour (midnight)\n1      -0.096       slightly fewer than midnight\n...\n6      +0.215       morning activity starting\n7      +0.502       morning rush building\n8      +0.917       PEAK morning rush\n9      +0.658       post-rush\n...\n17     +1.364       PEAK evening rush (5 PM!)\n18     +1.067       evening declining\n...\n23     +0.147       late night minimal\n\n\n## Model B: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_b <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train2024\n)\n\nsummary(model_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train2024)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7079 -0.4954 -0.1369  0.1959 20.3110 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)        0.1134745  0.0224648   5.051     0.00000043916484 ***\nas.factor(hour)1  -0.0322453  0.0105485  -3.057             0.002237 ** \nas.factor(hour)2  -0.0224157  0.0107320  -2.089             0.036737 *  \nas.factor(hour)3  -0.0493050  0.0109902  -4.486     0.00000724938815 ***\nas.factor(hour)4  -0.0290506  0.0108399  -2.680             0.007363 ** \nas.factor(hour)5   0.0894029  0.0107582   8.310 < 0.0000000000000002 ***\nas.factor(hour)6   0.2804593  0.0108490  25.851 < 0.0000000000000002 ***\nas.factor(hour)7   0.4430528  0.0108863  40.698 < 0.0000000000000002 ***\nas.factor(hour)8   0.6879627  0.0108458  63.431 < 0.0000000000000002 ***\nas.factor(hour)9   0.2946908  0.0109509  26.910 < 0.0000000000000002 ***\nas.factor(hour)10  0.2516438  0.0107455  23.419 < 0.0000000000000002 ***\nas.factor(hour)11  0.2793519  0.0108668  25.707 < 0.0000000000000002 ***\nas.factor(hour)12  0.4038799  0.0108105  37.360 < 0.0000000000000002 ***\nas.factor(hour)13  0.3724053  0.0106149  35.083 < 0.0000000000000002 ***\nas.factor(hour)14  0.3985993  0.0106161  37.547 < 0.0000000000000002 ***\nas.factor(hour)15  0.4591639  0.0106413  43.149 < 0.0000000000000002 ***\nas.factor(hour)16  0.6247453  0.0109242  57.189 < 0.0000000000000002 ***\nas.factor(hour)17  0.8761589  0.0110159  79.536 < 0.0000000000000002 ***\nas.factor(hour)18  0.4950094  0.0109098  45.373 < 0.0000000000000002 ***\nas.factor(hour)19  0.3897200  0.0108895  35.789 < 0.0000000000000002 ***\nas.factor(hour)20  0.2259800  0.0106075  21.304 < 0.0000000000000002 ***\nas.factor(hour)21  0.1364795  0.0106364  12.831 < 0.0000000000000002 ***\nas.factor(hour)22  0.1425271  0.0104227  13.675 < 0.0000000000000002 ***\nas.factor(hour)23  0.0571618  0.0109852   5.204     0.00000019561355 ***\ndotw_simple2       0.0280991  0.0060698   4.629     0.00000366922090 ***\ndotw_simple3       0.0182859  0.0059621   3.067             0.002162 ** \ndotw_simple4      -0.0215340  0.0059843  -3.598             0.000320 ***\ndotw_simple5      -0.0217106  0.0059403  -3.655             0.000257 ***\ndotw_simple6      -0.0419804  0.0059695  -7.032     0.00000000000203 ***\ndotw_simple7      -0.0517625  0.0059620  -8.682 < 0.0000000000000002 ***\nTemperature       -0.0017189  0.0002593  -6.629     0.00000000003375 ***\nPrecipitation      0.0447963  0.0270550   1.656             0.097773 .  \nlag1Hour           0.3725747  0.0013671 272.522 < 0.0000000000000002 ***\nlag3Hours          0.1280395  0.0013424  95.381 < 0.0000000000000002 ***\nlag1day            0.1515864  0.0012576 120.540 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.053 on 470905 degrees of freedom\nMultiple R-squared:  0.3318,\tAdjusted R-squared:  0.3317 \nF-statistic:  6876 on 34 and 470905 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Did adding lags improve R²? Why or why not?\nAdding lags improve R². The value goes from 0.1085 to 0.3318. Adding lags improve the value because it adds more information and unobserved factors, and captures the autocorrelation and short-term pattern that the model can't see otherwise. \n\n## Model C: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_c <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train2024\n)\n\nsummary(model_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train2024)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.1461 -0.7352 -0.2837  0.4567 20.2120 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.9020140076  0.0499884512  18.044\nas.factor(hour)1          0.0311467158  0.0363543047   0.857\nas.factor(hour)2          0.0561150806  0.0396743924   1.414\nas.factor(hour)3         -0.0742900431  0.0486401689  -1.527\nas.factor(hour)4         -0.1036715915  0.0432132121  -2.399\nas.factor(hour)5          0.0015568151  0.0323444958   0.048\nas.factor(hour)6          0.2651587866  0.0288212316   9.200\nas.factor(hour)7          0.4222727828  0.0274362807  15.391\nas.factor(hour)8          0.6976777408  0.0265384539  26.289\nas.factor(hour)9          0.1612896925  0.0267524861   6.029\nas.factor(hour)10         0.1503064391  0.0267396832   5.621\nas.factor(hour)11         0.2061391725  0.0268465327   7.678\nas.factor(hour)12         0.3196721811  0.0265069959  12.060\nas.factor(hour)13         0.2805154285  0.0262810040  10.674\nas.factor(hour)14         0.3250766413  0.0262451488  12.386\nas.factor(hour)15         0.4088997220  0.0262365957  15.585\nas.factor(hour)16         0.6065557574  0.0263262229  23.040\nas.factor(hour)17         0.9427119361  0.0262242901  35.948\nas.factor(hour)18         0.4635623318  0.0263039347  17.623\nas.factor(hour)19         0.3482045898  0.0265406222  13.120\nas.factor(hour)20         0.1639302499  0.0267014565   6.139\nas.factor(hour)21         0.1042240447  0.0275036454   3.789\nas.factor(hour)22         0.1528031992  0.0278806456   5.481\nas.factor(hour)23         0.0730181012  0.0300114385   2.433\ndotw_simple2              0.0382406455  0.0121202748   3.155\ndotw_simple3              0.0331493228  0.0119766342   2.768\ndotw_simple4             -0.0342169964  0.0120581256  -2.838\ndotw_simple5             -0.0765661056  0.0119547965  -6.405\ndotw_simple6             -0.0323932719  0.0123461062  -2.624\ndotw_simple7             -0.0439453699  0.0123554675  -3.557\nTemperature              -0.0007072493  0.0005172481  -1.367\nPrecipitation            -0.4575041466  0.0657788672  -6.955\nlag1Hour                  0.2729993934  0.0020899684 130.624\nlag3Hours                 0.0853579939  0.0021573483  39.566\nlag1day                   0.1191406018  0.0020476145  58.185\nMed_Inc.x                 0.0000007048  0.0000001105   6.378\nPercent_Taking_Transit.y -0.0038693329  0.0004024446  -9.615\nPercent_White.y           0.0030892285  0.0002034287  15.186\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.391582    \nas.factor(hour)2                     0.157249    \nas.factor(hour)3                     0.126679    \nas.factor(hour)4                     0.016438 *  \nas.factor(hour)5                     0.961611    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9           0.0000000016535432 ***\nas.factor(hour)10          0.0000000190040257 ***\nas.factor(hour)11          0.0000000000000162 ***\nas.factor(hour)12        < 0.0000000000000002 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14        < 0.0000000000000002 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19        < 0.0000000000000002 ***\nas.factor(hour)20          0.0000000008302855 ***\nas.factor(hour)21                    0.000151 ***\nas.factor(hour)22          0.0000000424437524 ***\nas.factor(hour)23                    0.014975 *  \ndotw_simple2                         0.001605 ** \ndotw_simple3                         0.005644 ** \ndotw_simple4                         0.004545 ** \ndotw_simple5               0.0000000001511197 ***\ndotw_simple6                         0.008697 ** \ndotw_simple7                         0.000376 ***\nTemperature                          0.171523    \nPrecipitation              0.0000000000035336 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                  0.0000000001798530 ***\nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.288 on 170624 degrees of freedom\n  (300278 observations deleted due to missingness)\nMultiple R-squared:  0.2207,\tAdjusted R-squared:  0.2205 \nF-statistic:  1306 on 37 and 170624 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model D: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_d <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train2024\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model_d)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2491771 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model_d)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2479962 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?** \nStation fixed effects capture time-invariant differences between stations — essentially each station’s baseline level of demand.\nThey control for the fact that some stations are naturally busier or quieter than others, regardless of day, weather, events, etc..\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_e <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train2024\n)\n\ncat(\"Model 5 R-squared:\", summary(model_e)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2542104 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model_e)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2530242 \n```\n\n\n:::\n:::\n\n\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest2024 <- test2024 %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test2024$dotw_simple) <- contr.treatment(7)\n\ntest2024 <- test2024 %>%\n  mutate(\n    pred1 = predict(model_a, newdata = test2024),\n    pred2 = predict(model_b, newdata = test2024),\n    pred3 = predict(model_c, newdata = test2024),\n    pred4 = predict(model_d, newdata = test2024),\n    pred5 = predict(model_e, newdata = test2024)\n  )\n\n# Calculate MAE for each model\nmae_results2024 <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test2024$Trip_Count - test2024$pred1), na.rm = TRUE),\n    mean(abs(test2024$Trip_Count - test2024$pred2), na.rm = TRUE),\n    mean(abs(test2024$Trip_Count - test2024$pred3), na.rm = TRUE),\n    mean(abs(test2024$Trip_Count - test2024$pred4), na.rm = TRUE),\n    mean(abs(test2024$Trip_Count - test2024$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results2024, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.69 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.91 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results2024, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/compare_models2024-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\nTemporal Lags gave us the biggest improvement.\n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model B) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest2024 <- test2024 %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test2024, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/obs_vs_pred2024-1.png){width=672}\n:::\n:::\n\n\n**Question:** Where is the model performing well? Where is it struggling?\nLike in Part 1, The model in Part 2 does well at low trip counts and especially overnight, where predictions stay close to the perfect-fit line.\nOn the other side, it struggles at higher trip counts—especially during AM and PM rush hours—where it consistently underpredicts demand. It's likely because bike demand becomes more variable and harder to predict at high-volume times (like rush hours). Low-trip periods—especially overnight—are more stable and predictable, so the model fits them better.\n\n##Result Comparison to Q1 2025:\n\n   - **How do MAE values compare? Why might they differ?**\nOverall MAE is lower in Q1 2025 than in Q3 2024 across every model variant.\nExample: the baseline model (Time + Weather) in Q3 2024 is 0.83, while in Q1 2025 is 0.60.           Also,the best model (with temporal lags) is 0.69 in Q3 2024, and it is 0.50 in Q1 2025.\n\n**Why they may differ:**\n1. Seasonal effect: Q3 (summer) has higher, more volatile demand, making trips harder to predict.\nQ1 (winter) demand is lower and more stable, so predictions improve.\n2. Weather patterns differ: summer storms/heat waves create sudden spikes, while winter conditions suppress demand consistently.\n3. Behavioral variability (tourists, events, leisure rides) is higher in Q3 than Q1.\n   \n   - **Are temporal patterns different in summer vs. winter?**\nYes, temporal patterns in summer and winter is different:\n\n**Q3 2024 (summer):**\nLargest errors during PM Rush and AM Rush.\nMid-Day errors are also high because summer mid-day demand fluctuates a lot (tourism, leisure).\n\n**Q1 2025 (winter):**\nErrors are lower across all periods.\nOvernight stays lowest in both quarters, but drops even further in Q1.\nRush-hour peaks remain the highest, but still much lower than summer.\n\n   - **Which features are most important in your quarter?**\nIn both Q3 2024 and Q1 2025, temporal lags are the most important feature.\nThis makes sense: recent bike usage is always the strongest predictor of near-future usage, regardless of season.\n\n# Error Analysis \n\nIn this part, I analyze model error across spatial, temporal, and demographic dimensions to understand where and why the model struggles. First, I examine spatial patterns by mapping prediction errors, identifying neighborhoods with consistently high mistakes, and considering whether missing features or unique local demand patterns might explain them. Next, I explore temporal patterns, looking at when errors peak, whether certain hours or days show systematic bias, and how seasonal shifts affect performance. Finally, I assess demographic patterns, connecting errors to census characteristics to see whether particular communities are harder to predict and to consider the potential equity implications of these discrepancies.\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors2024 <- test2024 %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors2024 <- test2024 %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors2024,\n    aes(x = start_lon, y = start_lat, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors2024,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n\n\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors2024,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors2024,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/spatial_errors2024-1.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/spatial_errors2024-2.png){width=672}\n:::\n:::\n\n\n**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?\nSame like Q1 2025's MAE spatial error pattern, The highest MAE values appear around Center City, University City, and nearby high-activity areas. These are busy neighborhoods with more variable trip patterns, so that might be the reason why the model struggles more in there.\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors2024 <- test2024 %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors2024, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/temporal_errors2024-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo2024 <- station_errors2024 %>%\n  left_join(\n    station_attributes2024 %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo2024, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo2024, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo2024, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Indego_SpaceTime_Prediction_files/figure-html/errors_demographics2024-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\nYes. The plots show higher prediction errors (MAE) in areas with higher median income and higher % White, while errors decrease in areas with higher transit usage.\n\nEquity implications:\nIf a model is less accurate for certain demographic groups, it can unevenly distribute errors, potentially reinforcing existing disparities. Higher-income or predominantly White areas receiving systematically worse predictions could affect resource allocation or service quality, while lower-error groups may benefit unfairly.\n\n# Feature Engineering & model improvement\n\nTo address the systematic patterns uncovered in the error analysis, I expanded the feature set with two temporal demand–history variables to my best model (model B): Rolling 7-day average demand and Same hour last week. These features capture short-term trends, recurring weekly cycles, and station-specific usage patterns that simpler temporal indicators miss. Because bike-share demand is highly seasonal, weather-sensitive, and strongly periodic (daily/weekly), incorporating recent historical behavior provides the model with direct information about underlying demand rhythms.\n\n##Add temporal feature\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add Rolling 7-day average (168 hours) and same hour last week (168 lag)\ntrain2024_plus <- train2024 %>%\n  group_by(start_station) %>%\n  arrange(interval60, .by_group = TRUE) %>%\n  mutate(\n    rolling7day = as.numeric(stats::filter(Trip_Count, rep(1/168, 168), sides = 1)),\n    same_hour_last_week = lag(Trip_Count, 168)\n  ) %>%\n  ungroup()\n\ntest2024_plus <- test2024 %>%\n  group_by(start_station) %>%\n  arrange(interval60, .by_group = TRUE) %>%\n  mutate(\n    rolling7day = as.numeric(stats::filter(Trip_Count, rep(1/168, 168), sides = 1)),\n    same_hour_last_week = lag(Trip_Count, 168)\n  ) %>%\n  ungroup() \n\n# Recreate temporal train/test split with new features \nmodel_b_plus <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rolling7day + same_hour_last_week,\n  data = train2024_plus\n)\n\n#Predict\ntest2024_plus <- test2024_plus %>%\n  mutate(\n    pred_b_plus = predict(model_b_plus, newdata = test2024_plus)\n  )\n\n#MAE Calculation\nmae_b_plus <- mean(abs(test2024_plus$Trip_Count - test2024_plus$pred_b_plus), na.rm = TRUE)\nmae_b_plus\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6726737\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MAE for Model B with new features:\", round(mae_b_plus, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMAE for Model B with new features: 0.67 \n```\n\n\n:::\n:::\n\nAfter adding the Rolling 7-day average demand and Same hour last week features, the model’s MAE improved from 0.69 → 0.67.\nThis indicates that incorporating recent demand patterns helped the model better capture temporal dynamics.\n\n##Poisson model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a Poisson regression using the enhanced feature set\nmodel_b_pois <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rolling7day + same_hour_last_week,\n  data = train2024_plus,\n  family = poisson(link = \"log\")\n)\n\n# Predict on the held-out test set\ntest2024_plus$pred_pois <- predict(\n  model_b_pois,\n  newdata = test2024_plus,\n  type = \"response\"   # gives predicted Trip_Count\n)\n\n# Compute MAE\nmae_pois <- mean(abs(test2024_plus$Trip_Count - test2024_plus$pred_pois), na.rm = TRUE)\n\nmae_pois\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7109296\n```\n\n\n:::\n:::\n\nThe Poisson model's MAE is 0.71 slightly increased from MAE of 0.67. This suggests that a standard Poisson regression is not the best choice for this dataset. In practice, bike-share demand often violates key Poisson assumptions:\n    - Overdispersion (variance >> mean), which makes the Poisson too restrictive\n    - Strong temporal cycles not captured by simple Poisson structure\n    - Nonlinear relationships that Poisson regression can’t model well\n\n## Part 4: Critical Reflection \n\nThe final MAE of 0.67 is reasonably strong for a citywide demand-forecasting tool, but whether it is “good enough” depends on how Indego plans to use it. For high-level staffing or fleet-sizing decisions, this accuracy is likely sufficient. However, for real-time rebalancing, even small errors can create operational issues—particularly during peak commuting hours or at high-volume stations where a misprediction of even a few bikes can lead to stockouts or docks filling up. I would recommend deploying the system only as a decision-support tool, not an automated controller: planners should use the forecasts to prioritize stations but still rely on staff judgment, live occupancy data, and dispatcher experience.\n\nThe error analysis suggests the model performs somewhat differently across neighborhood types, with modestly higher errors in higher-income and predominantly White areas and lower errors in zones with higher transit use. While these differences are not extreme, any systematic bias risks uneven service quality, especially if rebalancing resources are allocated based on predicted demand. If forecasting is less accurate in certain neighborhoods, those areas may experience more frequent stockouts or reduced availability, potentially reinforcing disparities in mobility access. To prevent this, I recommend safeguards such as equity-weighted rebalancing criteria, routine monitoring of prediction bias across demographic groups, and operational overrides ensuring minimum service levels across all neighborhoods regardless of predicted demand.\n\nThe model still misses important factors such as special events, disruptions (street closures, sports games), and sudden weather shifts, all of which can dramatically change demand. It also assumes that past patterns will continue to hold, which may not be true during seasonal transitions or policy changes. With more time and data, I would integrate event calendars, higher-resolution weather forecasts, richer spatial features, and more real-time signals (e.g., live trip start rates) to further reduce error and improve robustness for deployment.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}