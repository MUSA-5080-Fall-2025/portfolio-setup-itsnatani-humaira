{
  "hash": "63eced20d4b207df6c094c5a39437a64",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Diagnostics & Spatial Autocorrelation\"\nsubtitle: \"Week 7: MUSA 5080\"\nauthor: \"Dr. Elizabeth Delmelle\"\ndate: \"October 21, 2025\"\nformat: \n  revealjs:\n    theme: simple\n    slide-number: true\n    chalkboard: true\n    code-line-numbers: true\n    incremental: false\n    smaller: true\n    scrollable: true\n---\n\n# Homework Feedback & Tips\n\n## Before We Start: A Quick Note on Your Submissions\n\n**We noticed something in your homework submissions...**\n\nMany of you have **messy output** in your rendered HTML files from `tigris` and `tidycensus` functions.\n\n**Example of what we're seeing:**\n```\nRetrieving data for the year 2022\n  |======================================================================| 100%\n  |======================================================================| 100%\nDownloading: 4.3 MB     \nDownloading: 3.7 MB\n```\n\n**This clutters your professional report!**\n\n---\n\n## The Problem: Progress Bars in Rendered Output {.smaller}\n\n**What's happening:**\n\nWhen you use `tigris` or `tidycensus` functions, they show download progress by default.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### **In your console (good!):**\n\n::: {.cell}\n\n```{.r .cell-code}\ntracts <- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE\n)\n```\n:::\n\n\nShows:\n```\nGetting data from the 2018-2022 5-year ACS\n  |======| 100%\n```\n\n**This is helpful when coding!**\n:::\n\n::: {.column width=\"50%\"}\n### **In your rendered HTML (bad!):**\n\nAll those progress messages appear as ugly text in your final document.\n\n**This looks unprofessional** and makes your work harder to read.\n\n**Solution:** Suppress progress messages in your code chunks\n:::\n::::\n\n---\n\n## The Solution: Add `progress = FALSE` {.smaller}\n\n**Two ways to fix this:**\n\n### **Option 1: In each function call**\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add progress = FALSE to EVERY tigris/tidycensus call\ntracts <- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE,\n  progress = FALSE  # <-- Add this!\n)\n\nroads <- roads(state = \"PA\", \n               county = \"Philadelphia\",\n               progress = FALSE)  # <-- Add this!\n```\n:::\n\n\n### **Option 2: Set globally at top of document** (Recommended!)\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n```\n:::\n\n\n---\n\n## Action Required: Re-Render Before Final Grading {.smaller}\n\n::: {.callout-important}\n## **📝 To-Do Before We Grade**\n\n**Please go back to your homework and:**\n\n1. Open your `.qmd` file\n2. Add `progress = FALSE` to all `get_acs()`, `get_decennial()`, and `tigris` function calls\n   - OR add the global options at the top of your document\n3. **Re-render** your document (Click \"Render\" button)\n4. Check that the HTML output is clean\n5. **Re-submit** on Canvas if needed (but it should all update on your website once you re-render!)\n\n**Deadline:** Before our next class meeting\n\n**Why this matters:** We gotta look good!\n:::\n\n---\n\n# Today's Plan\n\n## Agenda Overview\n\n**Part 1: Review & Connect**\n\n- Where we've been and where we're going\n- The regression workflow so far\n\n**Part 2: Evaluating Model Quality**\n\n- Train/test splits vs. cross-validation review\n- Spatial patterns in errors\n- Introduction to spatial autocorrelation\n\n**Part 3: Moran's I as a Diagnostic**\n\n- Understanding spatial clustering\n- Calculating and interpreting Moran's I\n- Local vs. global measures\n\n**BREAK (10 min)**\n\n**Part 4: Midterm Work Session (90+ min)**\n\n---\n\n# Part 1: Where We Are\n\n## The Journey So Far\n\n**Weeks 1-3:** Data foundations\n\n- Census data, tidycensus, spatial data basics\n- Visualization and exploratory analysis\n\n**Week 5:** Linear regression fundamentals\n\n- Y = f(X) + ε framework\n- Train/test splits, cross-validation\n- Checking assumptions\n\n**Week 6:** Expanding the toolkit\n\n- Categorical variables and interactions\n- Spatial features (buffers, kNN, distance)\n- Neighborhood fixed effects\n\n## Last Week's Key Innovation\n\n**You learned to create spatial features:**\n\n\n::: {.cell}\n\n:::\n\n\n**Today's Question:**\n\n> *How do we know if our model still has spatial structure in its errors?*\n\nIf errors are spatially clustered, we're missing something important!\n\n## The Regression Workflow (Updated)\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**Building the model:**\n\n1. Visualize relationships\n2. Engineer features\n3. Fit the model\n4. Evaluate performance (RMSE, R²)\n5. Check assumptions\n:::\n\n::: {.column width=\"50%\"}\n**NEW: Spatial diagnostics:**\n\n6. **Are errors random or clustered?**\n7. **Do we predict better in some areas?**\n8. **Is there remaining spatial structure?**\n:::\n:::\n\n::: {.callout-important}\n## Why This Matters\n\nIf errors cluster spatially, it suggests:\n\n- Missing spatial variables\n- Misspecified relationships\n- Non-stationarity (relationships vary across space)\n:::\n\n---\n\n# Part 2: Understanding Spatial Patterns in Errors\n\n## What Are Model Errors?\n\n**Prediction error** for observation *i*:\n\n$$e_i = \\hat{y}_i - y_i$$\n\nWhere:\n\n- $\\hat{y}_i$ = predicted value\n- $y_i$ = actual value\n\n**In our house price context:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages and data\nlibrary(sf)\nlibrary(here)\nlibrary(tidyverse)\n\n# Load Boston housing data\nboston <- read_csv(here(\"data/boston.csv\"))\n\n# Simple model: Predict price from living area\nbaseline_model <- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,\tAdjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nboston_test <- boston %>%\n  mutate(\n    predicted = predict(baseline_model, boston),\n    error = predicted - SalePrice,\n    abs_error = abs(error),\n    pct_error = abs(error) / SalePrice\n  )\n```\n:::\n\n\n## Good Errors vs. Bad Errors\n\n::: {.columns}\n::: {.column width=\"50%\"}\n** Random errors (good)**\n\n- No systematic pattern\n- Scattered across space\n- Prediction equally good everywhere\n- Model captures key relationships\n:::\n\n::: {.column width=\"50%\"}\n** Clustered errors (bad)**\n\n- Spatial pattern visible\n- Under/over-predict in areas\n- Model misses something about location\n- Need more spatial features!\n:::\n:::\n\n**How do we test this?**\n\nLook for **spatial autocorrelation** in the errors\n\n## Tobler's First Law (Revisited)\n\n::: {.callout-note}\n## The First Law of Geography\n\n*\"Everything is related to everything else, but near things are more related than distant things.\"*\n\n— Waldo Tobler (1970)\n:::\n\n**Applied to house prices:**\n\n- Nearby houses have similar prices\n- Nearby neighborhoods have similar characteristics\n- Crime in one block affects adjacent blocks\n\n**Applied to model errors:**\n\n- If nearby houses have similar errors...\n- ...our model is missing a spatial pattern!\n- Need to add more spatial features or fixed effects\n\n## Visualizing Error Patterns\n\n**Map your errors to see patterns:**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week7_slides_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n**What to look for:**\n\n- Blue clusters (we under-predict)\n- Red clusters (we over-predict)\n- Random scatter (good!)\n\n## Scatter Plot: Spatial Lag of Errors\n\n**Create the spatial lag:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spdep)\n\n# Define neighbors (5 nearest)\ncoords <- st_coordinates(boston_test)\nneighbors <- knn2nb(knearneigh(coords, k=5))\nweights <- nb2listw(neighbors, style=\"W\")\n\n# Calculate spatial lag of errors\nboston_test$error_lag <- lag.listw(weights, boston_test$error)\n```\n:::\n\n\n**Then plot:**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week7_slides_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n---\n\n# Part 3: Moran's I\n\n## What is Moran's I?\n\n**Moran's I** measures spatial autocorrelation\n\n**Range:** -1 to +1\n\n- **+1** = Perfect positive correlation (clustering)\n- **0** = Random spatial pattern\n- **-1** = Perfect negative correlation (dispersion)\n\n**Formula (look's scary, but its so intuitive!):**\n\n$$I = \\frac{n \\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i \\sum_j w_{ij} \\sum_i (x_i - \\bar{x})^2}$$\n\nWhere $w_{ij}$ = spatial weight between locations *i* and *j*\n\n## Worked Example: Understanding the Formula\n\n**5 houses in a row, predicting sale prices:**\n\n| House | Actual Price | Predicted Price | Error | \n|-------|-------------|-----------------|-------|\n| A     | $500k       | $400k          | +$100k |\n| B     | $480k       | $400k          | +$80k  |\n| C     | $420k       | $400k          | +$20k  |\n| D     | $350k       | $400k          | -$50k  |\n| E     | $330k       | $400k          | -$70k  |\n\n**Mean error = +$16k**\n\n**The question:** Are errors for nearby houses similar to each other?\n\n---\n\n## Step 1: Calculate Deviations from Mean\n\n**Subtract the mean error from each house's error:**\n\n| House | Error | Mean Error | Deviation from Mean |\n|-------|-------|------------|-------------------|\n| A     | +$100k | +$16k     | +$84k            |\n| B     | +$80k  | +$16k     | +$64k            |\n| C     | +$20k  | +$16k     | +$4k             |\n| D     | -$50k  | +$16k     | -$66k            |\n| E     | -$70k  | +$16k     | -$86k            |\n\n**Positive deviation** = we over-predicted (actual > predicted)  \n**Negative deviation** = we under-predicted (actual < predicted)\n\n---\n\n## Step 2: Multiply Neighbor Deviations\n\n**For each neighbor pair, multiply their deviations:**\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Neighbor Pairs:**\n\n- A-B: $(+84k) \\times (+64k) = +5,376$\n- B-C: $(+64k) \\times (+4k) = +256$\n- C-D: $(+4k) \\times (-66k) = -264$\n- D-E: $(-66k) \\times (-86k) = +5,676$\n\n**Sum of products = 11,044**\n:::\n\n::: {.column width=\"50%\"}\n**What does this mean?**\n\n**Positive products** = similar neighbors\n- A-B: both over-predicted (both positive)\n- D-E: both under-predicted (both negative)\n\n**Negative product** = dissimilar neighbors  \n- C-D: one over, one under\n\n**The pattern:** High-error houses cluster together, low-error houses cluster together\n:::\n::::\n\n---\n\n## The Intuition Behind Moran's I\n\n**The formula is really just asking:**\n\n> _\"When I'm above/below average, are my neighbors also above/below average?\"_\n\n**Breaking it down:**\n\n1. $(x_i - \\bar{x})$ = How far is my house's error from the mean?\n\n2. $(x_j - \\bar{x})$ = How far is my neighbor's error from the mean?\n\n3. **Multiply them:**\n   - If both positive or both negative → **positive product** (similar)\n   - If opposite signs → **negative product** (dissimilar)\n\n4. **Sum across all neighbor pairs** and normalize\n\n**Result:**\n\n- Lots of positive products → **High Moran's I** (clustering)\n- Products near zero → **Low Moran's I** (random)\n- Negative products → **Negative Moran's I** (rare with errors)\n\n---\n\n## The Intuition Behind Moran's I\n\n**The formula is really just asking:**\n\n> *\"When I'm above/below average, are my neighbors also above/below average?\"*\n\n**Breaking it down:**\n\n1. $(x_i - \\bar{x})$ = How far am I from the mean?\n2. $(x_j - \\bar{x})$ = How far is my neighbor from the mean?\n\n3. **Multiply them:** \n\n   - If both positive or both negative → **positive product** (similar)\n   - If opposite signs → **negative product** (dissimilar)\n   \n4. **Sum across all neighbor pairs** and normalize\n\n**Result:**\n\n- Lots of positive products → **High Moran's I** (clustering)\n- Products near zero → **Low Moran's I** (random)\n- Negative products → **Negative Moran's I** (rare with errors)\n\n\n## Defining \"Neighbors\"\n\n**Different ways to define spatial relationships:**\n\n::: {.columns}\n::: {.column width=\"33%\"}\n**Contiguity**\n\n- Polygons that share a border\n- Queen vs. Rook\n:::\n\n::: {.column width=\"33%\"}\n**Distance**\n\n- All within X meters\n- Fixed threshold\n:::\n\n::: {.column width=\"33%\"}\n**k-Nearest**\n\n- Closest k points\n- Adaptive distance\n:::\n:::\n\n**For point data (houses), use k-nearest neighbors**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 5-nearest neighbors\ncoords <- st_coordinates(boston_test)\nnb <- knn2nb(knearneigh(coords, k=5))\nweights <- nb2listw(nb, style=\"W\")\n```\n:::\n\n\n## Calculating Spatial Lag\n\n**Spatial lag** = average value of neighbors\n\n::: {.callout-tip}\n## Example: 5 houses\n\n| House | Sale Price | 2 Nearest | Spatial Lag |\n|-------|-----------|-----------|-------------|\n| A     | $200k     | B, C      | $275k       |\n| B     | $250k     | A, C      | $250k       |\n| C     | $300k     | B, D      | $275k       |\n| D     | $350k     | C, E      | $350k       |\n| E     | $400k     | D         | $350k       |\n:::\n\n**In R:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboston$price_lag <- lag.listw(weights, boston$SalePrice)\n```\n:::\n\n\n## Computing Moran's I\n\n**Calculate Moran's I for your errors:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for spatial autocorrelation in errors\nmoran_test <- moran.mc(\n  boston_test$error,        # Your errors\n  weights,                  # Spatial weights matrix\n  nsim = 999                # Number of permutations\n)\n\n# View results\nmoran_test$statistic         # Moran's I value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstatistic \n0.7186593 \n```\n\n\n:::\n\n```{.r .cell-code}\nmoran_test$p.value          # Is it significant?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001\n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\n- **I > 0** and *p* < 0.05 → Significant clustering\n- **I ≈ 0** → Random pattern (good!)\n- **I < 0** → Dispersion (rare with errors)\n\n## Visualizing Significance\n\n**Compare observed I to random permutations:**\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week7_slides_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n## What Moran's I Tells You\n\n::: {.callout-important}\n## Decision Framework\n\n**If Moran's I is high (errors clustered):**\n\n1. Add more spatial features\n   - Try different buffer sizes\n   - Include more amenities/disamenities\n   - Create neighborhood-specific variables\n\n2. Try spatial fixed effects\n   - Neighborhood dummies\n   - Grid cell dummies\n\n3. Consider spatial regression models\n   - Spatial lag model\n   - Spatial error model\n   - (Advanced topic, not covered today)\n:::\n\n**If Moran's I ≈ 0 (random errors):**\n\n✅ Your model adequately captures spatial relationships!\n\n---\n\n## \"What About Spatial Lag/Error Models?\" {.smaller}\n\n*\"In my spatial statistics class, I learned about spatial lag and spatial error models for dealing with spatial autocorrelation. Why aren't we using those here?\"*\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### **Spatial Econometrics Models**\n**(Spatial Statistics Class)**\n\n**Spatial Lag Model:** $Y_i = \\rho WY + \\beta X_i + \\varepsilon$\n\n**Spatial Error Model:** $Y_i = \\beta X_i + \\lambda W\\varepsilon + \\xi$\n\n**Purpose:** \n\n- Causal inference with spatial spillovers\n- Understanding neighbor effects\n- Correct standard errors for hypothesis testing\n- Cross-sectional analysis\n\n**When to use:** Academic research on spillover effects, peer influence, regional economics\n:::\n\n::: {.column width=\"50%\"}\n### **Predictive Spatial Features**\n**(This Class)**\n\n**Our Approach:** \n$$Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2(\\text{crimes}_{500ft}) + \\beta_3(\\text{dist}_{downtown}) + \\varepsilon_i$$\n\n**Purpose:**\n\n- Out-of-sample prediction\n- Forecasting new observations  \n- Applied machine learning\n- Generalization to new areas\n\n**When to use:** Real estate prediction, housing market forecasting, policy planning\n:::\n::::\n\n---\n\n## Why Not Spatial Lag Models for Prediction? {.smaller}\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### **The Problems with Spatial Lag for Prediction:**\n\n**1. Simultaneity Problem**\n\n- Including $WY$ (neighbor prices) creates circular logic\n- My price affects neighbors → neighbors affect me\n- OLS estimates are **biased and inconsistent**\n\n**2. Prediction Paradox**\n\n- Need neighbors' prices to predict my price\n- But for new developments or future periods, those prices **don't exist yet**\n- Can't generalize to truly new areas\n\n**3. Data Leakage in CV**\n\n- Geographic CV holds out spatial regions\n- Spatial lag \"leaks\" information from test set\n- Artificially good performance that won't hold\n:::\n\n::: {.column width=\"50%\"}\n### **Our Solution: Spatial Features of X (Not Y)**\n\nInstead of modeling dependence in **Y** (prices), model proximity in **X** (predictors)\n\n| ❌ Spatial Lag | ✅ Our Approach |\n|----------------|-----------------|\n| \"Near expensive houses\" | \"Near low crime areas\" |\n| Uses neighbor **prices** | Uses neighbor **characteristics** |\n| Circular logic | Causal mechanism |\n| Can't predict new areas | Generalizes well |\n\n**If Moran's I shows clustered errors:**\n\n✅ **Add more spatial features** (different buffers, more amenities)  \n✅ **Try neighborhood fixed effects**  \n✅ **Use spatial cross-validation**\n\n❌ Don't add spatial lag of Y for prediction purposes\n\n::: {.callout-tip}\n## **The Bottom Line**\nBoth approaches are valid for different goals! Match method to purpose: **inference** → spatial lag/error models; **prediction** → spatial features.\n:::\n:::\n::::\n\n---\n\n## Quick Clarification: Biased vs. Inconsistent {.smaller}\n\nWhen we say OLS estimates are **\"biased and inconsistent\"** with spatial lag models, what does that mean?\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### **Biased Estimator**\n\n**Definition:** Expected value ≠ true parameter\n\n$$E[\\hat{\\beta}] \\neq \\beta$$\n\n**What this means:**\n\n- On average, across all possible samples, your estimate is **systematically wrong**\n- Doesn't get the right answer even in expectation\n- More data doesn't fix it\n\n**Example:**\n\n- True effect: β = 100\n- Your estimates average to: 80\n- You're systematically **20 units off**\n:::\n\n::: {.column width=\"50%\"}\n### **Inconsistent Estimator**\n\n**Definition:** Doesn't converge to true value as n → ∞\n\n$$\\hat{\\beta} \\not\\to \\beta \\text{ as } n \\to \\infty$$\n\n**What this means:**\n\n- Even with **infinite data**, you won't get the right answer\n- The problem doesn't go away with bigger samples\n- Violates a fundamental property of good estimators\n\n**Example:**\n- n = 100 → estimate = 80\n- n = 10,000 → estimate = 82\n- n = 1,000,000 → estimate = 84\n- Never reaches true value of 100\n:::\n::::\n\n---\n\n# Summary & Next Steps\n\n## Key Takeaways\n\n**Spatial autocorrelation in errors indicates model misspecification**\n\n**Moran's I is a diagnostic tool:**\n\n- Global I: overall clustering\n- Maps of residuals give clues to what you might be missing\n\n**Iterative improvement:**\n\n- Diagnose → Engineer features → Re-test → Repeat\n- Document what you try!\n\n\n\n## Resources\n\n**Spatial autocorrelation:**\n- [https://mgimond.github.io/Spatial/spatial-autocorrelation.html](https://mgimond.github.io/Spatial/spatial-autocorrelation.html)\n\n**spdep package:**\n- [https://r-spatial.github.io/spdep/](https://r-spatial.github.io/spdep/)\n\n---\n\n# Questions Before Work Time?\n\nCome see me during the work session for:\n\n- Help with Moran's I calculation\n- Ideas for new spatial features\n- Debugging code issues\n- Discussing your model strategy\n\n",
    "supporting": [
      "week7_slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}